"use strict";(self.webpackChunkfull_stack_chronicles=self.webpackChunkfull_stack_chronicles||[]).push([[58744],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>d});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(a),d=o,g=m["".concat(s,".").concat(d)]||m[d]||u[d]||r;return a?n.createElement(g,i(i({ref:t},c),{},{components:a})):n.createElement(g,i({ref:t},c))}));function d(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var p=2;p<r;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},12348:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var n=a(83117),o=(a(67294),a(3905));const r={slug:"apache-beam-in-five-minutes",title:"Apache Beam in Five Minutes",authors:["jeffreyaven"],draft:!1,image:"/img/blog/apache-beam-in-five-minutes.png",tags:["google","gcp","beam","apache beam","dataflow","google cloud dataflow","etl","stream processing"],keywords:["google","gcp","beam","apache beam","dataflow","google cloud dataflow","etl","stream processing"],description:"This article provides a quick introduction to Apache Beam - the batch and stream ETL programming model and SDK used by Google Cloud Dataflow."},i=void 0,l={permalink:"/apache-beam-in-five-minutes",editUrl:"https://github.com/stackql/fullstackchronicles.io/edit/main/blog/2022-12-12-apache-beam-in-five-minutes/index.md",source:"@site/blog/2022-12-12-apache-beam-in-five-minutes/index.md",title:"Apache Beam in Five Minutes",description:"This article provides a quick introduction to Apache Beam - the batch and stream ETL programming model and SDK used by Google Cloud Dataflow.",date:"2022-12-12T00:00:00.000Z",formattedDate:"December 12, 2022",tags:[{label:"google",permalink:"/tags/google"},{label:"gcp",permalink:"/tags/gcp"},{label:"beam",permalink:"/tags/beam"},{label:"apache beam",permalink:"/tags/apache-beam"},{label:"dataflow",permalink:"/tags/dataflow"},{label:"google cloud dataflow",permalink:"/tags/google-cloud-dataflow"},{label:"etl",permalink:"/tags/etl"},{label:"stream processing",permalink:"/tags/stream-processing"}],readingTime:4.015,hasTruncateMarker:!1,authors:[{name:"Jeffrey Aven",title:"Technologist and Cloud Consultant",url:"https://www.linkedin.com/in/jeffreyaven/",imageURL:"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80",key:"jeffreyaven"}],frontMatter:{slug:"apache-beam-in-five-minutes",title:"Apache Beam in Five Minutes",authors:["jeffreyaven"],draft:!1,image:"/img/blog/apache-beam-in-five-minutes.png",tags:["google","gcp","beam","apache beam","dataflow","google cloud dataflow","etl","stream processing"],keywords:["google","gcp","beam","apache beam","dataflow","google cloud dataflow","etl","stream processing"],description:"This article provides a quick introduction to Apache Beam - the batch and stream ETL programming model and SDK used by Google Cloud Dataflow."},prevItem:{title:"Use Deno Deploy to Serve Non-Traditional Artifacts",permalink:"/use-deno-deploy-to-serve-non-traditional-artifacts"},nextItem:{title:"AWS IAM vs Google IAM",permalink:"/aws-iam-vs-google-iam"}},s={authorsImageUrls:[void 0]},p=[{value:"Beam SDK and Execution Framework",id:"beam-sdk-and-execution-framework",level:2},{value:"Beam Programming Model",id:"beam-programming-model",level:2},{value:"Beam DSL",id:"beam-dsl",level:2},{value:"Map, FlatMap and Filter",id:"map-flatmap-and-filter",level:3},{value:"ParDo and DoFn",id:"pardo-and-dofn",level:3},{value:"GroupByKey, CombineByKey",id:"groupbykey-combinebykey",level:3},{value:"CoGroupByKey and Flatten",id:"cogroupbykey-and-flatten",level:3},{value:"Side Inputs",id:"side-inputs",level:2},{value:"Sources, Sinks and Connectors",id:"sources-sinks-and-connectors",level:2},{value:"Streaming and Unbounded PCollections",id:"streaming-and-unbounded-pcollections",level:2},{value:"Templates",id:"templates",level:2}],c={toc:p};function u(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Apache Beam is an open-source project which provides a unified programming model for Batch and Streaming data pipelines.  "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"B"),"(",(0,o.kt)("em",{parentName:"p"},"atch"),") ",(0,o.kt)("strong",{parentName:"p"},"+")," ",(0,o.kt)("em",{parentName:"p"},"str"),"(",(0,o.kt)("strong",{parentName:"p"},"EAM"),") ",(0,o.kt)("strong",{parentName:"p"},"=>")," ",(0,o.kt)("strong",{parentName:"p"},"BEAM"),"  "),(0,o.kt)("h2",{id:"beam-sdk-and-execution-framework"},"Beam SDK and Execution Framework"),(0,o.kt)("p",null,"Beam SDKs allow you to define ",(0,o.kt)("strong",{parentName:"p"},"Pipelines")," (in languages such as Java or Python).  A pipeline is essentially a graph (a DAG - Directed Acyclic Graph) of nodes that represent transformation steps.  "),(0,o.kt)("p",null,(0,o.kt)("a",{target:"_blank",href:a(84542).Z},(0,o.kt)("img",{alt:"Apache Beam Pipeline",src:a(66481).Z,width:"735",height:"216"}))),(0,o.kt)("p",null,"Pipelines can then be executed on a backend service (such as your local machine, Apache Spark, or Google Cloud Dataflow) using ",(0,o.kt)("strong",{parentName:"p"},"Runners"),".  For instance, to run a pipeline locally, you would use the DirectRunner; to run a pipeline on Google Cloud Dataflow you would use the DataflowRunner runner.  "),(0,o.kt)("h2",{id:"beam-programming-model"},"Beam Programming Model"),(0,o.kt)("p",null,"The ",(0,o.kt)("strong",{parentName:"p"},"PCollection")," is the most atomic data unit in the Beam programming model, akin to the RDD in the Apache Spark core API; it is a representation of an immutable collection of items that is physically broken down into ",(0,o.kt)("strong",{parentName:"p"},"bundles")," (subsets of elements for parallelization).  "),(0,o.kt)("p",null,"PCollections can be ",(0,o.kt)("em",{parentName:"p"},"bounded")," (which is a batch processing pattern) or ",(0,o.kt)("em",{parentName:"p"},"unbounded")," (which is a stream processing pattern).  "),(0,o.kt)("p",null,"A ",(0,o.kt)("strong",{parentName:"p"},"PTransform")," is an operator that takes a PCollection as an input and outputs a new PCollection with transforms applied.  This is the same coarse-grained transformation pattern employed by Spark.  "),(0,o.kt)("h2",{id:"beam-dsl"},"Beam DSL"),(0,o.kt)("p",null,"The Beam DSL is a set of higher-order functions that can be used to construct pipelines.  These functions are used to construct the graph of nodes that represent the pipeline.  "),(0,o.kt)("h3",{id:"map-flatmap-and-filter"},"Map, FlatMap and Filter"),(0,o.kt)("p",null,"The basic ",(0,o.kt)("strong",{parentName:"p"},"Map"),", ",(0,o.kt)("strong",{parentName:"p"},"FlatMap")," and ",(0,o.kt)("strong",{parentName:"p"},"Filter")," functions in the Beam API work similarly to their namesakes in the Spark Core API.  The Map and FlatMap functions are higher-order functions (that is, functions that have arguments of other functions) that operate on each element in a collection, emitting an output element for each input element.  The Filter function can be used to only emit elements from an input PCollection that satisfy a given expression.  "),(0,o.kt)("h3",{id:"pardo-and-dofn"},"ParDo and DoFn"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"ParDo")," is a wrapper function for parallel execution of a user-defined function called a ",(0,o.kt)("strong",{parentName:"p"},"DoFn")," (\"do function\"), ParDo's and DoFn's are used when the basic Map and FlatMap operators are not enough.  DoFns are executed in parallel on a PCollection and can be used for computational transformations or transformations other than 1:1 between inputs and outputs.  "),(0,o.kt)("p",null,"Think of these as user-defined functions to operate on a PCollection in parallel.  "),(0,o.kt)("h3",{id:"groupbykey-combinebykey"},"GroupByKey, CombineByKey"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"GroupByKey")," and ",(0,o.kt)("strong",{parentName:"p"},"CombineByKey")," are operators that group data (key-value pairs) by the key for each element.  This is typically a precursor to some aggregate operation (such as a count or sum operation).  "),(0,o.kt)("h3",{id:"cogroupbykey-and-flatten"},"CoGroupByKey and Flatten"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"CoGroupByKey")," is akin to a ",(0,o.kt)("inlineCode",{parentName:"p"},"JOIN")," operation in SQL (by the key for each element in two PCollections).  ",(0,o.kt)("strong",{parentName:"p"},"Flatten")," is akin to a ",(0,o.kt)("inlineCode",{parentName:"p"},"UNION")," in SQL.  "),(0,o.kt)("h2",{id:"side-inputs"},"Side Inputs"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Side Inputs")," can be used with ParDo and DoFn to provide additional data, which can be used to enrich your output PCollection, or utilized within the logic of your DoFn.  "),(0,o.kt)("h2",{id:"sources-sinks-and-connectors"},"Sources, Sinks and Connectors"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Sources")," represent where data is read into an Apache Beam pipeline; ",(0,o.kt)("strong",{parentName:"p"},"sinks")," represent destinations where data is written out from pipelines.  A Beam pipeline will contain one or more sources and sinks.  "),(0,o.kt)("p",null,"Sources can be bounded (for batch processing) or unbounded for stream processing.  "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Connectors")," can be source connectors or sink connectors to read from or write to the various sources and targets used in a Beam pipeline.  Examples include FileIO and TextIO for working with files or text data, BigQueryIO for reading or writing into BigQuery, PubSubIO for reading and writing messages into Google PubSub, and much more.  "),(0,o.kt)("h2",{id:"streaming-and-unbounded-pcollections"},"Streaming and Unbounded PCollections"),(0,o.kt)("p",null,"Streaming data sources are represented by Unbounded PCollections.  Unbounded PCollections support windowing operations using ",(0,o.kt)("strong",{parentName:"p"},"Fixed Windows"),", ",(0,o.kt)("strong",{parentName:"p"},"Sliding Windows"),", or ",(0,o.kt)("strong",{parentName:"p"},"Session")," Windows.  ",(0,o.kt)("strong",{parentName:"p"},"Watermarks")," are used to allow for late-arriving data to be processed within its associated time window, and ",(0,o.kt)("strong",{parentName:"p"},"Triggers")," can be used to control the processing of windowed batches of data.   "),(0,o.kt)("h2",{id:"templates"},"Templates"),(0,o.kt)("p",null,"Beam templates enable the reusability of pipelines, converting compile-time pipeline parameters to run-time arguments.  Jobs (invocations of pipelines) can be launched from templates.  "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Templates")," include classic templates, where the graph for the pipeline is built (compile-time) with the template, flex templates where the pipeline graph is created when the template is launched (runtime).  "),(0,o.kt)("p",null,"In addition, Google provides several templates with Cloud Dataflow (Google-provided templates), allowing you to launch routine jobs without writing any code.  "),(0,o.kt)("p",null,"Google-provided templates are available for batch, streaming, and utility pipelines, for example:  "),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Kafka to BigQuery"),(0,o.kt)("li",{parentName:"ul"},"Pub/Sub Topic to BigQuery"),(0,o.kt)("li",{parentName:"ul"},"Text Files on Cloud Storage to BigQuery"),(0,o.kt)("li",{parentName:"ul"},"Text Files on Cloud Storage to Cloud Spanner"),(0,o.kt)("li",{parentName:"ul"},"Bulk Compress or Decompress Files on Cloud Storage"),(0,o.kt)("li",{parentName:"ul"},"and more")),(0,o.kt)("p",null,"5 minutes is up!  I hope you enjoyed this quick introduction to Apache Beam.  If you want to learn more, check out the ",(0,o.kt)("a",{parentName:"p",href:"https://beam.apache.org/documentation/"},"Apache Beam documentation"),"."))}u.isMDXComponent=!0},84542:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/files/apache_beam_pipeline-57da72b3b0021f899068fb96d45ac459.png"},66481:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/apache_beam_pipeline-57da72b3b0021f899068fb96d45ac459.png"}}]);