"use strict";(self.webpackChunkfull_stack_chronicles=self.webpackChunkfull_stack_chronicles||[]).push([[89450],{16029:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"databricks-in-five-minutes","metadata":{"permalink":"/databricks-in-five-minutes","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2023-06-02-databricks-in-five-minutes/index.md","source":"@site/blog/2023-06-02-databricks-in-five-minutes/index.md","title":"Databricks in 5 Minutes","description":"This article provides a quick and concise introduction to Databricks - a unified platform for big data analytics and machine learning.","date":"2023-06-02T00:00:00.000Z","formattedDate":"June 2, 2023","tags":[{"label":"databricks","permalink":"/tags/databricks"},{"label":"big data","permalink":"/tags/big-data"},{"label":"data science","permalink":"/tags/data-science"},{"label":"machine learning","permalink":"/tags/machine-learning"}],"readingTime":3.73,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"databricks-in-five-minutes","title":"Databricks in 5 Minutes","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/databricks-in-five-minutes.png","tags":["databricks","big data","data science","machine learning"],"keywords":["databricks","big data","data science","machine learning"],"description":"This article provides a quick and concise introduction to Databricks - a unified platform for big data analytics and machine learning."},"nextItem":{"title":"Netlify logging to SumoLogic Without Log Drains","permalink":"/netlify-logging-to-sumologic-without-log-drains"}},"content":"[__Databricks__](https://databricks.com/) is a unified data management and analytics platform built by the creators of Apache Spark. It provides a collaborative environment for data scientists, engineers, and business analysts to work together. This brief overview will walk you through the basics of Databricks.\\r\\n\\r\\n## Summary\\r\\n\\r\\nDatabricks is a cloud-based \\"as-a-Service\\" platform for data management and analytics powered by Apache Spark. It enables organizations to deploy scalable, high-performance analytics workloads against large-scale datasets in their cloud environments. Databricks also supports multiple languages (SQL, Python, R, Scala, and Java), interactive notebooks and collaborative features, job scheduling, and more. The Databricks platform supports batch and stream processing and analytics, integrating with various data sources and formats.  \\r\\n\\r\\n## Architecture\\r\\n\\r\\nThe Databricks architecture consists of two main components: the __Control Plane__ and the __Data Plane__.  \\r\\n\\r\\n![Databricks Architecture](images/DatabricksArchitecture.png)\\r\\n\\r\\nThe user interfaces and APIs are located in the  __Control Plane__. It\'s where users write code in notebooks, manage clusters, and schedule jobs. The Control Plane does not handle customer data directly.  \\r\\n\\r\\nThe __Data Plane__ - deployed in the customer\'s cloud environment and managed by the Control Plane - is where compute clusters (Apache Spark clusters) and storage resources are located. Spark jobs run in the Data Plane to process a customer\'s data.  \\r\\n\\r\\nThis architecture enables a clear separation of responsibilities and increases overall system security. By keeping customer data within the Data Plane, Databricks ensures that sensitive information remains in the customer\'s environment and control.  \\r\\n\\r\\nDatabricks supports a multi-cloud architecture, allowing customers to choose between AWS, Azure, and Google Cloud as their preferred environment for the Data Plane.  \\r\\n\\r\\n## Clusters\\r\\n\\r\\nDatabricks allows you to create Spark clusters required to execute notebook code. Clusters can be __Job Clusters__ used mainly for non-interactive or scheduled workloads, or __All Purpose Clusters__ which are mainly used for ad-hoc, interactive analysis operations. All-Purpose Clusters are shared clusters that multiple users can run commands on simultaneously. The Databricks Control Plane provides cluster automation, scaling, and collaboration capabilities.\\r\\n\\r\\n## Workspaces and Notebooks\\r\\n\\r\\nThe __Workspace__ is a personalized space where users can create notebooks, import libraries, and run jobs. __Notebooks__ are documents combining code execution, visualizations, and narrative. They support Python, R, Scala, and SQL. Databricks notebooks are similar to popular notebook environments such as Jupyter Notebooks and Apache Zeppelin Notebooks.  \\r\\n\\r\\n![Databricks Notebook](images/databricks-notebook.png)\\r\\n\\r\\n## Databricks File System (DBFS)\\r\\nDBFS is an abstraction layer on top of scalable object storage and offers the benefits of distributed storage without needing local file I/O. DBFS can be used as a source or target for jobs, and Databricks offers multiple utilities for working with DBFS.  \\r\\n\\r\\n## Delta Lake and Lakehouse\\r\\n\\r\\n[__Delta Lake__](https://delta.io/) is an open-source project providing ACID transactions, scalable metadata handling, and unifying streaming and batch data processing on top of your existing data lake. It brings reliability to data lakes, and it is fully compatible with Apache Spark APIs.  \\r\\n\\r\\nA __Lakehouse__ is a new kind of data management paradigm combining the benefits of data warehouses and data lakes. It provides a data warehouse\'s reliability, performance, and transactional capabilities with schema-on-read flexibility and low-cost data lake storage.\\r\\n\\r\\n## Delta Live Tables\\r\\n__Delta Live Tables__ represent the state of a streaming dataset, views, or materialized views. Delta Live Tables provide improved data availability, quality, and reliability.\\r\\n\\r\\n## Medallion Architecture\\r\\n\\r\\nThe __Medallion Architecture__ is a methodology for organizing data in your data lake. This is not a new concept; it has been around for a while in the field of data engineering. The names for the layers might differ (like *Raw*, *Clean*, and *Refined* etc), but the concept remains the same. It provides a guideline to systematically organize the data transformation process, with a clear separation between stages.  \\r\\n\\r\\nThe Medallion Architecture is named after the \'bronze\', \'silver\', and \'gold\' stages of refining raw material.  \\r\\n\\r\\n__Bronze__ tables, also known as raw tables, store the raw data ingested from various sources. This data is in its original form and hasn\'t undergone any transformation or cleaning.  \\r\\n\\r\\n__Silver__ tables contain cleansed and enriched data. This results from applying transformations, such as deduplication and schema normalization, to the bronze data.  \\r\\n\\r\\n__Gold__ tables hold business-level aggregates often used for reporting and dashboarding. This might be like daily active users or revenue by geography and product.  \\r\\n\\r\\nThis architecture aims to separate data processing into logical layers and allow different teams to work on each layer independently. The Databricks Lakehouse is designed to support this methodology.\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"netlify-logging-to-sumologic-without-log-drains","metadata":{"permalink":"/netlify-logging-to-sumologic-without-log-drains","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2023-02-06-netlify-logging-to-sumologic-without-log-drains/index.md","source":"@site/blog/2023-02-06-netlify-logging-to-sumologic-without-log-drains/index.md","title":"Netlify logging to SumoLogic Without Log Drains","description":"This is a solution to get verbose logs from Netlify functions and CDN logs to SumoLogic without needing a Log Drain.","date":"2023-02-06T00:00:00.000Z","formattedDate":"February 6, 2023","tags":[{"label":"netlify","permalink":"/tags/netlify"},{"label":"sumologic","permalink":"/tags/sumologic"},{"label":"JAMStack","permalink":"/tags/jam-stack"},{"label":"logging","permalink":"/tags/logging"},{"label":"log drain","permalink":"/tags/log-drain"}],"readingTime":1.92,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"netlify-logging-to-sumologic-without-log-drains","title":"Netlify logging to SumoLogic Without Log Drains","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/netlify-logging-to-sumologic-without-log-drains.png","tags":["netlify","sumologic","JAMStack","logging","log drain"],"keywords":["netlify","sumologic","JAMStack","logging","log drain"],"description":"This is a solution to get verbose logs from Netlify functions and CDN logs to SumoLogic without needing a Log Drain."},"prevItem":{"title":"Databricks in 5 Minutes","permalink":"/databricks-in-five-minutes"},"nextItem":{"title":"Enhancing dbt Snapshots with Operational Metadata","permalink":"/enhancing-dbt-snapshots-with-operational-metadata"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nLog Drains for function and CDN logs from Netlify, which have native SumoLogic collectors available, are included with Enterprise level subscriptions.  However, there is a quantum jump in subscription costs between the Teams or Pro Editions of Netlify and the Enterprise Edition.  \\r\\n\\r\\nIf you are deploying a JAMStack app in Netlify, the odds are each page will include an API call (using Netlify functions), and for authenticated pages, the request will include a Bearer Token.  \\r\\n\\r\\n## Solution Overview\\r\\n\\r\\nCreate a logging module that exports functions that can be used by any of your Netlify functions to log verbose data to the console (seen in the function log output in the Netlify app) and to push log events to a Hosted SumoLogic Collector Source.  This will allow you to see verbose logs for each function call and CDN logs for each page load.  \\r\\n\\r\\n## Steps\\r\\n\\r\\nSteps to deploy this solution are detailed here:  \\r\\n\\r\\n### Step 1.  Set up a Collector and Source\\r\\n\\r\\nSet up a Custom HTTP Collector and Source in SumoLogic (copy the source URL) using the SumoLogic web interface; see the documentation here:  [https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Data-to-an-HTTP-Source](https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Data-to-an-HTTP-Source).\\r\\n\\r\\n\\r\\n### Step 2.  Create Netlify Environment Variable(s)\\r\\n\\r\\nCreate a build time environment variable in Netlify for the Sumo Source URL you generated in Step 1 (in this case, I have called it `SUMO_REQUEST_SOURCE_URL`).\\r\\n\\r\\n### Step 3.  Create a logging module\\r\\n\\r\\nCreate your logging module (this example is in TypeScript, but you could implement the same logic in any other language/runtime).  In this case, we will unpack the request event and send the fields to the collector source.  \\r\\n\\r\\n<Gist id=\\"7b218eee7984c81302ac59e15279c72b\\"\\r\\n/>\\r\\n\\r\\n### Step 4.  Import and use the logging module\\r\\n\\r\\nNow import and use the `logRequest` function at the beginning of each Netlify function you deploy.  \\r\\n\\r\\n<Gist id=\\"f0e6c21b256a9fa72cb40d9f09084a8d\\"\\r\\n/>\\r\\n\\r\\nYou will now get verbose logs (similar to CDN logs) to the Netlify console and to Sumo Logic!  \\r\\n\\r\\nYou can implement similar functions for each request to log the `jwtId` and `userInfo` from the Bearer Token.  This will allow you to see the user information for each request in SumoLogic.  \\r\\n\\r\\n\\r\\n:::info\\r\\n\\r\\nYour http response header should include:  \\r\\n\\r\\n```typescript\\r\\nheaders: {\\r\\n  \'Access-Control-Allow-Origin\': \'*\',\\r\\n  \'Access-Control-Allow-Headers\': \'Content-Type\',\\r\\n  ...\\r\\n},\\r\\n```\\r\\n\\r\\n:::\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"enhancing-dbt-snapshots-with-operational-metadata","metadata":{"permalink":"/enhancing-dbt-snapshots-with-operational-metadata","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2023-01-30-enhancing-dbt-snapshots-with-operational-metadata/index.md","source":"@site/blog/2023-01-30-enhancing-dbt-snapshots-with-operational-metadata/index.md","title":"Enhancing dbt Snapshots with Operational Metadata","description":"Override default dbt snapshot macros to add operational metadata to snapshot tables while still applying dbt logic and strategy processing.","date":"2023-01-30T00:00:00.000Z","formattedDate":"January 30, 2023","tags":[{"label":"dbt","permalink":"/tags/dbt"},{"label":"snapshot","permalink":"/tags/snapshot"},{"label":"dbt snapshot","permalink":"/tags/dbt-snapshot"},{"label":"cdc","permalink":"/tags/cdc"},{"label":"change data capture","permalink":"/tags/change-data-capture"},{"label":"bigquery","permalink":"/tags/bigquery"}],"readingTime":5.19,"hasTruncateMarker":false,"authors":[{"name":"Chris Ottinger","title":"Senior Technologist","url":"https://github.com/datwiz","imageURL":"http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80","key":"chrisottinger"}],"frontMatter":{"slug":"enhancing-dbt-snapshots-with-operational-metadata","title":"Enhancing dbt Snapshots with Operational Metadata","authors":["chrisottinger"],"draft":false,"image":"/img/blog/enhancing-dbt-snapshots-with-operational-metadata.png","tags":["dbt","snapshot","dbt snapshot","cdc","change data capture","bigquery"],"keywords":["dbt","snapshot","dbt snapshot","cdc","change data capture","bigquery"],"description":"Override default dbt snapshot macros to add operational metadata to snapshot tables while still applying dbt logic and strategy processing."},"prevItem":{"title":"Netlify logging to SumoLogic Without Log Drains","permalink":"/netlify-logging-to-sumologic-without-log-drains"},"nextItem":{"title":"Yoast (like) JSON-LD Structured Data for Docusaurus","permalink":"/json-ld-structured-data-for-docusaurus"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nThe out-of-the-box [__dbt snapshots__](https://docs.getdbt.com/docs/build/snapshots) provide change data capture (CDC) capability for tracking the changes to data in your data lake or data warehouse.  The dbt snapshot metadata columns enable a view of change to data - which records \\r\\nhave been updated and when.   However, the dbt snapshot metadata doesn\'t provide a view of the processing audit - which process\\r\\nor job was responsible for the changes.  The ability to audit at the processing level requires additional operational metadata.\\r\\n\\r\\nThe out-of-the-box `dbt snapshot` strategies (rules for detecting changes) likely provide the desired logic for detecting and managing data change.\\r\\nNo change to the snapshot strategies or snapshot pipeline processing is desired, but additional operational metadata\\r\\nfields must be set and carried through with the data.\\r\\n\\r\\n:::note\\r\\n\\r\\nThe full source code for this article is available at [__github.com/datwiz/dbt-snapshot-metadata__](https://github.com/datwiz/dbt-snapshot-metadata).\\r\\n\\r\\n:::\\r\\n\\r\\n## Objectives\\r\\nBoth operational and governance requirements can drive the need for greater fidelity of operational metadata.\\r\\nExample considerations could include:\\r\\n* use of the out-of-the-box `dbt snapshot` logic and strategies for Change Data Capture (CDC)\\r\\n* addition of operational metadata fields to snapshot tables with processing details for ops support and audit\\r\\n  - when new records are inserted, add operational processing metadata information to each record\\r\\n  - when existing records are closed or end-dated, update operational metadata fields with processing metadata\\r\\n\\r\\n```mermaid\\r\\n---\\r\\ntitle: Example snapshot table out-of-the-box\\r\\n---\\r\\nerDiagram\\r\\n  BASIC_THING {\\r\\n    date txn_dt\\r\\n    int id\\r\\n    string status\\r\\n    string dbt_scd_id\\r\\n    datetime dbt_updated_at\\r\\n    datetime dbt_valid_from\\r\\n    datetime dbt_valid_to\\r\\n  }\\r\\n\\r\\n```\\r\\n![standard snapshot table](images/standard-snapshot-table.png)\\r\\n\\r\\n```mermaid\\r\\n---\\r\\ntitle: Example snapshot table with enhanced metadata\\r\\n---\\r\\nerDiagram\\r\\n  ENHANCED_THING {\\r\\n    date txn_dt\\r\\n    int id\\r\\n    string status\\r\\n    string insert_id\\r\\n    string update_id\\r\\n    timestamp start_dttm\\r\\n    timestamp end_dttm\\r\\n    timestamp insert_dttm\\r\\n    timestamp update_dttm\\r\\n    string dbt_scd_id\\r\\n    datetime dbt_updated_at\\r\\n    datetime dbt_valid_from\\r\\n    datetime dbt_valid_to\\r\\n  }\\r\\n```\\r\\n![enhanced snapshot table](images/enhanced-snapshot-table.png)\\r\\n\\r\\nAside from including a new `process_id` value in records, these enhancements don\'t add further information to the\\r\\ntable.  Instead they are a materialization of the operational data that is easier to access.  The same information\\r\\ncould be derived from standard dbt metadata fields but would require a more complex SQL statement that includes\\r\\na left outer self-join.  As with any materialization decision, there is a trade-off between ease of access\\r\\nvs. additional storage requirements.\\r\\n\\r\\n### `NULL` vs High-End Date/Timestamp\\r\\nIn addition to the ops support and audit requirements, there can also be a legacy migration complication\\r\\nrelated to how open records (the most current version of the record) are represented in snapshots.  dbt snapshots\\r\\nrepresent open records using `NULL` values for `dbt_valid_to` fields.\\r\\nIn legacy data lakes or data warehouses, the open records often are identified using a\\r\\nwell-known high value for the effective end date/timestamp, such as `9999-12-31` or `9999-12-31 23:59:59`.  Adding\\r\\nadditional snapshot metadata columns enables a legacy view of record changes without having to alter the\\r\\n`dbt snapshot` strategy or processing logic.  \\r\\n\\r\\n:::tip\\r\\n\\r\\nTransitioning to `NULL` values for the `valid_to` end date/timestamp value for open records\\r\\nis highly recommended, especially when porting to a new database platform or cloud-based service.  On-premise\\r\\nlegacy database platforms often use `TIMESTAMP` values without including timezones or timezone offsets, relying on a system-wide default timezone setting.\\r\\nDifferent databases may also have extra millisecond precision for `TIMESTAMP` columns.\\r\\nPrecision and timezone treatment can cause unexpected issues when migrating to a new database platform.\\r\\n\\r\\nFor example, in BigQuery\\r\\n```\\r\\ndatetime(\'9999-12-31 23:59:59.999999\', \'Australia/Melbourne\')\\r\\n```\\r\\nwill generate an invalid value error, while\\r\\n```\\r\\ntimestamp(\'9999-12-31 23:59:59.999999\', \'Australia/Melbourne\')\\r\\n```\\r\\nwill silently convert the localised timestamp to UTC `9999-12-31 23:59:59.999999+00`\\r\\n\\r\\nThe use of `NULL` values for open records/`valid_to` fields avoids this risk of subtle breakage.\\r\\n\\r\\n:::\\r\\n\\r\\n## Enhancing the default Snapshot\\r\\nModify the default dbt snapshot behavior by overriding the [dbt snapshot materialization macros](https://github.com/dbt-labs/dbt-core/tree/main/core/dbt/include/global_project/macros/materializations/snapshots).\\r\\ndbt enbles macros to be overridden using the following resolution or search order:\\r\\n1.  locally defined macros in the project\'s ./macros directory\\r\\n2.  macros defined in additional dbt packages included in the project `packages.yml` file\\r\\n3.  dbt adaptor-specific macros\\r\\n3.  dbt provided default macros\\r\\n\\r\\nTo inject additional snapshot metadata fields into snapshot tables override the following two default macros:\\r\\n* `default__build_snapshot_table()` creates the snapshot table on the first run\\r\\n* `default__snapshot_staging_table()` stages in the inserts and updates to be applied to the snapshot table\\r\\n\\r\\nTo update fields on snapshot update, override the following default macro:\\r\\n* `default__snapshot_merge_sql()` performs the MERGE/UPSERT \\r\\n\\r\\nNote that if the dbt database adaptor implements adaptor-specific versions of these macros, then update\\r\\nthe adaptor-specific macro accordingly.  For example the [dbt-spark](https://github.com/dbt-labs/dbt-spark/blob/main/dbt/include/spark/macros/materializations/snapshot.sql) adaptor overrides the\\r\\ndbt `default__snapshot_merge_sql()` as `spark__snapshot_merge_sql()`.\\r\\n\\r\\n### `build_snapshot_table()`\\r\\nThe [default__build_snapshot_table()](https://github.com/datwiz/dbt-snapshot-metadata/tree/main/dbt_snapshot_ops_metadata/macros/default__build_snapshot_table.sql) macro is called on the first `dbt snapshot` invocation.  This\\r\\nmacro defines the content to include in the `CREATE TABLE` statement.  The following example adds \\r\\nprocess id\'s using the dbt `invocation_id` and additional timestamp fields, including use of the\\r\\nwell-known high timestamp value for open records.  This value is defined as the variable `default_high_dttm`\\r\\nin the `dbt_project.yml` file.  The `dbt snapshot` strategy processing uses the unmodified\\r\\nstandard dbt columns, so modification to change detection logic is not required.\\r\\n\\r\\n<Gist id=\\"750d131f1e4c3891d1a358b3e655574e\\"\\r\\n/>\\r\\n\\r\\n### `snapshot_staging_table()`\\r\\nThe [default__snapshot_staging_table()](https://github.com/datwiz/dbt-snapshot-metadata/tree/main/dbt_snapshot_ops_metadata/macros/default__snapshot_staging_table.sql) macro is called on subsequent `dbt snapshot` invocations.  This macro\\r\\ndefines the content in the `MERGE` statement for inserts and updates.  The following example adds\\r\\nthe additional operational metadata fields to the `insertions` common table expression (CTE) and the `updates` (CTE).\\r\\nThe dbt `invocation_id` is used again as the `process_id` for inserts on new records and updates that\\r\\nclose existing records.\\r\\n\\r\\n<Gist id=\\"e4601e9d2cb4a1186390d47834d570f8\\"\\r\\n/>\\r\\n\\r\\nNote that the `deletes` CTE has not been updated with the additional fields.  In scenarios that use the \\r\\nhard deletes feature, the `deletes` CTE would need to be modified as well.\\r\\n\\r\\n### `snapshot_merge_sql()`\\r\\nThe [default__snapshot_merge_sql()](https://github.com/datwiz/dbt-snapshot-metadata/tree/main/dbt_snapshot_ops_metadata/macros/default__snapshot_merge_sql.sql) macro is called to perform the `MERGE`/`UPSERT` into the target snapshot table.  This macro defines how fields in the records being closed should be updated.  The `update set`\\r\\nsection of the `MERGE` statement defines the updated columns and values.\\r\\n\\r\\n<Gist id=\\"d8b3180a11e7cf334ef7be3ac75429d9\\"\\r\\n/>\\r\\n\\r\\n## Conclusion\\r\\nOverriding the default dbt snapshot macros enables the injection and updating of additional operational\\r\\nmetadata in snapshot tables.  Fields can be added such that the provided dbt logic and snapshot\\r\\nstrategy processing is still applied. Still, the resulting snapshot tables contain the columns required\\r\\nfor the data lake or data warehouse.\\r\\n\\r\\nThe sample dbt project in [__datwiz/dbt-snapshot-metadata/tree/main/dbt_snapshot_ops_metadata__](https://github.com/datwiz/dbt-snapshot-metadata/tree/main/dbt_snapshot_ops_metadata) contains an implementation of the snapshot customization."},{"id":"json-ld-structured-data-for-docusaurus","metadata":{"permalink":"/json-ld-structured-data-for-docusaurus","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2023-01-25-json-ld-structured-data-for-docusaurus/index.md","source":"@site/blog/2023-01-25-json-ld-structured-data-for-docusaurus/index.md","title":"Yoast (like) JSON-LD Structured Data for Docusaurus","description":"This article demonstrates the use of the docusaurus-plugin-structured-data plugin to add JSON-LD structured data to sites built with docusaurus for improved on-page SEO and rich results.","date":"2023-01-25T00:00:00.000Z","formattedDate":"January 25, 2023","tags":[{"label":"structured data","permalink":"/tags/structured-data"},{"label":"docusaurus","permalink":"/tags/docusaurus"},{"label":"seo","permalink":"/tags/seo"},{"label":"json-ld","permalink":"/tags/json-ld"},{"label":"yoast","permalink":"/tags/yoast"}],"readingTime":5.05,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"json-ld-structured-data-for-docusaurus","title":"Yoast (like) JSON-LD Structured Data for Docusaurus","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/json-ld-structured-data-for-docusaurus.png","tags":["structured data","docusaurus","seo","json-ld","yoast"],"keywords":["structured data","docusaurus","seo","json-ld","yoast"],"description":"This article demonstrates the use of the docusaurus-plugin-structured-data plugin to add JSON-LD structured data to sites built with docusaurus for improved on-page SEO and rich results."},"prevItem":{"title":"Enhancing dbt Snapshots with Operational Metadata","permalink":"/enhancing-dbt-snapshots-with-operational-metadata"},"nextItem":{"title":"Introducing GitVer - an alternative versioning scheme","permalink":"/gitver-an-alternative-versioning-scheme-to-semver-or-calver"}},"content":"[Yoast](https://developer.yoast.com/) is a well-known SEO plugin for WordPress which automagically generates structured data for every page (amongst other things). This helps render rich results for search as well as improve general on-site SEO.  \\r\\n\\r\\nWe use [Docusaurus](https://docusaurus.io/), a React-based Static Site Generator from Facebook, for all of our docs and blog properties. Docusaurus does have some native structured data capabilities through [Microdata](https://html.spec.whatwg.org/multipage/microdata.html#microdata). We were after:  \\r\\n\\r\\n- Structured data implemented using [JSON-LD](https://json-ld.org/) - which is the recommended approach by Google; and\\r\\n- Support multi-level structured data (like Yoast does), including `Organization`, `WebSite`, `WebPage`, `Article`, and `Breadcrumb` level data\\r\\n\\r\\nThe solution was to create a Docusaurus plugin, [__docusaurus-plugin-structured-data__](https://github.com/stackql/docusaurus-plugin-structured-data).  \\r\\n\\r\\n:::info\\r\\n\\r\\nGoogle allows you to combine structured data in Microdata format with data in JSON-LD format. You can see the union of the two markup approaches using the [Rich Results Test](https://search.google.com/test/rich-results).\\r\\n\\r\\n:::\\r\\n\\r\\n## How it works\\r\\n\\r\\n`Organization` and `Website` level structured data are defined in the plugin configurations (see [Configuration](#configuration)). `WebPage`, `Article` and `Breadcrumb` level data are derived for each page from metadata sourced from the [`postBuild` lifecycle api](https://docusaurus.io/docs/api/plugin-methods/lifecycle-apis#postBuild) and then injected into the `<head>` of each page using JSON-LD format as follows:  \\r\\n\\r\\n```html\\r\\n<head>\\r\\n...\\r\\n<script type=\\"application/ld+json\\">\\r\\n{\\"@context\\":\\"https://schema.org\\",\\"@graph\\":[...]}\\r\\n<\/script>\\r\\n...\\r\\n</head>\\r\\n```\\r\\n\\r\\n### Breadcrumbs\\r\\n\\r\\nDocusaurus allows you to create hierarchical document structures using categories and folders defined at build time; although this is useful for organization and context, to search engines, it can appear too complex (with leaf-level documents seemingly multiple clicks from the home page). In actuality, this is not the case, as the sidebar in Docusuarus makes any page one click away from the home page.  \\r\\n\\r\\nAs a solution (to keep the hierarchy), the plugin takes each level in the route, maps it to a friendly term or token (using the `breadCrumbLabelMap` configuration property), and creates a concatenated string, so for a route such as:  \\r\\n\\r\\n```\\r\\n/docs/language-spec/functions/aggregate/group_concat\\r\\n```\\r\\n\\r\\nThe resultant `Breadcrumb` structured data looks like this...  \\r\\n\\r\\n```json\\r\\n    {\\r\\n      \\"@type\\": \\"BreadcrumbList\\",\\r\\n      \\"@id\\": \\"https://stackql.io/docs/language-spec/functions/aggregate/group_concat/#breadcrumb\\",\\r\\n      \\"itemListElement\\": [\\r\\n        {\\r\\n          \\"@type\\": \\"ListItem\\",\\r\\n          \\"position\\": 1,\\r\\n          \\"item\\": \\"https://stackql.io\\",\\r\\n          \\"name\\": \\"Home\\"\\r\\n        },\\r\\n        {\\r\\n          \\"@type\\": \\"ListItem\\",\\r\\n          \\"position\\": 2,\\r\\n          \\"item\\": \\"https://stackql.io/docs\\",\\r\\n          \\"name\\": \\"Documentation\\"\\r\\n        },\\r\\n        {\\r\\n          \\"@type\\": \\"ListItem\\",\\r\\n          \\"position\\": 3,\\r\\n          \\"name\\": \\"Language Specification - Functions - Aggregate - GROUP_CONCAT\\"\\r\\n        }\\r\\n      ]\\r\\n    },\\r\\n```\\r\\n\\r\\n### Blog Posts\\r\\n\\r\\nThe [__docusaurus-plugin-structured-data__](https://github.com/stackql/docusaurus-plugin-structured-data) plugin detects blog posts and injects [Article](https://schema.org/Article) structured data accordingly, including the following properties:  \\r\\n\\r\\n- `author`\\r\\n- `headline`\\r\\n- `datePublished`\\r\\n- `dateModified`\\r\\n- `wordCount`\\r\\n- `keywords`\\r\\n- and more...\\r\\n\\r\\nAn example is shown here:  \\r\\n\\r\\n```json\\r\\n    {\\r\\n      \\"@type\\": \\"Article\\",\\r\\n      \\"@id\\": \\"https://stackql.io/blog/sumologic-provider-for-stackql-now-available/#article\\",\\r\\n      \\"isPartOf\\": {\\r\\n        \\"@type\\": \\"WebPage\\",\\r\\n        \\"@id\\": \\"https://stackql.io/blog/sumologic-provider-for-stackql-now-available/#webpage\\"\\r\\n      },\\r\\n      \\"author\\": {\\r\\n        \\"name\\": \\"Jeffrey Aven\\",\\r\\n        \\"@id\\": \\"https://stackql.io/#/schema/person/1\\"\\r\\n      },\\r\\n      \\"headline\\": \\"Sumologic Provider for StackQL Now Available\\",\\r\\n      \\"datePublished\\": \\"2023-01-03T00:00:00.000Z\\",\\r\\n      \\"dateModified\\": \\"2023-01-03T00:00:00.000Z\\",\\r\\n      \\"mainEntityOfPage\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/blog/sumologic-provider-for-stackql-now-available/#webpage\\"\\r\\n      },\\r\\n      \\"wordCount\\": 201,\\r\\n      \\"publisher\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/#organization\\"\\r\\n      },\\r\\n      \\"image\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/blog/sumologic-provider-for-stackql-now-available/#primaryimage\\"\\r\\n      },\\r\\n      \\"thumbnailUrl\\": \\"https://stackql.io/img/blog/stackql-sumologic-provider-featured-image.png\\",\\r\\n      \\"keywords\\": [\\"stackql\\", \\"sumologic\\", \\"multicloud\\", \\"monitoring\\", \\"logging\\"],\\r\\n      \\"articleSection\\": [\\"Blog\\"],\\r\\n      \\"inLanguage\\": \\"en-US\\"\\r\\n    }\\r\\n```\\r\\n\\r\\n## Installation\\r\\n\\r\\nThe [__docusaurus-plugin-structured-data__](https://github.com/stackql/docusaurus-plugin-structured-data) is available via NPMJS at [__@stackql/docusaurus-plugin-structured-data__](https://www.npmjs.com/package/@stackql/docusaurus-plugin-structured-data).  \\r\\n\\r\\nTo install via NPM use:  \\r\\n\\r\\n```\\r\\nnpm i @stackql/docusaurus-plugin-structured-data\\r\\n```\\r\\n\\r\\nTo install using Yarn use:\\r\\n\\r\\n```\\r\\nyarn add @stackql/docusaurus-plugin-structured-data\\r\\n```\\r\\n\\r\\n## Configuration\\r\\n\\r\\nAdd the [__docusaurus-plugin-structured-data__](https://github.com/stackql/docusaurus-plugin-structured-data) plugin to `plugins` section in `docusaurus.config.js`:\\r\\n\\r\\n```javascript\\r\\n{\\r\\n  plugins: [\\r\\n    \'@stackql/docusaurus-plugin-structured-data\',\\r\\n    ...\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\nUpdate `themeConfig` in the `docusaurus.config.js` file, the following shows mandatory properties:  \\r\\n\\r\\n```javascript\\r\\n{\\r\\n  ...,\\r\\n  themeConfig: {\\r\\n    structuredData: {\\r\\n      excludedRoutes: [], // array of routes to exclude from structured data generation, include custom redirects here\\r\\n      verbose: boolean, // print verbose output to console (default: false)\\r\\n      featuredImageDimensions: {\\r\\n        width: 1200,\\r\\n        height: 630,\\r\\n      },\\r\\n      authors:{\\r\\n          author_name: {\\r\\n            authorId: string, // unique id for the author - used as an identifier in structured data\\r\\n            url: string, // MUST be the same as the `url` property in the `authors.yml` file in the `blog` directory\\r\\n            imageUrl: string, // gravatar url\\r\\n            sameAs: [] // synonymous entity links, e.g. github, linkedin, twitter, etc.\\r\\n          },\\r\\n      },\\r\\n      organization: {}, // Organization properties can be added to this object\\r\\n      website: {}, // WebSite properties can be added to this object\\r\\n      webpage: {\\r\\n        datePublished: string, // default is the current date\\r\\n        inLanguage: string, // default: en-US\\r\\n      },\\r\\n      breadcrumbLabelMap: {} // used to map the breadcrumb labels to a custom value\\r\\n      }\\r\\n    },\\r\\n    ...\\r\\n  }\\r\\n```\\r\\n\\r\\n## Resultant Structured Data Example\\r\\n\\r\\nBelow is an example of the data created and injected into the `<head>` of each page in the generated site (this is formatted for readability - the actual structured data generated is minified for performance).    \\r\\n\\r\\n<details>\\r\\n  <summary>Docusaurus Structured Data Example</summary>\\r\\n\\r\\n```html\\r\\n<script type=\\"application/ld+json\\">\\r\\n{\\r\\n  \\"@context\\": \\"https://schema.org\\",\\r\\n  \\"@graph\\": [\\r\\n    {\\r\\n      \\"@type\\": \\"WebPage\\",\\r\\n      \\"isPartOf\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/#website\\"\\r\\n      },\\r\\n      \\"inLanguage\\": \\"en-US\\",\\r\\n      \\"datePublished\\": \\"2021-07-01\\",\\r\\n      \\"@id\\": \\"https://stackql.io/docs/language-spec/functions/json/json_extract/#webpage\\",\\r\\n      \\"url\\": \\"https://stackql.io/docs/language-spec/functions/json/json_extract\\",\\r\\n      \\"name\\": \\"JSON_EXTRACT\\",\\r\\n      \\"description\\": \\"Query and Deploy Cloud Infrastructure and Resources using SQL\\",\\r\\n      \\"dateModified\\": \\"2023-01-23T23:56:08.545Z\\",\\r\\n      \\"breadcrumb\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/docs/language-spec/functions/json/json_extract/#breadcrumb\\"\\r\\n      },\\r\\n      \\"potentialAction\\": [\\r\\n        {\\r\\n          \\"@type\\": \\"ReadAction\\",\\r\\n          \\"target\\": [\\r\\n            \\"https://stackql.io/docs/language-spec/functions/json/json_extract\\"\\r\\n          ]\\r\\n        }\\r\\n      ]\\r\\n    },\\r\\n    {\\r\\n      \\"@type\\": \\"BreadcrumbList\\",\\r\\n      \\"@id\\": \\"https://stackql.io/docs/language-spec/functions/json/json_extract/#breadcrumb\\",\\r\\n      \\"itemListElement\\": [\\r\\n        {\\r\\n          \\"@type\\": \\"ListItem\\",\\r\\n          \\"position\\": 1,\\r\\n          \\"item\\": \\"https://stackql.io\\",\\r\\n          \\"name\\": \\"Home\\"\\r\\n        },\\r\\n        {\\r\\n          \\"@type\\": \\"ListItem\\",\\r\\n          \\"position\\": 2,\\r\\n          \\"item\\": \\"https://stackql.io/docs\\",\\r\\n          \\"name\\": \\"Documentation\\"\\r\\n        },\\r\\n        {\\r\\n          \\"@type\\": \\"ListItem\\",\\r\\n          \\"position\\": 3,\\r\\n          \\"name\\": \\"Language Specification - Functions - JSON - JSON_EXTRACT\\"\\r\\n        }\\r\\n      ]\\r\\n    },\\r\\n    {\\r\\n      \\"@type\\": \\"WebSite\\",\\r\\n      \\"@id\\": \\"https://stackql.io/#website\\",\\r\\n      \\"name\\": \\"StackQL\\",\\r\\n      \\"url\\": \\"https://stackql.io\\",\\r\\n      \\"description\\": \\"Provision and Query Cloud and SaaS Resources using SQL\\",\\r\\n      \\"publisher\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/#organization\\"\\r\\n      },\\r\\n      \\"potentialAction\\": [\\r\\n        {\\r\\n          \\"@type\\": \\"SearchAction\\",\\r\\n          \\"target\\": {\\r\\n            \\"@type\\": \\"EntryPoint\\",\\r\\n            \\"urlTemplate\\": \\"https://stackql.io/search?q={searchTerms}\\"\\r\\n          },\\r\\n          \\"query-input\\": \\"required name=searchTerms\\"\\r\\n        }\\r\\n      ],\\r\\n      \\"inLanguage\\": \\"en-US\\"\\r\\n    },\\r\\n    {\\r\\n      \\"@type\\": \\"Organization\\",\\r\\n      \\"@id\\": \\"https://stackql.io/#organization\\",\\r\\n      \\"name\\": \\"StackQL\\",\\r\\n      \\"url\\": \\"https://stackql.io\\",\\r\\n      \\"sameAs\\": [\\r\\n        \\"https://twitter.com/stackql\\",\\r\\n        \\"https://www.linkedin.com/company/stackql\\",\\r\\n        \\"https://github.com/stackql\\",\\r\\n        \\"https://www.youtube.com/@stackql\\",\\r\\n        \\"https://hub.docker.com/u/stackql\\"\\r\\n      ],\\r\\n      \\"contactPoint\\": {\\r\\n        \\"@type\\": \\"ContactPoint\\",\\r\\n        \\"email\\": \\"info@stackql.io\\"\\r\\n      },\\r\\n      \\"logo\\": {\\r\\n        \\"@type\\": \\"ImageObject\\",\\r\\n        \\"inLanguage\\": \\"en-US\\",\\r\\n        \\"@id\\": \\"https://stackql.io/#logo\\",\\r\\n        \\"url\\": \\"https://stackql.io/img/stackql-cover.png\\",\\r\\n        \\"contentUrl\\": \\"https://stackql.io/img/stackql-cover.png\\",\\r\\n        \\"width\\": 1440,\\r\\n        \\"height\\": 900,\\r\\n        \\"caption\\": \\"StackQL - your cloud using SQL\\"\\r\\n      },\\r\\n      \\"image\\": {\\r\\n        \\"@id\\": \\"https://stackql.io/#logo\\"\\r\\n      },\\r\\n      \\"address\\": {\\r\\n        \\"@type\\": \\"PostalAddress\\",\\r\\n        \\"addressCountry\\": \\"AU\\",\\r\\n        \\"postalCode\\": \\"3001\\",\\r\\n        \\"streetAddress\\": \\"Level 24, 570 Bourke Street, Melbourne, Victoria\\"\\r\\n      },\\r\\n      \\"duns\\": \\"750469226\\",\\r\\n      \\"taxID\\": \\"ABN 65 656 147 054\\"\\r\\n    }\\r\\n  ]\\r\\n}\\r\\n<\/script>\\r\\n```\\r\\n\\r\\n</details>\\r\\n\\r\\n## Testing\\r\\n\\r\\nOnce you have built and deployed your site (using `yarn build`), you can use the [Schema Validator Tool](https://validator.schema.org/) or the [Google Rich Results Tool](https://search.google.com/test/rich-results) to inspect urls from your site.  \\r\\n\\r\\nPull requests or issues are welcome.  Please feel free to contribute. Thanks!    \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"gitver-an-alternative-versioning-scheme-to-semver-or-calver","metadata":{"permalink":"/gitver-an-alternative-versioning-scheme-to-semver-or-calver","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2023-01-18-gitver-an-alternative-versioning-scheme-to-semver-or-calver/index.md","source":"@site/blog/2023-01-18-gitver-an-alternative-versioning-scheme-to-semver-or-calver/index.md","title":"Introducing GitVer - an alternative versioning scheme","description":"This article demonstrates the use of Deno Deploy to serve static text files as non-traditional development artifacts.","date":"2023-01-18T00:00:00.000Z","formattedDate":"January 18, 2023","tags":[{"label":"gitver","permalink":"/tags/gitver"},{"label":"semver","permalink":"/tags/semver"},{"label":"calver","permalink":"/tags/calver"},{"label":"gitops","permalink":"/tags/gitops"},{"label":"versioning","permalink":"/tags/versioning"},{"label":"git","permalink":"/tags/git"},{"label":"github","permalink":"/tags/github"},{"label":"gitlab","permalink":"/tags/gitlab"},{"label":"bitbucket","permalink":"/tags/bitbucket"},{"label":"gitflow","permalink":"/tags/gitflow"}],"readingTime":1.84,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"gitver-an-alternative-versioning-scheme-to-semver-or-calver","title":"Introducing GitVer - an alternative versioning scheme","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/gitver-an-alternative-versioning-scheme-to-semver-or-calver.png","tags":["gitver","semver","calver","gitops","versioning","git","github","gitlab","bitbucket","gitflow"],"keywords":["gitver","semver","calver","gitops","versioning","git","github","gitlab","bitbucket","gitflow"],"description":"This article demonstrates the use of Deno Deploy to serve static text files as non-traditional development artifacts."},"prevItem":{"title":"Yoast (like) JSON-LD Structured Data for Docusaurus","permalink":"/json-ld-structured-data-for-docusaurus"},"nextItem":{"title":"Use Deno Deploy to Serve Non-Traditional Artifacts","permalink":"/use-deno-deploy-to-serve-non-traditional-artifacts"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nWith our StackQL Provider Registry, we had an interesting challenge:  \\r\\n\\r\\n1.  Maintain different versions for one or more different documents in the same repo(which were decoupled from releases)\\r\\n2.  Provide dynamic versioning (with no user input required and not dictated by tags)\\r\\n3.  Maintain some traceability to the source repo (pull requests, commit shas, etc)\\r\\n\\r\\n[SemVer](https://semver.org/) required users to make arbitrary decisions on major, minor, and build numbers.  \\r\\n\\r\\nAlthough [CalVer](https://calver.org/) required less user discretion for the major and minor components, the micro-component was still an arbitrary number.  This was not ideal for our use case.  \\r\\n\\r\\nAs our document versioning was not related to tags, and we have implemented GitFlow (specifically based upon PRs to dev or main) as our release path, we created a new variant scheme... __GitVer__.  \\r\\n\\r\\n> This is completely different from [GitVersion](https://github.com/GitTools/GitVersion), which is a tool to determine the version of a project based on Git history.  \\r\\n\\r\\nThis scheme is implemented using GitHub as the remote but could easily be adapted to GitLab, Bitbucket, etc.  \\r\\n\\r\\n## How it works\\r\\n\\r\\nEach pull request is assigned a version based on the date the PR was raised or merged, and the PR number.   This version (the GitVer) can then be used to version artifacts (which could be pushed to releases if desired).  \\r\\n\\r\\n\\r\\n## Workflow Example Code\\r\\n\\r\\nThis is an example using GitHub actions.  The version is determined automatically within the workflow.  \\r\\n\\r\\n`main.yml` example:  \\r\\n\\r\\n<Gist id=\\"94bcf43ba5deaf088a271d54d2a9c33e\\"\\r\\n/>\\r\\n\\r\\nThe code used to get the relevant PR info is here (`setup-job.js`), the tricky bit is that the PR number presents differently for a pull request open or sync (pushing changes to an open PR) and a merge commit (which is simply a push to a protetcted branch).  See the code below:\\r\\n\\r\\n<Gist id=\\"bec7982143a9637f866b7239e8c18130\\"\\r\\n/>\\r\\n\\r\\n:::tip\\r\\n\\r\\nyou can export some other metadata while you are here like the commit sha, source and target branch, (PR) action, etc.\\r\\n\\r\\n:::\\r\\n\\r\\nThe code to generate the GitVer for the PR is here (`get-version.js`):  \\r\\n\\r\\n<Gist id=\\"659b56d2474ed1fb8944e1c816b28f49\\"\\r\\n/>\\r\\n\\r\\nYou can see it at work here [stackql/stackql-provider-registry](https://github.com/stackql/stackql-provider-registry/blob/dev/.github/workflows/main.yml) which builds and deploys [providers for StackQL](https://registry.stackql.io/).   \\r\\n\\r\\nThoughts?\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"use-deno-deploy-to-serve-non-traditional-artifacts","metadata":{"permalink":"/use-deno-deploy-to-serve-non-traditional-artifacts","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-12-31-use-deno-deploy-to-serve-non-traditional-artifacts/index.md","source":"@site/blog/2022-12-31-use-deno-deploy-to-serve-non-traditional-artifacts/index.md","title":"Use Deno Deploy to Serve Non-Traditional Artifacts","description":"This article demonstrates the use of Deno Deploy to serve static text files as non-traditional development artifacts.","date":"2022-12-31T00:00:00.000Z","formattedDate":"December 31, 2022","tags":[{"label":"deno deploy","permalink":"/tags/deno-deploy"},{"label":"deno","permalink":"/tags/deno"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"typescript","permalink":"/tags/typescript"},{"label":"api","permalink":"/tags/api"},{"label":"openapi","permalink":"/tags/openapi"},{"label":"stackql","permalink":"/tags/stackql"}],"readingTime":0.705,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"use-deno-deploy-to-serve-non-traditional-artifacts","title":"Use Deno Deploy to Serve Non-Traditional Artifacts","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/use-deno-deploy-to-serve-non-traditional-artifacts.png","tags":["deno deploy","deno","javascript","typescript","api","openapi","stackql"],"keywords":["deno deploy","deno","javascript","typescript","api","openapi","stackql"],"description":"This article demonstrates the use of Deno Deploy to serve static text files as non-traditional development artifacts."},"prevItem":{"title":"Introducing GitVer - an alternative versioning scheme","permalink":"/gitver-an-alternative-versioning-scheme-to-semver-or-calver"},"nextItem":{"title":"Apache Beam in Five Minutes","permalink":"/apache-beam-in-five-minutes"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nIn the [__stackql__](https://github.com/stackql/stackql) project we needed an API to serve configuration file packages (stackql providers) to the stackql application at runtime.  \\r\\n\\r\\nTraditional artifact repositories or package managers were unsuitable as they were mainly designed for container images, JavaScript modules, Python packages etc.  The artifacts, in this case, are signed tarball sets of OpenAPI specification documents (text files).  \\r\\n\\r\\nWe have recently moved our provider registry ([__stackql-provider-registry__](https://github.com/stackql/stackql-provider-registry)) to use [__Deno Deploy__](https://deno.com/deploy) as the serving layer (the API).  \\r\\n\\r\\n## The code\\r\\n\\r\\nThe code is reasonably straightforward as shown here:  \\r\\n\\r\\n<Gist id=\\"8747fa98e61e411373a95dbe2e261bd6\\"\\r\\n/>\\r\\n\\r\\n## The deployment\\r\\n\\r\\nWe are using GitHub Actions to push assets and code to Deno Deploy, this was straightforward as well as you can see here:  \\r\\n\\r\\n<Gist id=\\"e4e98f427c80ecee4f82fb8731565289\\"\\r\\n/>\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"apache-beam-in-five-minutes","metadata":{"permalink":"/apache-beam-in-five-minutes","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-12-12-apache-beam-in-five-minutes/index.md","source":"@site/blog/2022-12-12-apache-beam-in-five-minutes/index.md","title":"Apache Beam in Five Minutes","description":"This article provides a quick introduction to Apache Beam - the batch and stream ETL programming model and SDK used by Google Cloud Dataflow.","date":"2022-12-12T00:00:00.000Z","formattedDate":"December 12, 2022","tags":[{"label":"google","permalink":"/tags/google"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"beam","permalink":"/tags/beam"},{"label":"apache beam","permalink":"/tags/apache-beam"},{"label":"dataflow","permalink":"/tags/dataflow"},{"label":"google cloud dataflow","permalink":"/tags/google-cloud-dataflow"},{"label":"etl","permalink":"/tags/etl"},{"label":"stream processing","permalink":"/tags/stream-processing"}],"readingTime":4.11,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"apache-beam-in-five-minutes","title":"Apache Beam in Five Minutes","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/apache-beam-in-five-minutes.png","tags":["google","gcp","beam","apache beam","dataflow","google cloud dataflow","etl","stream processing"],"keywords":["google","gcp","beam","apache beam","dataflow","google cloud dataflow","etl","stream processing"],"description":"This article provides a quick introduction to Apache Beam - the batch and stream ETL programming model and SDK used by Google Cloud Dataflow."},"prevItem":{"title":"Use Deno Deploy to Serve Non-Traditional Artifacts","permalink":"/use-deno-deploy-to-serve-non-traditional-artifacts"},"nextItem":{"title":"AWS IAM vs Google IAM","permalink":"/aws-iam-vs-google-iam"}},"content":"Apache Beam is an open-source project which provides a unified programming model for Batch and Streaming data pipelines.  \\r\\n\\r\\n__B__(*atch*) __+__ *str*(__EAM__) __=>__ __BEAM__  \\r\\n\\r\\n## Beam SDK and Execution Framework\\r\\n\\r\\nBeam SDKs allow you to define __Pipelines__ (in languages such as Java or Python).  A pipeline is essentially a graph (a DAG - Directed Acyclic Graph) of nodes that represent transformation steps.  \\r\\n\\r\\n[![Apache Beam Pipeline](images/apache_beam_pipeline.png)](images/apache_beam_pipeline.png)\\r\\n\\r\\nPipelines can then be executed on a backend service (such as your local machine, Apache Spark, or Google Cloud Dataflow) using __Runners__.  For instance, to run a pipeline locally, you would use the DirectRunner; to run a pipeline on Google Cloud Dataflow you would use the DataflowRunner runner.  \\r\\n\\r\\n## Beam Programming Model\\r\\n\\r\\nThe __PCollection__ is the most atomic data unit in the Beam programming model, akin to the RDD in the Apache Spark core API; it is a representation of an immutable collection of items that is physically broken down into __bundles__ (subsets of elements for parallelization).  \\r\\n\\r\\nPCollections can be *bounded* (which is a batch processing pattern) or *unbounded* (which is a stream processing pattern).  \\r\\n\\r\\nA __PTransform__ is an operator that takes a PCollection as an input and outputs a new PCollection with transforms applied.  This is the same coarse-grained transformation pattern employed by Spark.  \\r\\n\\r\\n## Beam DSL\\r\\n\\r\\nThe Beam DSL is a set of higher-order functions that can be used to construct pipelines.  These functions are used to construct the graph of nodes that represent the pipeline.  \\r\\n\\r\\n### Map, FlatMap and Filter\\r\\n\\r\\nThe basic __Map__, __FlatMap__ and __Filter__ functions in the Beam API work similarly to their namesakes in the Spark Core API.  The Map and FlatMap functions are higher-order functions (that is, functions that have arguments of other functions) that operate on each element in a collection, emitting an output element for each input element.  The Filter function can be used to only emit elements from an input PCollection that satisfy a given expression.  \\r\\n\\r\\n### ParDo and DoFn\\r\\n\\r\\n__ParDo__ is a wrapper function for parallel execution of a user-defined function called a __DoFn__ (\\"do function\\"), ParDo\'s and DoFn\'s are used when the basic Map and FlatMap operators are not enough.  DoFns are executed in parallel on a PCollection and can be used for computational transformations or transformations other than 1:1 between inputs and outputs.  \\r\\n\\r\\nThink of these as user-defined functions to operate on a PCollection in parallel.  \\r\\n\\r\\n### GroupByKey, CombineByKey\\r\\n\\r\\n__GroupByKey__ and __CombineByKey__ are operators that group data (key-value pairs) by the key for each element.  This is typically a precursor to some aggregate operation (such as a count or sum operation).  \\r\\n\\r\\n### CoGroupByKey and Flatten\\r\\n\\r\\n__CoGroupByKey__ is akin to a `JOIN` operation in SQL (by the key for each element in two PCollections).  __Flatten__ is akin to a `UNION` in SQL.  \\r\\n\\r\\n## Side Inputs\\r\\n\\r\\n__Side Inputs__ can be used with ParDo and DoFn to provide additional data, which can be used to enrich your output PCollection, or utilized within the logic of your DoFn.  \\r\\n\\r\\n## Sources, Sinks and Connectors\\r\\n\\r\\n__Sources__ represent where data is read into an Apache Beam pipeline; __sinks__ represent destinations where data is written out from pipelines.  A Beam pipeline will contain one or more sources and sinks.  \\r\\n\\r\\nSources can be bounded (for batch processing) or unbounded for stream processing.  \\r\\n\\r\\n__Connectors__ can be source connectors or sink connectors to read from or write to the various sources and targets used in a Beam pipeline.  Examples include FileIO and TextIO for working with files or text data, BigQueryIO for reading or writing into BigQuery, PubSubIO for reading and writing messages into Google PubSub, and much more.  \\r\\n\\r\\n## Streaming and Unbounded PCollections\\r\\n\\r\\nStreaming data sources are represented by Unbounded PCollections.  Unbounded PCollections support windowing operations using __Fixed Windows__, __Sliding Windows__, or __Session__ Windows.  __Watermarks__ are used to allow for late-arriving data to be processed within its associated time window, and __Triggers__ can be used to control the processing of windowed batches of data.   \\r\\n\\r\\n## Templates \\r\\n\\r\\nBeam templates enable the reusability of pipelines, converting compile-time pipeline parameters to run-time arguments.  Jobs (invocations of pipelines) can be launched from templates.  \\r\\n\\r\\n__Templates__ include classic templates, where the graph for the pipeline is built (compile-time) with the template, flex templates where the pipeline graph is created when the template is launched (runtime).  \\r\\n\\r\\nIn addition, Google provides several templates with Cloud Dataflow (Google-provided templates), allowing you to launch routine jobs without writing any code.  \\r\\n\\r\\nGoogle-provided templates are available for batch, streaming, and utility pipelines, for example:  \\r\\n\\r\\n- Kafka to BigQuery\\r\\n- Pub/Sub Topic to BigQuery\\r\\n- Text Files on Cloud Storage to BigQuery\\r\\n- Text Files on Cloud Storage to Cloud Spanner\\r\\n- Bulk Compress or Decompress Files on Cloud Storage\\r\\n- and more\\r\\n\\r\\n5 minutes is up!  I hope you enjoyed this quick introduction to Apache Beam.  If you want to learn more, check out the [Apache Beam documentation](https://beam.apache.org/documentation/).  \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"aws-iam-vs-google-iam","metadata":{"permalink":"/aws-iam-vs-google-iam","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-11-26-aws-iam-vs-google-iam/index.md","source":"@site/blog/2022-11-26-aws-iam-vs-google-iam/index.md","title":"AWS IAM vs Google IAM","description":"A brief comparison of Identity and Access Management (IAM) in the Google Cloud Platform and AWS.","date":"2022-11-26T00:00:00.000Z","formattedDate":"November 26, 2022","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"google","permalink":"/tags/google"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"iam","permalink":"/tags/iam"},{"label":"aws iam","permalink":"/tags/aws-iam"},{"label":"google iam","permalink":"/tags/google-iam"},{"label":"identity and access management","permalink":"/tags/identity-and-access-management"}],"readingTime":5.71,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"aws-iam-vs-google-iam","title":"AWS IAM vs Google IAM","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/aws-iam-vs-google-iam.png","tags":["aws","google","gcp","iam","aws iam","google iam","identity and access management"],"keywords":["aws","google","gcp","iam","aws iam","google iam","identity and access management"],"description":"A brief comparison of Identity and Access Management (IAM) in the Google Cloud Platform and AWS."},"prevItem":{"title":"Apache Beam in Five Minutes","permalink":"/apache-beam-in-five-minutes"},"nextItem":{"title":"Create and use Custom Magic Commands in Jupyter","permalink":"/create-and-use-custom-magic-commands-in-jupyter"}},"content":"AWS and Google (and Microsoft Azure) have services called IAM, which stands for __Identity and Access Management__. The IAM service serves roughly the same purpose in each provider: to authorize principals (users, groups, or service accounts) to access and use services and resources on the respective platform. There are subtle yet significant differences and distinctions across the major cloud providers.  \\r\\n\\r\\nThis article will look at the differences between IAM in AWS and IAM in Google.  \\r\\n\\r\\n## Identity Management\\r\\nFirstly, Google\'s IAM is a slight misnomer regarding the I as it does not manage identities (with the single exception of service accounts). Google identities are sourced from Google accounts created and managed outside the Google Cloud Platform. Google identities (users and groups) are Google accounts which could be accounts in a Google Workspace domain, a Google Cloud Identity domain, or Gmail accounts. Still, these accounts are *NOT* created or managed using the Google IAM service.  \\r\\n\\r\\nConversely, AWS IAM creates and manages identities for use in the AWS platform (__IAM Users__), which can be used to access AWS resources using the AWS console or programmatically using API keys.  \\r\\n\\r\\n## Overview of IAM entities in AWS and Google\\r\\nIt can be confusing for people coming from AWS to Google or vice-versa. Some of the same terms exist in both providers but mean different things. The table below summarises the difference in the meaning of terms in both providers. We will unpack this in more detail in the sections that follow.  \\r\\n\\r\\n| AWS | Google<sup>*</sup> |\\r\\n| --- | ----------- |\\r\\n| Role | [Service Account](#service-accounts-in-google) |\\r\\n| Managed Policy | [Predefined Role](#predefined-roles) |\\r\\n| Customer Managed Policy | [Custom Role](#custom-roles) |\\r\\n| Policy Attachment | [IAM Binding](#google-iam-bindings) |\\r\\n\\r\\n<sup>*</sup> nearest equivalent\\r\\n\\r\\n## Roles and Policies in AWS\\r\\nAn __AWS IAM Role__ is an identity that can be assumed by trusted entities using short-lived credentials (issued by the AWS Security Token Service or STS API). A trusted entity could be an IAM User, Group, or a service (such as Lambda or EC2).  \\r\\n\\r\\nPermissions are assigned to IAM Roles (and IAM Users and Groups) through the attachment of __IAM Policies__.  \\r\\n\\r\\nAWS Policies are collections of permissions in different services which can be used to *Allow* or *Deny* access (__Effect__); these can be scoped to a resource or have conditions attached. The following is an example of an AWS Policy:  \\r\\n\\r\\n```json\\r\\n{\\r\\n    \\"Version\\": \\"2012-10-17\\",\\r\\n    \\"Statement\\": [\\r\\n        {\\r\\n            \\"Effect\\": \\"Allow\\",\\r\\n            \\"Action\\": \\"ec2:Describe*\\",\\r\\n            \\"Resource\\": \\"*\\"\\r\\n        },\\r\\n        {\\r\\n            \\"Effect\\": \\"Allow\\",\\r\\n            \\"Action\\": \\"autoscaling:Describe*\\",\\r\\n            \\"Resource\\": \\"*\\"\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n```\\r\\n\\r\\nPolicies in AWS can be __Managed Policies__ (created and managed by AWS) or __Customer Managed Policies__ - where the customer defines and manages these policies.  \\r\\n\\r\\nAn IAM Role that is used by a service such as Lambda or EC2 will have a __Trust Policy__ attached, which will look something like this:  \\r\\n\\r\\n```json\\r\\n{\\r\\n    \\"Version\\": \\"2012-10-17\\",\\r\\n    \\"Statement\\": [\\r\\n        {\\r\\n            \\"Effect\\": \\"Allow\\",\\r\\n            \\"Principal\\": {\\r\\n                \\"Service\\": \\"lambda.amazonaws.com\\"\\r\\n            },\\r\\n            \\"Action\\": \\"sts:AssumeRole\\"\\r\\n        }\\r\\n    ]\\r\\n}\\r\\n```\\r\\n\\r\\n## Roles and Policies in Google\\r\\n__Roles__ in Google IAM are __*NOT identities*__ ; they are collections of permissions (similar to Policies in AWS). Roles can be of the following types:  \\r\\n\\r\\n### Basic Roles (also referred to as Legacy or Primitive Roles) \\r\\n__Basic Roles__ are coarse-grained permissions set at a Project level across all services, such as *Owner*, *Editor*, and *Viewer*. Support for Basic Roles is maintained by Google, however, Google does not recommend using Basic Roles after a Project is created.  \\r\\n\\r\\n### Predefined Roles\\r\\n__Predefined Roles__ are pre-curated sets of permissions that align with a role that an actor (human or service account) would play, such as *BigQuery Admin*. Predefined roles are considered best practice in Google as the permissions for these roles are maintained by Google. Predefined Roles in Google would be the nearest equivalent to Managed Policies in AWS.  \\r\\n\\r\\n### Custom Roles\\r\\n__Custom Roles__ are user-specified and managed sets of permissions. These roles are scoped within your Project in Google and are your responsibility to maintain. Custom Roles are typically used when the permissions granted through a Predefined Role are too broad. Custom Roles would be the nearest equivalent of Customer Managed Policies in AWS.  \\r\\n\\r\\n## Google IAM Bindings\\r\\nRoles (collections of permissions) are attached to Principals (Identities such as users (Google accounts), groups and service accounts through __IAM bindings__.  The example below shows a binding between a user principal (a Google Workspace account) and a predefined role (*BigQuery Admin*) within a GCP project:    \\r\\n\\r\\n[![Google IAM Binding](images/iam_binding.png)](images/iam_binding.png)\\r\\n\\r\\n## Policies in Google\\r\\nA __Policy__ in Google is a collection of IAM Bindings between *members* (principals) and *roles*. An example policy would be:  \\r\\n\\r\\n```json\\r\\n{\\r\\n  \\"bindings\\": [\\r\\n    {\\r\\n      \\"members\\": [\\r\\n        \\"user:javen@avensolutions.com\\"\\r\\n      ],\\r\\n      \\"role\\": \\"bigquery.admin\\"\\r\\n    },\\r\\n    ... another binding ...\\r\\n  ]\\r\\n}\\r\\n```\\r\\n\\r\\n## Service Accounts in Google\\r\\nA __Service Account__ in GCP is a password-less identity created in a GCP Project that can be used to access GCP resources (usually by a process or service). Service accounts are identified by an email address, but these are NOT Google accounts (like the accounts used for users or groups). Service accounts can be associated with services such as Compute Engine, Cloud Functions, or Cloud Run (in much the same way as AWS Roles can be assigned to services such as Lambda functions or EC2 instances). Google Service accounts can use keys created in the IAM service, which are exchanged for short-lived credentials, or service accounts can use get tokens directly, which include OAuth 2.0 access tokens and OpenID Connect ID tokens. Service accounts in Google are the nearest equivalent to AWS IAM Roles.  \\r\\n\\r\\n## Inheritance\\r\\nAWS (save AWS Organizations) is a flat structure with no inherent hierarchy and is oriented around regions that are seperate API endpoints (almost providers unto themselves); IAM, however, is a global service in AWS.  \\r\\n\\r\\nIn contrast, GCP is hierarchical and globally scoped for all services, including IAM. Resources (such as Google Compute Engine Instances or Big Query Datasets) are created in Projects (similar to Resource Groups in Azure). Projects are nested under a resource hierarchy, starting at the root (the organization or org). Organizations can contain folders, which can be nested, and these folders can contain Projects.  \\r\\n\\r\\nIAM Bindings (and the permissions they enable) are inherited from ancestor nodes in the GCP hierarchy. A Principal\'s net effective permissions are the union of the permissions assigned through IAM Bindings in the Project and the permissions set through IAM Bindings in all ancestor nodes (including Folders and the Org itself).  \\r\\n\\r\\n## Summary\\r\\nIAM governs access and entitlements to services and resources in cloud providers, although the design, implementation, and terminology are quite different as you get into the details. This is not to say one approach is better than the other, but as a multi-cloud warrior, you should understand the differences. \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"create-and-use-custom-magic-commands-in-jupyter","metadata":{"permalink":"/create-and-use-custom-magic-commands-in-jupyter","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-11-18-create-and-use-custom-magic-commands-in-jupyter/index.md","source":"@site/blog/2022-11-18-create-and-use-custom-magic-commands-in-jupyter/index.md","title":"Create and use Custom Magic Commands in Jupyter","description":"Quick example of creating and using a custom Jupyter magic command.","date":"2022-11-18T00:00:00.000Z","formattedDate":"November 18, 2022","tags":[{"label":"jupyter","permalink":"/tags/jupyter"},{"label":"magic","permalink":"/tags/magic"},{"label":"python","permalink":"/tags/python"},{"label":"ipython","permalink":"/tags/ipython"},{"label":"pandas","permalink":"/tags/pandas"},{"label":"postgresql","permalink":"/tags/postgresql"},{"label":"sql","permalink":"/tags/sql"},{"label":"postgres","permalink":"/tags/postgres"}],"readingTime":2.88,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"create-and-use-custom-magic-commands-in-jupyter","title":"Create and use Custom Magic Commands in Jupyter","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/custom-jupyter-magic-featured-image.png","tags":["jupyter","magic","python","ipython","pandas","postgresql","sql","postgres"],"keywords":["jupyter","magic","python","ipython","pandas","postgresql","sql","postgres"],"description":"Quick example of creating and using a custom Jupyter magic command."},"prevItem":{"title":"AWS IAM vs Google IAM","permalink":"/aws-iam-vs-google-iam"},"nextItem":{"title":"Deno in 5 Minutes","permalink":"/deno-in-five-minutes"}},"content":"We were looking to implement a variant of the `%sql` magic command in Jupyter without using the default `sqlalchemy` module (in our case, just using `psycopg2` to connect to a local server - a StackQL postrges wire protocol server).  \\r\\n\\r\\n## Create the extension module\\r\\n\\r\\nWe named our extension and cell magic command `stackql`, so start by creating a file named `stackql.py`.  We made this file in a directory name `ext` in the Jupyter working directory.    \\r\\n\\r\\n## Write the magic extension\\r\\n\\r\\nMagic commands can be __line-based__ or __cell-based__ or __line-or-cell-based__; in this example, we will use line-or-cell-based magic, meaning the decorator `%stackql` will be used to evaluate a line of code and the `%%stackql` decorator will be used to evaluate the entire contents of the cell it is used in.    \\r\\n\\r\\nThe bare-bones class and function definitions required for this extension are described below:  \\r\\n\\r\\n### Create a Magic Class\\r\\n\\r\\nWe will need to define a **magics class**, which we will use to define the magic commands.  The class name is arbitrary, but it must be a subclass of `IPython.core.magic.Magics`.  An example is below:  \\r\\n\\r\\n```python\\r\\nfrom IPython.core.magic import (Magics, magics_class, line_cell_magic)\\r\\n\\r\\n@magics_class\\r\\nclass StackqlMagic(Magics):\\r\\n\\r\\n    @line_cell_magic\\r\\n    def stackql(self, line, cell=None):\\r\\n        if cell is None:\\r\\n            # do something with line\\r\\n        else:\\r\\n            # do something with cell\\r\\n        return results\\r\\n```\\r\\n\\r\\n### Load and register the extension\\r\\n\\r\\nTo register the magic functions in the `StackqlMagic` class we created above, use a function named `load_ipython_extension`, like the following:  \\r\\n\\r\\n```python\\r\\ndef load_ipython_extension(ipython):\\r\\n    ipython.register_magics(StackqlMagic)\\r\\n```\\r\\n\\r\\n### Complete extension code\\r\\n\\r\\nThe complete code for our extension is shown here:  \\r\\n\\r\\n```python\\r\\nfrom __future__ import print_function\\r\\nimport pandas as pd\\r\\nimport psycopg2, json\\r\\nfrom psycopg2.extras import RealDictCursor\\r\\nfrom IPython.core.magic import (Magics, magics_class, line_cell_magic)\\r\\nfrom io import StringIO\\r\\nfrom string import Template\\r\\n\\r\\nconn = psycopg2.connect(\\"dbname=stackql user=stackql host=localhost port=5444\\")\\r\\n\\r\\n@magics_class\\r\\nclass StackqlMagic(Magics):\\r\\n\\r\\n    def get_rendered_query(self, data):\\r\\n        t = Template(StringIO(data).read())\\r\\n        rendered = t.substitute(self.shell.user_ns)\\r\\n        return rendered\\r\\n\\r\\n    def run_query(self, query):\\r\\n        cur = conn.cursor(cursor_factory=RealDictCursor)\\r\\n        cur.execute(query)\\r\\n        rows = cur.fetchall()\\r\\n        cur.close()\\r\\n        return pd.read_json(json.dumps(rows))\\r\\n\\r\\n    @line_cell_magic\\r\\n    def stackql(self, line, cell=None):\\r\\n        if cell is None:\\r\\n            results = self.run_query(self.get_rendered_query(line))\\r\\n        else:\\r\\n            results = self.run_query(self.get_rendered_query(cell))\\r\\n        return results            \\r\\n\\r\\ndef load_ipython_extension(ipython):\\r\\n    ipython.register_magics(StackqlMagic)\\r\\n```\\r\\n\\r\\n## Load the magic extension\\r\\n\\r\\nTo use our extension, we need to use the `%load_ext magic` command referencing the extension we created.  \\r\\n\\r\\n```python\\r\\n%load_ext ext.stackql\\r\\n```\\r\\nNote that since our extension was a file named `stackql.py` in a directory named `ext` we reference it using `ext.stackql`.   \\r\\n\\r\\n## Use the magic function in a cell\\r\\n\\r\\nTo use the magic function in a cell (operating on all contents of the cell), we use the `%%` decorator, like:\\r\\n\\r\\n```python\\r\\n%%stackql\\r\\nSHOW SERVICES IN azure\\r\\n```\\r\\n## Use the magic function on a line\\r\\n\\r\\nTo use the magic function on a line, we use the `%` decorator, like:\\r\\n\\r\\n```python\\r\\n%stackql DESCRIBE aws.ec2.instances\\r\\n```\\r\\n\\r\\n:::tip Using Variable Expansion\\r\\n\\r\\nIn our example, we implemented variable expansion using the \\"batteries included\\" String templating capabilities in Python3.  This allows for variables to be set globally in our notebooks and then used in our queries.  For example, we can set a variable in a cell like:\\r\\n\\r\\n```python\\r\\nproject = \'stackql-demo\'\\r\\nzone = \'australia-southeast1-a\'\\r\\n```\\r\\n\\r\\nThen use those variables in our queries like:  \\r\\n\\r\\n```python\\r\\n%%stackql\\r\\nSELECT status, count(*) as num_instances\\r\\nFROM google.compute.instances\\r\\nWHERE project = \'$project\' \\r\\nAND zone = \'$zone\'\\r\\nGROUP BY status\\r\\n```\\r\\n\\r\\n:::\\r\\n\\r\\nAn example is shown here:  \\r\\n\\r\\n[![Using a Custom Jupyter Magic Command](images/custom-jupyter-magic-command.png)](images/custom-jupyter-magic-command.png)\\r\\n\\r\\nThe complete code can be found at [__stackql/stackql-jupyter-demo__](https://github.com/stackql/stackql-jupyter-demo).\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"deno-in-five-minutes","metadata":{"permalink":"/deno-in-five-minutes","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-09-05-deno-in-five-minutes/index.md","source":"@site/blog/2022-09-05-deno-in-five-minutes/index.md","title":"Deno in 5 Minutes","description":"A five minute introduction to Deno, a \\"package manager-less\\" runtime for TypeScript and JavaScript.","date":"2022-09-05T00:00:00.000Z","formattedDate":"September 5, 2022","tags":[{"label":"deno","permalink":"/tags/deno"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"typescript","permalink":"/tags/typescript"},{"label":"nodejs","permalink":"/tags/nodejs"}],"readingTime":2.36,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"deno-in-five-minutes","title":"Deno in 5 Minutes","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/deno-in-five-minutes.png","tags":["deno","javascript","typescript","nodejs"],"keywords":["deno","javascript","typescript","nodejs"],"description":"A five minute introduction to Deno, a \\"package manager-less\\" runtime for TypeScript and JavaScript."},"prevItem":{"title":"Create and use Custom Magic Commands in Jupyter","permalink":"/create-and-use-custom-magic-commands-in-jupyter"},"nextItem":{"title":"DBT in 5 Minutes","permalink":"/dbt-in-five-minutes"}},"content":"Those who have built projects (front end or back end) with JavaScript or TypeScript would have no doubt felt pain or been frustrated with some aspect of package management - `package.json`, `package-lock.json`, `node_modules`, `npm`, `npmjs`, `yarn` etc.  \\r\\n\\r\\nEnter [__Deno__](https://deno.land/), a *\\"package manager-less\\"* runtime for JavaScript and TypeScript.  That\'s right, no `package.json`, no `node_modules` folder, no `npm` or `yarn`.  \\r\\n\\r\\n## Background\\r\\n\\r\\nDeno was created by Ryan Dahl, the creator of Node.js, who realised the monster that was created with package management and managers, dependencies, and dependency management, which in many cases is more complex than the frameworks or projects that are being implemented.  \\r\\n\\r\\n## Packages\\r\\n\\r\\nI said deno was *\\"package manager-less\\"*; however it is not *\\"package-less\\"*.  Deno does not use `npmjs`; instead, packages (js or ts) can be hosted at any reachable URL.  Local imports and exports are supported too.  \\r\\n\\r\\nDeno\'s standard library packages (\\"batteries included\\" modules) are hosted at [deno.land/std@LATEST_VERSION](https://deno.land/std@0.153.0), a third-party hosted library is available at [deno.land/x](https://deno.land/x).  \\r\\n\\r\\nWith `deno` installed, you can run something like this:  \\r\\n\\r\\n```\\r\\ndeno run https://deno.land/std@0.154.0/examples/welcome.ts\\r\\n```\\r\\n\\r\\n## Imports\\r\\n\\r\\nUsing the Deno runtime, developers specify modules to use in their program using this import syntax:  \\r\\n\\r\\n```js\\r\\nimport { Application, Router } from \\"https://deno.land/x/oak/mod.ts\\";\\r\\n```\\r\\n\\r\\nYou can specify a version if desired in the URL, such as:\\r\\n\\r\\n```js\\r\\nimport {\\r\\n  Bson,\\r\\n  MongoClient,\\r\\n} from \\"https://deno.land/x/mongo@v0.31.0/mod.ts\\";\\r\\n```\\r\\n\\r\\nPackages __do not__ have to be downloaded or installed before running your code.  \\r\\n\\r\\nThe first time your code is run, all packages are downloaded to a local cache and any Typescript modules are transpiled to JavaScript.  \\r\\n\\r\\n## Publishing with Deno Deploy\\r\\n\\r\\nDeno deploy is a package publishing framework that directly integrates with GitHub (no separate npm publish step).  Deno deploy is backed by a CDN and a network of edge servers to make deno packages available.  Packages can be published by a push to a branch, reviewed via a deployed preview, and merged to release to production.  \\r\\n\\r\\n## Quickstart\\r\\n\\r\\nTo get going, you first need to download and install `deno`, which will vary based upon your operating system, but there are all the usual suspect installers available (homebrew, chocolatey, etc); see [__here__](https://deno.land/#installation).  \\r\\n\\r\\nOnce you have installed `deno` on your system, you can create a project folder (__no__ `npm init` or `package.json` required), and create the following file (as `server.ts`), which will run a very simple middleware server using a third-party module, `oak`.  \\r\\n\\r\\n```js\\r\\nimport { Application, Router } from \\"https://deno.land/x/oak/mod.ts\\";\\r\\n\\r\\nconst router = new Router();\\r\\nrouter\\r\\n  .get(\\"/ping\\", (context) => {\\r\\n    context.response.body = \\"pong\\";\\r\\n}); \\r\\n\\r\\nconst app = new Application();\\r\\napp.use(router.routes());\\r\\napp.use(router.allowedMethods());\\r\\n\\r\\nawait app.listen({ port: 8080 });\\r\\n```\\r\\n\\r\\nnow run your server using the following command:\\r\\n\\r\\n```\\r\\ndeno run --allow-net server.ts\\r\\n```\\r\\n\\r\\nnow:\\r\\n\\r\\n```\\r\\ncurl -XGET http://localhost:8080/ping\\r\\n```\\r\\n\\r\\nshould return:\\r\\n\\r\\n```\\r\\npong\\r\\n```\\r\\neasy!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"dbt-in-five-minutes","metadata":{"permalink":"/dbt-in-five-minutes","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-08-05-dbt-in-five-minutes/index.md","source":"@site/blog/2022-08-05-dbt-in-five-minutes/index.md","title":"DBT in 5 Minutes","description":"This article gives a quick and straightforward introduction to dbt - the Data Build Tool - with examples using BigQuery.","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"dbt","permalink":"/tags/dbt"},{"label":"elt","permalink":"/tags/elt"},{"label":"etl","permalink":"/tags/etl"},{"label":"sql","permalink":"/tags/sql"},{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"bigquery","permalink":"/tags/bigquery"}],"readingTime":5.175,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"dbt-in-five-minutes","title":"DBT in 5 Minutes","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/dbt-in-five-minutes.png","tags":["dbt","elt","etl","sql","snowflake","bigquery"],"keywords":["dbt","elt","etl","sql","snowflake","bigquery"],"description":"This article gives a quick and straightforward introduction to dbt - the Data Build Tool - with examples using BigQuery."},"prevItem":{"title":"Deno in 5 Minutes","permalink":"/deno-in-five-minutes"},"nextItem":{"title":"Loading Parquet Files into Snowflake","permalink":"/loading-parquet-files-into-snowflake"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\n\\r\\n[__DBT (or Data Build Tool)__](https://docs.getdbt.com/) is a modern data transformation tool born in the cloud/DevOps era. It\'s a great project which much has been written about; I will try to give as brief an overview as possible.  \\r\\n\\r\\n<details>\\r\\n<summary>ETL vs ELT Refresher</summary>\\r\\n<p>A quick refresher on ELT vs ETL before we discuss DBT. I have created an infographic for this...</p>\\r\\n<a target=\\"_blank\\" href=\\"/img/blog/dbt-in-five-minutes/etl-vs-elt.png\\">\\r\\n<img loading=\\"lazy\\" alt=\\"ETL vs ELT\\" src=\\"/img/blog/dbt-in-five-minutes/etl-vs-elt.png\\" width=\\"595\\" height=\\"410\\" class=\\"img_node_modules-@docusaurus-theme-classic-lib-theme-MDXComponents-Img-styles-module\\" /></a>\\r\\n</details>\\r\\n\\r\\n## Summary\\r\\n\\r\\nDBT is an open-source command line tool written in Python from DBT Labs (formerly Fishtown Analytics).  \\r\\n\\r\\nDBT is designed to manage data transformations while applying software engineering best practices (including version control, automated testing, reviews, approvals, etc).  Its modern software engineering and cloud-first design goals separate it from its old-school ETL/ELT predecessors.  \\r\\n\\r\\nDBT is an __ELT__ tool focusing on the __T__(ransform) only, the E(xtract) and L(oad) are up to you (there are plenty of tools that specialize in this).  \\r\\n\\r\\nAt its core DBT is a templating engine using Jinja (Python templating engine); it generates templates that represent SQL commands to create, replace or update objects in your database (the **\u201cT\u201d** in ELT), then oversees the execution of the templated commands.  The work is \\"pushed down\\" to the underlying database containing the source and target objects and data.   \\r\\n\\r\\n## Models\\r\\n\\r\\nThe concept most integral to DBT is the [__Model__](https://docs.getdbt.com/docs/building-a-dbt-project/building-models).  A Model is simply a representation of a transform (or set of transforms) to a dataset, resulting in a target object (which could be a table or tables in a datamart).  A model is expressed as a `SELECT` statement stored in a `.sql` file in your `dbt` project (well get to that in a minute).  \\r\\n\\r\\nSuppose we want to create a denormalized fact table for commits to store in a datamart in BigQuery.  This is what a model file might look like (using the BigQuery SQL dialect and referencing objects that should exist and be accessible at runtime when we execute the model).  \\r\\n\\r\\n```sql\\r\\n{{ config(materialized=\'table\') }}\\r\\n\\r\\nwith commits as (\\r\\n    SELECT \\r\\n    SUBSTR(commit, 0, 13) as commit_short_sha,\\r\\n    committer.name as commiter_name,\\r\\n    committer.date as commit_date,\\r\\n    message,\\r\\n    repo_name\\r\\n    FROM `bigquery-public-data.github_repos.sample_commits` c\\r\\n)\\r\\n\\r\\nselect * from commits\\r\\n```\\r\\n\\r\\nModels are created as views by default, but you can materialize these as tables where needed.  \\r\\n\\r\\nYou configure connectivity to the target database using [__adapters__](https://docs.getdbt.com/docs/available-adapters) (software libraries provided by dbt in the case of most mainstream databases) and [__profiles__](https://docs.getdbt.com/dbt-cli/configure-your-profile) (which contain details around authentication, databases/datasets, schemas etc).  \\r\\n\\r\\n## DBT Project\\r\\n\\r\\nA DBT project is simply a folder containing your models and some other configuration data.  You can initialize this by running [__`dbt init`__](https://docs.getdbt.com/reference/commands/init) from your desired project directory.  In its most basic form, the structure looks like this:  \\r\\n\\r\\n```\\r\\nmodels/\\r\\n\u251c\u2500 your_model.sql\\r\\n\u251c\u2500 schema.yml\\r\\ndbt_project.yml\\r\\n```\\r\\nYour models can be created under subfolders for organization.  [__`schema.yml`__](https://docs.getdbt.com/reference/resource-configs/schema) is an optional file that  contains tests for columns, can also include descriptions for documentation.  The [__`dbt_project.yml`__](https://docs.getdbt.com/reference/dbt_project.yml) file is the main entry point for the `dbt` program, it contains the configuration for the project, including which `profile` to use.  Profiles (stored in a file called [__`profiles.yml`__](https://docs.getdbt.com/reference/profiles.yml) store all of the necessary connectivity information for your target database platform.  By default `dbt init` creates this file a `.dbt` folder under your home directory.  \\r\\n\\r\\n:::info\\r\\n\\r\\nYou could store this with your project (be careful not to commit secrets like database credentials to source control).  If you store it in any other directory than the default, you will need to tell `dbt` where it can find this file using the `--profiles-dir` argument of any `dbt` command, see [__here__](https://docs.getdbt.com/dbt-cli/configure-your-profile#advanced-customizing-a-profile-directory) for more information.\\r\\n\\r\\n:::\\r\\n\\r\\nTo confirm your project is ship shape, run [__`dbt parse`__](https://docs.getdbt.com/reference/commands/parse); if there are no errors, you are good to proceed running and testing your models.  \\r\\n\\r\\n## Running DBT Models\\r\\n\\r\\nTo run your models, simply run the following command from the directory containing your `dbt_project.yml` file (typically the root of your project folder):  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"command\\"\\r\\n  values={[\\r\\n    { label: \'Command\', value: \'command\', },\\r\\n    { label: \'Output\', value: \'output\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"command\\">\\r\\n<pre>\\r\\n<b><a target=\\"_blank\\" href=\\"https://docs.getdbt.com/reference/commands/run\\">dbt run</a></b>\\r\\n</pre>\\r\\n</TabItem>\\r\\n<TabItem value=\\"output\\">\\r\\n<pre>\\r\\n06:56:36  Running with dbt=1.2.0<br />\\r\\n06:56:36  Found 1 model, 2 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics<br />\\r\\n06:56:36<br />\\r\\n06:56:37  Concurrency: 1 threads (target=\'dev\')<br />\\r\\n06:56:37<br />\\r\\n06:56:37  1 of 1 START table model dbt_dataset.fct_commits ............................... [RUN]<br />\\r\\n06:56:50  1 of 1 OK created table model dbt_dataset.fct_commits .......................... [CREATE TABLE (672.3k rows, 396.7 MB processed) in 13.28s]<br />\\r\\n06:56:50<br />\\r\\n06:56:50  Finished running 1 table model in 0 hours 0 minutes and 14.01 seconds (14.01s).<br />\\r\\n06:56:50<br />\\r\\n06:56:50  Completed successfully<br />\\r\\n06:56:50<br />\\r\\n06:56:50  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\\r\\n</pre>\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\nModel deployed!  Let\'s test it:   \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"command\\"\\r\\n  values={[\\r\\n    { label: \'Command\', value: \'command\', },\\r\\n    { label: \'Output\', value: \'output\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"command\\">\\r\\n<pre>\\r\\n<b><a target=\\"_blank\\" href=\\"https://docs.getdbt.com/reference/commands/test\\">dbt test</a></b>\\r\\n</pre>\\r\\n</TabItem>\\r\\n<TabItem value=\\"output\\">\\r\\n<pre>\\r\\n06:57:19  Running with dbt=1.2.0<br />\\r\\n06:57:19  Found 1 model, 2 tests, 0 snapshots, 0 analyses, 285 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics<br />\\r\\n06:57:19<br />\\r\\n06:57:19  Concurrency: 1 threads (target=\'dev\')<br />\\r\\n06:57:19<br />\\r\\n06:57:19  1 of 2 START test not_null_fct_commits_commit_short_sha ........................ [RUN]<br />\\r\\n06:57:21  1 of 2 PASS not_null_fct_commits_commit_short_sha .............................. [PASS in 1.88s]<br />\\r\\n06:57:21  2 of 2 START test unique_fct_commits_commit_short_sha .......................... [RUN]<br />\\r\\n06:57:24  2 of 2 PASS unique_fct_commits_commit_short_sha ................................ [PASS in 3.14s]<br />\\r\\n06:57:24<br />\\r\\n06:57:24  Finished running 2 tests in 0 hours 0 minutes and 5.41 seconds (5.41s).<br />\\r\\n06:57:24<br />\\r\\n06:57:24  Completed successfully<br />\\r\\n06:57:24<br />\\r\\n06:57:24  Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\\r\\n</pre>\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\nThis will run all of the tests associated with your model(s) - in this case, `not null` and `unique` tests defined in the `schema.yml` file.  That\'s it, deployed and tested.  \\r\\n\\r\\n## Other Stuff\\r\\n\\r\\nThere is some other stuff in DBT you should be aware of, like `seeds`, `snapshots`, `analyses`, `macros`, and more but our five minutes is up :smiley:.  We can discuss these next time; you are up and running with the basics of DBT now, get transforming!  \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"loading-parquet-files-into-snowflake","metadata":{"permalink":"/loading-parquet-files-into-snowflake","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-07-30-loading-parquet-files-into-snowflake/index.md","source":"@site/blog/2022-07-30-loading-parquet-files-into-snowflake/index.md","title":"Loading Parquet Files into Snowflake","description":"This article demonstrates how to automate and streamline the ingestion of Parquet formatted files into Snowflake.","date":"2022-07-30T00:00:00.000Z","formattedDate":"July 30, 2022","tags":[{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"parquet","permalink":"/tags/parquet"},{"label":"python","permalink":"/tags/python"},{"label":"spark","permalink":"/tags/spark"},{"label":"pyspark","permalink":"/tags/pyspark"},{"label":"bigquery","permalink":"/tags/bigquery"}],"readingTime":6.305,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"loading-parquet-files-into-snowflake","title":"Loading Parquet Files into Snowflake","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/parquet-to-snowflake.png","tags":["snowflake","parquet","python","spark","pyspark","bigquery"],"keywords":["snowflake","parquet","python","spark","pyspark","bigquery"],"description":"This article demonstrates how to automate and streamline the ingestion of Parquet formatted files into Snowflake."},"prevItem":{"title":"DBT in 5 Minutes","permalink":"/dbt-in-five-minutes"},"nextItem":{"title":"Analyze Developer Activity with StackQL, Jupyter and BigQuery","permalink":"/analyze-developer-activity-with-stackql-jupyter-bigquery"}},"content":"Loading Parquet format files into BigQuery is straightforward, you just need to specify the file location (local, Google Cloud Storage, Drive, Amazon S3 or Azure Blob storage) and thats pretty much it, BigQuery works the rest out from there.  \\r\\n\\r\\n```\\r\\nbq load \\\\\\r\\n--location=australia-southeast2 \\\\\\r\\n--project_id=parquet-demo \\\\\\r\\n--source_format=PARQUET \\\\\\r\\nparquet_test.dim_calendar \\\\\\r\\n.\\\\Calendar.gzip\\r\\n```\\r\\n\\r\\nIn Snowflake, however, it is not as simple, I\'ll share my approach to automating this here.  \\r\\n\\r\\n:::info\\r\\n\\r\\nParquet is a self-describing, column-oriented storage format commonly used in distributed systems for input and output.  Data in Parquet files is serialised for optimised consumption from Parquet client libraries and packages such as `pandas`, `pyarrow`, `fastparquet`, `dask`, and `pyspark`.\\r\\n\\r\\n:::\\r\\n\\r\\n## Background\\r\\n\\r\\nData in a Parquet file is stored in a single column for a self-contained dataset.  If you were to ingest this into Snowflake without knowing the schema you could do something like this...   \\r\\n\\r\\n```sql\\r\\nCREATE OR REPLACE TABLE PARQUET_TEST.PUBLIC.DIM_CALENDAR (\\r\\n  Data variant\\r\\n);\\r\\n\\r\\nCOPY INTO PARQUET_TEST.PUBLIC.DIM_CALENDAR \\r\\n(\\r\\n  Data\\r\\n) FROM (\\r\\nSELECT\\r\\n*\\r\\nFROM\\r\\n@PARQUET_TEST.PUBLIC.DIM_CALENDAR_STAGE)\\r\\n  file_format = (TYPE = parquet);\\r\\n```\\r\\n\\r\\nYou would end up with something like...   \\r\\n\\r\\n| `Row` | `Data` |\\r\\n|-------|------|\\r\\n|  `1`  | `{\\"CalMonthOfYearNo\\": 6, \\"CalYear\\": 2020, ... }` |\\r\\n|  `2`  | `{\\"CalMonthOfYearNo\\": 6, \\"CalYear\\": 2020, ... }` |\\r\\n| `...` | `...` |\\r\\n\\r\\nYou could then have a second stage of processing to convert this into a normal relational structure.  \\r\\n\\r\\nOr you could do this in one step, with a little prep work ahead of time.  In my scenario I was given several parquet files from a client for a one-off load into Snowflake, several files for a fact table and multiple single files representing different dimension tables.  \\r\\n\\r\\n## Streamlined Ingestion for Parquet Files into Snowflake \\r\\n\\r\\nTo collapse the formatting and uploading of Parquet files into a materialized table into one step, we need to do a couple of things:  \\r\\n\\r\\n1. Create the target table with the correct schema (column names and data types); and\\r\\n2. perform a projection in our `COPY` command from the single column containing all of the data (represented by `$1` in Snowflake) into columns defined in step 1\\r\\n\\r\\nSince this is technically a transformation and only named stages are supported for `COPY` transformations, we need to create a stage for the copy.  In my case there is a pre-existing Storage Integration in place that can be used by the stage.  \\r\\n\\r\\n### Generate Table DDL\\r\\n  \\r\\nTo automate the generation of the DDL to create the table and stage and the `COPY` command, I used Python and Spark (which has first class support for Parquet files).  Parquet datatypes are largely the same as Snowflake, but if we needed to, we could create a map and modify the target types during the DDL generation.  \\r\\n\\r\\nFirst copy specimen Parquet formatted files to a local directory, the script we are creating can then iterate through the parquet files and generate all of the commands we will need saved to a `.sql` file.  \\r\\n\\r\\nWith some setup information provided (not shown for brevity), we will first go through each file in the directory, capture metadata along with the schema (column name and data type) as shown here:  \\r\\n\\r\\n```python\\r\\nfor file in files:\\r\\n    tableMap = {}\\r\\n    table = file.stem\\r\\n    spark = launch_spark_session()\\r\\n    parquetFile = spark.read.parquet(\\"%s/%s\\" %(BASE_DIR, file))\\r\\n    data_types = parquetFile.dtypes\\r\\n    stop_spark_session(spark)\\r\\n    tableMap[\'name\'] = table\\r\\n    tableMap[\'file\'] = file\\r\\n    tableMap[\'data_types\'] = data_types\\r\\n    allTables.append(tableMap)\\r\\n```\\r\\n\\r\\nThe `allTables` list looks something like this...  \\r\\n\\r\\n```python\\r\\n[{\'name\': \'Calendar\', \'file\': PosixPath(\'data/dim/Calendar.gzip\'), \'data_types\': [(\'Time_ID\', \'bigint\'), (\'CalYear\', \'bigint\'), (\'CalMonthOfYearNo\', \'bigint\'), (\'FinYear\', \'bigint\'), (\'FinWeekOfYearNo\', \'bigint\')]}, ... ]\\r\\n```\\r\\n\\r\\nNext we generate the `CREATE TABLE` statement using the `allTables` list:  \\r\\n\\r\\n```python\\r\\n# create output file for all sql\\r\\nwith open(\'all_tables.sql\', \'w\') as f:\\r\\n    for table in allTables:\\r\\n        print(\\"processing %s...\\" % table[\'name\'])\\r\\n        f.write(\\"/*** Create %s Table***/\\" % table[\'name\'].upper())\\r\\n        sql = \\"\\"\\"\\r\\nCREATE OR REPLACE TABLE %s.%s.%s (\\r\\n\\"\\"\\" % (database, schema, table[\'name\'].upper())\\r\\n        for column in table[\'data_types\']:\\r\\n            sql += \\"  %s %s,\\\\n\\" % (column[0], column[1])\\r\\n        sql = sql[:-2] + \\"\\\\n);\\"\\r\\n        f.write(sql)\\r\\n        f.write(\\"\\\\n\\\\n\\")\\r\\n```\\r\\n\\r\\n### Generate Named Stage DDL\\r\\n\\r\\nThen we generate the stage in S3 from which the files will be loaded:  \\r\\n\\r\\n```python\\r\\n        f.write(\\"/*** Create %s Stage***/\\" % table[\'name\'].upper())\\r\\n        sql = \\"\\"\\"\\r\\nCREATE OR REPLACE STAGE %s.%s.%s_STAGE \\r\\n  url=\'%s/%s\'\\r\\n  storage_integration = %s\\r\\n  encryption=(type=\'AWS_SSE_KMS\' kms_key_id = \'%s\');\\r\\n\\"\\"\\" % (database, schema, table[\'name\'].upper(), s3_prefix, table[\'file\'], storage_int, kms_key_id)\\r\\n        f.write(sql)\\r\\n        f.write(\\"\\\\n\\\\n\\")\\r\\n```\\r\\n\\r\\n### Generate `COPY` commands\\r\\n\\r\\nThen we generate the `COPY` commands...  \\r\\n\\r\\n```python\\r\\n        f.write(\\"/*** Copying Data into %s ***/\\" % table[\'name\'].upper())\\r\\n        sql = \\"\\"\\"\\r\\nCOPY INTO %s.%s.%s \\r\\n(\\\\n\\"\\"\\" % (database, schema, table[\'name\'].upper())\\r\\n        for column in table[\'data_types\']:\\r\\n            sql += \\"  %s,\\\\n\\" % column[0]\\r\\n        sql = sql[:-2] + \\"\\\\n)\\"\\r\\n        sql += \\" FROM (\\\\nSELECT\\\\n\\"\\r\\n        for column in table[\'data_types\']:\\r\\n            sql += \\"  $1:%s::%s,\\\\n\\" % (column[0], column[1])\\r\\n        sql = sql[:-2] + \\"\\\\nFROM\\\\n\\"\\r\\n        sql += \\"@%s.%s.%s_STAGE)\\\\n\\" % (database, schema, table[\'name\'].upper()) \\r\\n        sql += \\"  file_format = (TYPE = parquet);\\"\\r\\n        f.write(sql)\\r\\n        f.write(\\"\\\\n\\\\n\\")\\r\\n```\\r\\n\\r\\nSince this is a one off load, we will go ahead and drop the stage we created as it is no longer needed (this step is optional)..\\r\\n\\r\\n```python\\r\\n        f.write(\\"/*** Dropping stage for %s ***/\\" % table[\'name\'].upper())\\r\\n        sql = \\"\\"\\"\\r\\nDROP STAGE %s.%s.%s_STAGE; \\r\\n\\"\\"\\" % (database, schema, table[\'name\'].upper())\\r\\n        f.write(sql)\\r\\n        f.write(\\"\\\\n\\\\n\\")\\r\\n```\\r\\n\\r\\nThe resultant file created looks like this..\\r\\n\\r\\n```sql\\r\\n/*** Create CALENDAR Table***/\\r\\nCREATE OR REPLACE TABLE PARQUET_TEST.PUBLIC.DIM_CALENDAR (\\r\\n  Time_ID bigint,\\r\\n  CalYear bigint,\\r\\n  CalMonthOfYearNo bigint,\\r\\n  FinYear bigint,\\r\\n  FinWeekOfYearNo bigint\\r\\n);\\r\\n\\r\\n/*** Create DIM_CALENDAR Stage***/\\r\\nCREATE OR REPLACE STAGE PARQUET_TEST.PUBLIC.DIM_CALENDAR_STAGE \\r\\n  url=\'s3://my-bucket/data/dim/Calendar.gzip\'\\r\\n  storage_integration = my_storage_int\\r\\n  encryption=(type=\'AWS_SSE_KMS\' kms_key_id = \'4f715ec9-ee8e-44ab-b35d-8daf36c05f19\');\\r\\n\\r\\n/*** Copying Data into DIM_CALENDAR ***/\\r\\nCOPY INTO PARQUET_TEST.PUBLIC.DIM_CALENDAR \\r\\n(\\r\\n  Time_ID,\\r\\n  CalYear,\\r\\n  CalMonthOfYearNo,\\r\\n  FinYear,\\r\\n  FinWeekOfYearNo\\r\\n) FROM (\\r\\nSELECT\\r\\n  $1:Time_ID::bigint,\\r\\n  $1:CalYear::bigint,\\r\\n  $1:CalMonthOfYearNo::bigint,\\r\\n  $1:FinYear::bigint,\\r\\n  $1:FinWeekOfYearNo::bigint\\r\\nFROM\\r\\n@PARQUET_TEST.PUBLIC.DIM_CALENDAR_STAGE)\\r\\n  file_format = (TYPE = parquet);\\r\\n\\r\\n/*** Dropping stage for DIM_CALENDAR ***/\\r\\nDROP STAGE PARQUET_TEST.PUBLIC.DIM_CALENDAR_STAGE; \\r\\n```\\r\\n\\r\\n### Load your data\\r\\n\\r\\nYou can then run this along with all of the other dimension and fact table DDL and COPY commands generated to perform the one-off load from parquet files. You can find the complete code below, enjoy!  \\r\\n\\r\\n<details>\\r\\n<summary>Complete Code</summary>\\r\\n<pre>\\r\\nfrom pathlib import Path<br/>\\r\\nfrom pyspark.sql import SparkSession</pre>\\r\\n\\r\\n    def launch_spark_session():\\r\\n        return SparkSession \\\\\\r\\n            .builder \\\\\\r\\n            .appName(\\"Parquet DDL Generation\\") \\\\\\r\\n            .getOrCreate()\\r\\n\\r\\n    def stop_spark_session(spark):\\r\\n        spark.stop()\\r\\n\\r\\n    allTables = []\\r\\n    database = \\"PARQUET_TEST\\" \\r\\n    schema = \\"PUBLIC\\"\\r\\n    s3_prefix = \'s3://my-bucket\'\\r\\n    storage_int = \'my_storage_int\'\\r\\n    kms_key_id = \'4f715ec9-ee8e-44ab-b35d-8daf36c05f19\'\\r\\n\\r\\n    BASE_DIR = Path(__file__).resolve().parent\\r\\n    directory = \'data/dim\'\\r\\n    files = Path(directory).glob(\'*.gzip\')\\r\\n    for file in files:\\r\\n        tableMap = {}\\r\\n        table = file.stem\\r\\n        spark = launch_spark_session()\\r\\n        parquetFile = spark.read.parquet(\\"%s/%s\\" %(BASE_DIR, file))\\r\\n        data_types = parquetFile.dtypes\\r\\n        stop_spark_session(spark)\\r\\n        tableMap[\'name\'] = table\\r\\n        tableMap[\'file\'] = file\\r\\n        tableMap[\'data_types\'] = data_types\\r\\n        allTables.append(tableMap)\\r\\n\\r\\n    # create output file for all sql\\r\\n    with open(\'all_tables.sql\', \'w\') as f:\\r\\n        for table in allTables:\\r\\n            print(\\"processing %s...\\" % table[\'name\'])\\r\\n            f.write(\\"/*** Create %s Table***/\\" % table[\'name\'].upper())\\r\\n            sql = \\"\\"\\"\\r\\n    CREATE OR REPLACE TABLE %s.%s.%s (\\r\\n    \\"\\"\\" % (database, schema, table[\'name\'].upper())\\r\\n            for column in table[\'data_types\']:\\r\\n                sql += \\"  %s %s,\\\\n\\" % (column[0], column[1])\\r\\n            sql = sql[:-2] + \\"\\\\n);\\"\\r\\n            f.write(sql)\\r\\n            f.write(\\"\\\\n\\\\n\\")\\r\\n            \\r\\n            f.write(\\"/*** Create %s Stage***/\\" % table[\'name\'].upper())\\r\\n            sql = \\"\\"\\"\\r\\n    CREATE OR REPLACE STAGE %s.%s.%s_STAGE \\r\\n      url=\'%s/%s\'\\r\\n      storage_integration = %s\\r\\n      encryption=(type=\'AWS_SSE_KMS\' kms_key_id = \'%s\');\\r\\n    \\"\\"\\" % (database, schema, table[\'name\'].upper(), s3_prefix, table[\'file\'], storage_int, kms_key_id)\\r\\n            f.write(sql)\\r\\n            f.write(\\"\\\\n\\\\n\\")\\r\\n\\r\\n            f.write(\\"/*** Copying Data into %s ***/\\" % table[\'name\'].upper())\\r\\n            sql = \\"\\"\\"\\r\\n    COPY INTO %s.%s.%s \\r\\n    (\\\\n\\"\\"\\" % (database, schema, table[\'name\'].upper())\\r\\n            for column in table[\'data_types\']:\\r\\n                sql += \\"  %s,\\\\n\\" % column[0]\\r\\n            sql = sql[:-2] + \\"\\\\n)\\"\\r\\n            sql += \\" FROM (\\\\nSELECT\\\\n\\"\\r\\n            for column in table[\'data_types\']:\\r\\n                sql += \\"  $1:%s::%s,\\\\n\\" % (column[0], column[1])\\r\\n            sql = sql[:-2] + \\"\\\\nFROM\\\\n\\"\\r\\n            sql += \\"@%s.%s.%s_STAGE)\\\\n\\" % (database, schema, table[\'name\'].upper()) \\r\\n            sql += \\"  file_format = (TYPE = parquet);\\"\\r\\n            f.write(sql)\\r\\n            f.write(\\"\\\\n\\\\n\\")\\r\\n\\r\\n            f.write(\\"/*** Dropping stage for %s ***/\\" % table[\'name\'].upper())\\r\\n            sql = \\"\\"\\"\\r\\n    DROP STAGE %s.%s.%s_STAGE; \\r\\n    \\"\\"\\" % (database, schema, table[\'name\'].upper())\\r\\n            f.write(sql)\\r\\n            f.write(\\"\\\\n\\\\n\\")\\r\\n</details>\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"analyze-developer-activity-with-stackql-jupyter-bigquery","metadata":{"permalink":"/analyze-developer-activity-with-stackql-jupyter-bigquery","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-07-24-analyze-developer-activity-with-stackql-jupyter-bigquery/index.md","source":"@site/blog/2022-07-24-analyze-developer-activity-with-stackql-jupyter-bigquery/index.md","title":"Analyze Developer Activity with StackQL, Jupyter and BigQuery","description":"This article demonstrates how to use StackQL, Jupyter and BigQuery to analyze developer data from GitHub.","date":"2022-07-24T00:00:00.000Z","formattedDate":"July 24, 2022","tags":[{"label":"stackql","permalink":"/tags/stackql"},{"label":"github","permalink":"/tags/github"},{"label":"sql","permalink":"/tags/sql"},{"label":"jupyter","permalink":"/tags/jupyter"},{"label":"bigquery","permalink":"/tags/bigquery"}],"readingTime":2.475,"hasTruncateMarker":false,"authors":[{"name":"Yuncheng Yang","title":"Full Stack Engineer","url":"https://www.linkedin.com/in/yuncheng-fabio-yang/","imageURL":"https://en.gravatar.com/userimage/195643035/72fc562ee87d0c67847c8989d2808129.jpg?size=80","key":"yunchengyang"}],"frontMatter":{"slug":"analyze-developer-activity-with-stackql-jupyter-bigquery","title":"Analyze Developer Activity with StackQL, Jupyter and BigQuery","authors":["yunchengyang"],"draft":false,"image":"/img/blog/analyze-developer-activity-with-stackql-jupyter-bigquery.png","tags":["stackql","github","sql","jupyter","bigquery"],"keywords":["stackql","github","sql","jupyter","bigquery"],"description":"This article demonstrates how to use StackQL, Jupyter and BigQuery to analyze developer data from GitHub."},"prevItem":{"title":"Loading Parquet Files into Snowflake","permalink":"/loading-parquet-files-into-snowflake"},"nextItem":{"title":"Converting Google Discovery Docs to OpenAPI3 Specs","permalink":"/converting-google-discovery-docs-to-openapi3-specs"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nIt is common to have a remote and dispersed team these days. As face to face meetings are less common and with geographically dispersed development teams not possible, it is challenging to have a clear picture of where your team is.  \\r\\n\\r\\nGitHub provides useful data to help us understand your development team\'s workload and progress.  StackQL has an official GitHub provider which allows you to access this data using SQL. \\r\\n\\r\\n:::info\\r\\n\\r\\nStackQL is an open source project which enables you to query, analyze and interact with cloud and SaaS provider resources using SQL, see [__stackql.io__](https://stackql.io/) \\r\\n\\r\\n:::\\r\\n\\r\\nIn this example we will use the [`pystackql`](https://github.com/stackql/pystackql) Python package (Python wrapper for StackQL) along with a Jupyter Notebook to retrieve data from GitHub using SQL, then sink the data into a cloud native data warehouse for long term storage and analytics at scale, in this example we have used [BigQuery](https://cloud.google.com/bigquery).  \\r\\n\\r\\n## Step by Step Guide\\r\\n\\r\\nThis guide will walk you through the steps involved in capturing and analyzing developer data using StackQL, Python, Jupyter and BigQuery.  \\r\\n\\r\\n### 1. Create GitHub Personal Access Token\\r\\n\\r\\nYou will need to create a Personal Access Token in GitHub for a user which has access to the org or orgs in GitHub you will be analyzing.  Follow [this guide](https://docs.github.com/en/enterprise-server@3.4/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) to create your GitHub token and store it somewhere safe.  \\r\\n\\r\\n### 2. Setup your Jupyter Notebook\\r\\nYou need to set up your Jupyter environment, you can either use the Docker, see [stackql/stackql-jupyter-demo](https://github.com/stackql/stackql-jupyter-demo) or:  \\r\\n1. [Create your Jupyter project](https://jupyter.org/try)\\r\\n2. [Download and install StackQL](https://stackql.io/downloads)\\r\\n3. [Clone the pystackql repo](https://github.com/stackql/pystackql)\\r\\n\\r\\n### 3. Setup StackQL Authentication to GitHub\\r\\n\\r\\nYou can find instructions on how to use your personal access token to authenticate to GitHub [here](https://registry.stackql.io/providers/github/#authentication).  The following example shows how to do this in a Jupyter notebook cell using `pystackql`.\\r\\n\\r\\n<Gist id=\\"18e9de9c1a184a5d3e7d623d4681ceb3\\"\\r\\n/>\\r\\n\\r\\n### 4. Retrieve data\\r\\n\\r\\nNext, we will use StackQL SQL queries to get commits, pull requests and pull request reviews, then we will aggregate by usernames of contributors. You can use `JOIN` semantics in StackQL to do this as well.  \\r\\n\\r\\n#### Get Contributors, Commits, Pull Requests and Reviews\\r\\n\\r\\nIn the following cell we will query data from GitHub using StackQL:  \\r\\n\\r\\n<Gist id=\\"29da46f9428ea7c2dcb53fdded019785\\"\\r\\n/>\\r\\n\\r\\n#### Aggregate Data By Username\\r\\n\\r\\nNow we will aggregate the data by each contributor, see the following example:\\r\\n\\r\\n<Gist id=\\"4414fcf6bdd6aff4d227c07b461887e6\\"\\r\\n/>\\r\\n\\r\\n### 5. Store the Data in BigQuery\\r\\nAfter the transformation of data, we will then upload it to BigQuery.  First, we will store the data as a new line delimited `json` file, making the uploading process much easier and handling the nested schema better, as shown in the following cell:  \\r\\n\\r\\n<Gist id=\\"34ac71d46030693bbaa3c7c4309855f5\\"\\r\\n/>\\r\\n\\r\\nNow we can see the table on BigQuery as shown here:  \\r\\n\\r\\n[![BigQuery User Activity Table](images/bq-user-activity.png)](images/bq-user-activity.png)\\r\\n\\r\\nFrom here you can use the same process to append data to the table and use BigQuery to perform analytics at scale on the data.\\r\\n\\r\\n:::info\\r\\n\\r\\nThe complete notebook for this article can be accessed at [FabioYyc/stackql-github-notebook-bq](https://github.com/FabioYyc/stackql-github-notebook-bq) \\r\\n\\r\\n:::"},{"id":"converting-google-discovery-docs-to-openapi3-specs","metadata":{"permalink":"/converting-google-discovery-docs-to-openapi3-specs","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-06-24-converting-google-discovery-docs-to-openapi3-specs/index.md","source":"@site/blog/2022-06-24-converting-google-discovery-docs-to-openapi3-specs/index.md","title":"Converting Google Discovery Docs to OpenAPI3 Specs","description":"This article demonstrates how to generate OpenAPI3 specification documents for Google Cloud services.","date":"2022-06-24T00:00:00.000Z","formattedDate":"June 24, 2022","tags":[{"label":"openapi","permalink":"/tags/openapi"},{"label":"openapi3","permalink":"/tags/openapi-3"},{"label":"googlecloud","permalink":"/tags/googlecloud"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"swagger","permalink":"/tags/swagger"}],"readingTime":1.545,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"converting-google-discovery-docs-to-openapi3-specs","title":"Converting Google Discovery Docs to OpenAPI3 Specs","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/google-discovery-doc-to-openapi-conversion.png","tags":["openapi","openapi3","openapi","googlecloud","gcp","swagger"],"keywords":["openapi","openapi3","openapi","googlecloud","gcp","swagger"],"description":"This article demonstrates how to generate OpenAPI3 specification documents for Google Cloud services."},"prevItem":{"title":"Analyze Developer Activity with StackQL, Jupyter and BigQuery","permalink":"/analyze-developer-activity-with-stackql-jupyter-bigquery"},"nextItem":{"title":"Recurse JavaScript Object to Get Values for a Given Key the Easy Way","permalink":"/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nThis article walks through the process of converting service specific OpenAPI specifications from Google Discovery REST URLs using Python.  \\r\\n\\r\\n> Full code for this article can be found at [__stackql/google-discovery-to-openapi__](https://github.com/stackql/google-discovery-to-openapi)  \\r\\n\\r\\nGoogle publishes JSON specifications for all of their APIs (including GCP services as well as other APIs associated with other products - like analytics or workspace).  These specifications can be accessed without authentication starting with the root document ([https://discovery.googleapis.com/discovery/v1/apis](https://discovery.googleapis.com/discovery/v1/apis)) which contains metadata and the URL for each service specific document (for services like `compute` or `storage`).\\r\\n\\r\\n## Code Overview\\r\\n\\r\\nThe program fetches the service document for each service that is included and not explicitly excluded (configured through variables in the program).  Non preferred services (beta or alpha versions) can be included by setting the variable `get_preferred_only` to `False`.  \\r\\n\\r\\nAn OpenAPI spec is constructed for each service based upon the data in the service discovery doc.  In many cases this is a straightforward one to one mapping, such as to top level `info`, `title` and `description` values, it gets more complicated with parameters and schemas where some extra logic is required to keep the json pointers (`$ref`) valid.  \\r\\n\\r\\n### Extracting Paths and Verbs\\r\\n\\r\\nThe real magic is in extracting paths and verbs in a compliant OpenAPI format, as Google nests this data (potentially multiple levels deep) under resources.  \\r\\n\\r\\nThe first step is to identify `methods` nested under a `resources` object (which can be mapped to operations - with a path and HTTP verb required to populate an OpenAPI spec), this function does this:  \\r\\n\\r\\n<Gist id=\\"11ee413049cdcd81a433d4df8925c016\\" \\r\\n/>\\r\\n\\r\\nNow each method can be processed yielding an operation (combination of `path` and `verb`), this is done using this function:  \\r\\n\\r\\n<Gist id=\\"2ffd64aa8ba07a8f9ccd441ed9709ef3\\" \\r\\n/>\\r\\n\\r\\nFull source code can be found at [__stackql/google-discovery-to-openapi__](https://github.com/stackql/google-discovery-to-openapi).\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way","metadata":{"permalink":"/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-06-06-recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way/index.md","source":"@site/blog/2022-06-06-recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way/index.md","title":"Recurse JavaScript Object to Get Values for a Given Key the Easy Way","description":"I had a scenario where I needed to find values for a key in a complex JavaScript object which could be nested *n* levels deep.","date":"2022-06-06T00:00:00.000Z","formattedDate":"June 6, 2022","tags":[{"label":"nodejs","permalink":"/tags/nodejs"},{"label":"javascript","permalink":"/tags/javascript"},{"label":"openapi","permalink":"/tags/openapi"},{"label":"swagger","permalink":"/tags/swagger"}],"readingTime":1.555,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way","title":"Recurse JavaScript Object to Get Values for a Given Key the Easy Way","authors":["jeffreyaven"],"draft":false,"image":"/img/fullstackchronicles-cover-image.png","tags":["nodejs","javascript","openapi","swagger"],"keywords":["nodejs","javascript","openapi","swagger"]},"prevItem":{"title":"Converting Google Discovery Docs to OpenAPI3 Specs","permalink":"/converting-google-discovery-docs-to-openapi3-specs"},"nextItem":{"title":"DataOps with Container Images and Multi-Stage Builds","permalink":"/dataops-with-container-images-and-multi-stage-builds"}},"content":"I had a scenario where I needed to find values for a key in a complex JavaScript object which could be nested __*n*__ levels deep.  \\r\\n\\r\\nI found numerous approaches to doing this, most were overly complicated, so I thought I would share the most straightforward, concise process.  \\r\\n\\r\\n## the Code\\r\\n\\r\\nYou can do this in a straightforward function implementing the __*\\"tail call recursion\\"*__ pattern to search for a key (`key`) from the root of an object (`obj`), excluding any keys in `excludeKeys`.  \\r\\n\\r\\nThis will return a list of values for the given key, searching all levels in all branches of the object.   \\r\\n\\r\\n```javascript\\r\\nfunction getAllValuesForKey(obj, key, excludeKeys=[], values=[]) {\\r\\n    for (let k in obj) {\\r\\n        if (typeof obj[k] === \\"object\\") {\\r\\n            if(!excludeKeys.includes(k)){\\r\\n                getAllValuesForKey(obj[k], key, excludeKeys, values)\\r\\n            }\\r\\n        } else {\\r\\n            if (k === key){\\r\\n                values.push(obj[k]);\\r\\n            }\\r\\n        }\\r\\n    }\\r\\n    return values;\\r\\n}\\r\\n```\\r\\n\\r\\n## Example\\r\\n\\r\\nIn parsing an OpenAPI or Swagger specification, I am looking for all of the schema `refs` in a successful response body, for example:  \\r\\n\\r\\n```yaml\\r\\npaths:\\r\\n\\t\'/orgs/{org}/actions/permissions/selected-actions\':\\r\\n\\t\\tget:\\r\\n\\t\\t  ...\\r\\n\\t\\t  responses:\\r\\n\\t\\t\\t\'200\': \'...\'\\r\\n```\\r\\n\\r\\nhowever these refs can present in various different ways depending upon the response type, such as:  \\r\\n\\r\\n```yaml\\r\\n\'200\':\\r\\n\\t$ref: \'#/components/responses/actions_runner_labels\'\\r\\n```\\r\\n\\r\\nor  \\r\\n\\r\\n```yaml\\r\\n\'200\':      \\r\\n\\tcontent:\\r\\n\\t\\tapplication/json:\\r\\n\\t\\t  schema:\\r\\n\\t\\t\\t$ref: \'#/components/schemas/runner\'\\r\\n```\\r\\n\\r\\nor  \\r\\n\\r\\n```yaml\\r\\n\'200\':\\r\\n  content:\\r\\n\\tapplication/json:\\r\\n\\t  schema:\\r\\n\\t\\tanyOf:\\r\\n\\t\\t  - $ref: \'#/components/schemas/interaction-limit-response\'\\r\\n```\\r\\n\\r\\nor\\r\\n\\r\\n```yaml\\r\\n\'200\':\\r\\n  content:\\r\\n\\tapplication/json:\\r\\n\\t  schema:\\r\\n\\t\\ttype: object\\r\\n\\t\\trequired:\\r\\n\\t\\t  - total_count\\r\\n\\t\\t  - runners\\r\\n\\t\\tproperties:\\r\\n\\t\\t  total_count:\\r\\n\\t\\t\\ttype: integer\\r\\n\\t\\t  runners:\\r\\n\\t\\t\\ttype: array\\r\\n\\t\\t\\titems:\\r\\n\\t\\t\\t  $ref: \'#/components/schemas/runner\'\\r\\n```\\r\\n\\r\\nTo find all of the schema refs without knowing the response type or structure I used the above function as follows (excluding refs for `examples`):  \\r\\n\\r\\n```javascript\\r\\nfunction getRespSchemaName(op){\\r\\n    for(let respCode in op.responses){\\r\\n        if(respCode.startsWith(\'2\')){\\r\\n            return getAllValuesForKey(op.responses[respCode], \\"$ref\\", [\'examples\']);\\r\\n        }\\r\\n    }\\r\\n}\\r\\n```\\r\\n\\r\\nYou can find this implementation in [__`openapi-doc-util`__](https://github.com/stackql/openapi-doc-util) and [__`@stackql/openapi-doc-util`__](https://www.npmjs.com/package/@stackql/openapi-doc-util).  \\r\\n\\r\\nsimple!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"dataops-with-container-images-and-multi-stage-builds","metadata":{"permalink":"/dataops-with-container-images-and-multi-stage-builds","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-05-28-dataops-with-container-images-and-multi-stage-builds/index.md","source":"@site/blog/2022-05-28-dataops-with-container-images-and-multi-stage-builds/index.md","title":"DataOps with Container Images and Multi-Stage Builds","description":"This article demonstrates how multi-stage builds and tools can automate and simplify container maintenance.","date":"2022-05-28T00:00:00.000Z","formattedDate":"May 28, 2022","tags":[{"label":"python","permalink":"/tags/python"},{"label":"docker","permalink":"/tags/docker"},{"label":"ci-cd","permalink":"/tags/ci-cd"},{"label":"data engineering","permalink":"/tags/data-engineering"},{"label":"poetry","permalink":"/tags/poetry"}],"readingTime":7.15,"hasTruncateMarker":false,"authors":[{"name":"Chris Ottinger","title":"Senior Technologist","url":"https://github.com/datwiz","imageURL":"http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80","key":"chrisottinger"}],"frontMatter":{"slug":"dataops-with-container-images-and-multi-stage-builds","title":"DataOps with Container Images and Multi-Stage Builds","authors":["chrisottinger"],"draft":false,"image":"/img/blog/dataops-featured-image.png","tags":["python","docker","ci-cd","data engineering","poetry"],"keywords":["python","docker","ci-cd","data engineering","poetry"],"description":"This article demonstrates how multi-stage builds and tools can automate and simplify container maintenance."},"prevItem":{"title":"Recurse JavaScript Object to Get Values for a Given Key the Easy Way","permalink":"/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way"},"nextItem":{"title":"Using the Snowflake SQL API with TypeScript","permalink":"/using-the-snowflake-sql-api-with-typescript"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\n\\r\\nContainer images provide an ideal software packaging solution for DataOps and python based data pipeline workloads.  Containers enable Data Scientists and Data Engineers to incorporate the latest packages and libraries without the issues associated with introducing breaking changes into shared environments.  A Data Engineer or Data Scienctist can quickly release new functionality with the best tools available.  \\r\\n\\r\\nContainer images provide safer developer environments but as the number of container images used for production workloads grow, a maintenance challenge can emerge.  Whether using [pip](https://pypi.org/project/pip) or [poetry](https://python-poetry.org/) to manage python packages and dependencies, updating a container definition requires edits to the explicit package versions as well as to the pinned or locked versions of the package dependencies. This process can be error prone without automation and a repeatable CICD workflow.  \\r\\n\\r\\nA workflow pattern based on [docker buildkit](https://docs.docker.com/develop/develop-images/build_enhancements/) / [moby buildkit](https://github.com/moby/buildkit) multi-stage builds provides an approach that maintains all the build specifications in a single `Dockerfile`, while build tools like `make` provide a simple and consistent interface into the container build stages.  The data pipeline challenges addresses with a multi-stage build pattern include:  \\r\\n\\r\\n- automating lifecycle management of the Python packages used by data pipelines\\r\\n- integrating smoke testing of container images to weed out compatibility issues early\\r\\n- simplifying the developer experience with tools like `make` that can be used both locally and in CI/CD pipelines\\r\\n\\r\\nThe `Dockerfile` contains the definitions of the different target build stages and order of execution from one stage to the next.  The `Makefile` wraps the Dockerfile build targets into a standard set of workflow activities, following a similar to `$ config && make && make install` \\r\\n\\r\\n## The DataOps Container Lifecycle Workflow\\r\\n\\r\\nA typical dataops/gitops style workflow for maintaining container images includes actions in the local environment to define the required packages and produce the pinned dependency `poetry.lock` file or `requirements.txt` packages list containing the full set of pinned dependent packages.  \\r\\n\\r\\nGiven and existing project in a remote git repository with a CI/CD pipeline defined, the following workflow would be used to update package versions and dependencies:  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"flow\\"\\r\\n  values={[\\r\\n    { label: \'Workflow\', value: \'flow\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"flow\\">\\r\\n\\r\\n[![Multi-stage build workflow](images/multi-stage-build-workflow.png)](images/multi-stage-build-workflow.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```plantuml\\r\\n@startuml Multi-stage build workflow\\r\\n|Local Maintainer|\\r\\nstart\\r\\n:Clone git repository and\\r\\ncreate a feature branch;\\r\\n:Update declared\\r\\ndependencies;\\r\\n:Run build with\\r\\nrefresh option;\\r\\n:Update new pinned\\r\\npackages file in the\\r\\ngit repository;\\r\\n:Commit changes and push\\r\\nto remote repository;\\r\\n|Remote Git Service|\\r\\n:Validate feature branch\\r\\nchanges;\\r\\n:Merge changes into\\r\\nmain branch;\\r\\n:build target image and\\r\\npush to package registry;\\r\\n|Package Registry|\\r\\n:publish new image;\\r\\nstop\\r\\n@enduml\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\nThe image maintainer selects the packages to update or refresh using a local development environment, working from a feature branch.  This includes performing an image smoke-test to validate the changes within the container image.  \\r\\n\\r\\nOnce refreshed image has been validated, the lock file or full pinned package list is commited back to the repository and pushed to the remote repository.  The CI/CD pipeline performs a trial build and conducts smoke testing.  On merge into the main branch, the target image is built, re-validated, and pushed to the container image registry.  \\r\\n\\r\\nThe multi-stage build pattern can support both defining both the declared packages for an environment as well as the dependent packages, but `poetry` splits the two into distinct files, a `pyproject.toml` file containing the declated packages and a `poetry.lock` file that contains the full set of declared and dependent packages, including pinned versions.  `pip` supports loading packages from different files, but requires a convention for which requirements file contains the declared packages and while contains the full set of pinned package versions produced by `pip freeze`.  The example code repo contains examples using both `pip` and `poetry`.  \\r\\n\\r\\nThe following example uses [poetry](https://python-poetry.org/) in a `python:3.8` base image to illustrate managing the dependencies and version pinning of python packages.  \\r\\n\\r\\n## Multi-stage Dockerfile\\r\\n\\r\\nThe `Dockerfile` defines the build stages used for both local refresh and by the CICD pipelines to build the target image.  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"stages\\"\\r\\n  values={[\\r\\n    { label: \'Stages\', value: \'stages\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"stages\\">\\r\\n\\r\\n[![Dockerfile Stages](images/Dockerfile-stages.png)](images/Dockerfile-stages.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```plantuml\\r\\n@startuml Dockerfile stages\\r\\n!define C4_PLANTUML https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master\\r\\n!include C4_PLANTUML/C4_Component.puml\\r\\n\\r\\nHIDE_STEREOTYPE()\\r\\nUpdateElementStyle(Container, $bgColor=green)\\r\\n\\r\\nTitle: Docker build stages\\r\\nContainer(pre, base-pre-pkg,)\\r\\nContainer(refresh, python-pkg-refresh,)\\r\\nContainer(pinned, python-pkg-pinned,)\\r\\nContainer(post, base-post-pkg,)\\r\\nContainer(smoke, smoke-test,)\\r\\nContainer(target, target-image,)\\r\\n\\r\\nRel(pre, refresh, \\"refresh\\")\\r\\nRel(pre, pinned, \\"pinned\\")\\r\\nRel(refresh, post, \\" \\")\\r\\nRel(pinned, post, \\" \\")\\r\\nRel(post, smoke, \\"QA\\")\\r\\nRel(post, target, \\"artefact\\")\\r\\n@enduml\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\nThe Dockerfile makes use of the docker build arguments feature to pass in whether the build should refresh package versions or build the image from pinned packages.  \\r\\n\\r\\n### Build Stage: base-pre-pkg\\r\\n\\r\\nAny image setup and pre-python package installation steps.  For `poetry`, this includes setting the config option to skip the creation of a virtual environment as the container already provides the required isolation.  \\r\\n\\r\\n```Dockerfile\\r\\nARG PYTHON_PKG_VERSIONS=pinned\\r\\nFROM python:3.8 as base-pre-pkg\\r\\n\\r\\nRUN install -d /src && \\\\\\r\\n    pip install --no-cache-dir poetry==1.1.13 && \\\\\\r\\n    poetry config virtualenvs.create false\\r\\nWORKDIR /src\\r\\n```\\r\\n\\r\\n### Build Stage: python-pkg-refresh\\r\\n\\r\\nThe steps to generate a `poetry.lock` file containing the pinned package versions.  \\r\\n\\r\\n```Dockerfile\\r\\nFROM base-pre-pkg as python-pkg-refresh\\r\\nCOPY pyproject.toml poetry.lock /src/\\r\\nRUN poetry update && \\\\\\r\\n    poetry install \\r\\n```\\r\\n\\r\\n### Build Stage: python-pkg-pinned\\r\\n\\r\\nThe steps to install packages using the pinned package versions.\\r\\n\\r\\n```Dockerfile\\r\\nFROM base-pre-pkg as python-pkg-pinned\\r\\nCOPY pyproject.toml poetry.lock /src/\\r\\nRUN poetry install \\r\\n```\\r\\n\\r\\n### Build Stage: base-post-pkg\\r\\n\\r\\nA consolidation build target that can refer to either the python-pkg-refresh or the python-pkg-pinned stages, depending on the docker build argument and includes any post-package installation steps.  \\r\\n\\r\\n```Dockerfile\\r\\nFROM python-pkg-${PYTHON_PKG_VERSIONS} as base-post-pkg\\r\\n```\\r\\n\\r\\n### Build Stage: smoke-test\\r\\n\\r\\nSimple smoke tests and validation commands to validate the built image.  \\r\\n\\r\\n```Dockerfile\\r\\nFROM base-post-pkg as smoke-test\\r\\nWORKDIR /src\\r\\nCOPY tests/ ./tests\\r\\nRUN poetry --version && \\\\\\r\\n    python ./tests/module_smoke_test.py\\r\\n```\\r\\n\\r\\n### Build Stage: target-image\\r\\n\\r\\nThe final build target container image.  Listing the `target-image` as the last stage in the `Dockerfile` has the effect of also making this the default build target.  \\r\\n\\r\\n```Dockerfile\\r\\nFROM base-post-pkg as target-image\\r\\n```\\r\\n\\r\\n## Multi-stage Makefile\\r\\n\\r\\nThe Makefile provides a workflow oriented wrapper over the Dockerfile build stage targets.  The Makefile targets can be executed both in a local development environment as well as via a CICD pipeline.  The `Makefile` includes several variables that can either be run using default values, or overridden by the CI/CD pipeline.  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"targets\\"\\r\\n  values={[\\r\\n    { label: \'Targets\', value: \'targets\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"targets\\">\\r\\n\\r\\n[![Makefile targets](images/Makefile-targets.png)](images/Makefile-targets.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```plantuml\\r\\n@startuml Makefile targets\\r\\n!define C4_PLANTUML https://raw.githubusercontent.com/plantuml-stdlib/C4-PlantUML/master\\r\\n!include C4_PLANTUML/C4_Component.puml\\r\\n\\r\\nHIDE_STEREOTYPE()\\r\\n\\r\\nTitle: Makefile targets\\r\\nContainer(style, style-check,)\\r\\nContainer(refresh, python-pkg-refresh, \\"docker target=smoke-test\\")\\r\\nContainer(smoke, smoke-test, \\"docker target=smoke-test\\")\\r\\nContainer(build, build, \\"docker target=target-image\\")\\r\\n\\r\\nRel(style, refresh, \\" \\")\\r\\nRel(style, build, \\" \\")\\r\\nRel(build, smoke, \\" \\")\\r\\n\\r\\n@enduml\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n## Make Target: style-check\\r\\n\\r\\nLinting and style checking of source code.  Can include both application code as well as the Dockerfile itself using tools such as [hadolint](https://github.com/hadolint/hadolint).  \\r\\n\\r\\n```Makefile\\r\\nstyle-check:\\r\\n\\thadolint ./Dockerfile\\r\\n```\\r\\n\\r\\n## Make Target: python-pkg-refresh\\r\\n\\r\\nThe `python-pkg-refresh` target builds a version of the target image with refreshed package versions. A temporary container instance is created from the target image and the `poetry.lock` file is copied into the local file system. The `smoke-test` docker build target is used to ensure image validation is also performed.\\r\\nThe temporary container as well as the package refresh image are removed after the build.  \\r\\n\\r\\n```Makefile\\r\\npython-pkg-refresh:\\r\\n\\t@echo \\">> Update python packages in container image\\"\\r\\n\\tdocker build ${DOCKER_BUILD_ARGS} \\\\\\r\\n\\t       --target smoke-test \\\\\\r\\n\\t\\t   --build-arg PYTHON_PKG_VERSIONS=refresh \\\\\\r\\n\\t\\t   --tag ${TARGET_IMAGE_NAME}:$@ .\\r\\n\\t@echo \\">> Copy the new poetry.lock file with updated package versions\\"\\r\\n\\tdocker create --name ${TARGET_IMAGE_NAME}-$@ ${TARGET_IMAGE_NAME}:$@\\r\\n\\tdocker cp ${TARGET_IMAGE_NAME}-$@:/src/poetry.lock .\\r\\n\\t@echo \\">> Clean working container and refresh image\\"\\r\\n\\tdocker rm ${TARGET_IMAGE_NAME}-$@\\r\\n\\tdocker rmi ${TARGET_IMAGE_NAME}:$@\\r\\n```\\r\\n\\r\\n### Make Target: build\\r\\n\\r\\nThe standard build target using pinned python package versions.  \\r\\n\\r\\n```Makefile\\r\\nbuild:\\r\\n\\tdocker build ${DOCKER_BUILD_ARGS} \\\\\\r\\n\\t       --target target-image \\\\\\r\\n\\t\\t   --tag ${TARGET_IMAGE_NAME}:${BUILD_TAG} .\\r\\n\\r\\n```\\r\\n\\r\\n### Make Target: smoke-test\\r\\n\\r\\nBuilds an image and peforms smoke testing.  The smoke-testing image is removed after the build.  \\r\\n\\r\\n```Makefile\\r\\nsmoke-test:\\r\\n\\tdocker build ${DOCKER_BUILD_ARGS} \\\\\\r\\n\\t       --target smoke-test \\\\\\r\\n\\t\\t   --tag ${TARGET_IMAGE_NAME}:$@ .\\r\\n\\tdocker rmi ${TARGET_IMAGE_NAME}:$@\\r\\n```\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nThe toolchain combination of multi-stage container image builds with `make` provides a codified method for the lifecycle management of the containers used in data science and data engineering workloads.  \\r\\n\\r\\nThe maintainer:  \\r\\n\\r\\n```bash\\r\\ngit checkout -b my-refresh-feature\\r\\nmake python-pkg-refresh\\r\\nmake smoke-test\\r\\ngit add pyproject.toml poetry.lock\\r\\ngit commit -m \\"python package versions updated\\"\\r\\ngit push\\r\\n```\\r\\n\\r\\nThe CICD pipeline:  \\r\\n\\r\\n```bash\\r\\nmake build\\r\\nmake smoke-test\\r\\ndocker push <target-image>:<build-tag>\\r\\n```\\r\\n\\r\\n:::info\\r\\n\\r\\nYou can find the complete source code for this article at [https://gitlab.com/datwiz/multistage-pipeline-image-builds](https://gitlab.com/datwiz/multistage-pipeline-image-builds)\\r\\n\\r\\n:::"},{"id":"using-the-snowflake-sql-api-with-typescript","metadata":{"permalink":"/using-the-snowflake-sql-api-with-typescript","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-05-19-using-the-snowflake-sql-api-with-typescript/index.md","source":"@site/blog/2022-05-19-using-the-snowflake-sql-api-with-typescript/index.md","title":"Using the Snowflake SQL API with TypeScript","description":"This article demonstrates how to use the Snowflake REST API to retrieve data for a web application using TypeScript","date":"2022-05-19T00:00:00.000Z","formattedDate":"May 19, 2022","tags":[{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"sql","permalink":"/tags/sql"},{"label":"typescript","permalink":"/tags/typescript"},{"label":"react","permalink":"/tags/react"},{"label":"jamstack","permalink":"/tags/jamstack"}],"readingTime":2.045,"hasTruncateMarker":false,"authors":[{"name":"Yuncheng Yang","title":"Full Stack Engineer","url":"https://www.linkedin.com/in/yuncheng-fabio-yang/","imageURL":"https://en.gravatar.com/userimage/195643035/72fc562ee87d0c67847c8989d2808129.jpg?size=80","key":"yunchengyang"}],"frontMatter":{"slug":"using-the-snowflake-sql-api-with-typescript","title":"Using the Snowflake SQL API with TypeScript","authors":["yunchengyang"],"draft":false,"image":"/img/blog/snowflake-sql-api-featured-image.png","tags":["snowflake","sql","typescript","react","jamstack"],"keywords":["snowflake","sql","typescript","react","jamstack"],"description":"This article demonstrates how to use the Snowflake REST API to retrieve data for a web application using TypeScript"},"prevItem":{"title":"DataOps with Container Images and Multi-Stage Builds","permalink":"/dataops-with-container-images-and-multi-stage-builds"},"nextItem":{"title":"Split a large Open API or Swagger Specification into smaller documents","permalink":"/split-a-large-swagger-openapi-specification-into-smaller-documents"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nThis article demonstrates how to use the Snowflake REST API to retrieve data for a web application using TypeScript, in this case we are using keypair authentication with Snowflake.  \\r\\n\\r\\n## Overview\\r\\n\\r\\nSnowflake\u2019s SQL API allows you to access snowflake objects using SQL via a REST API request, this allows for easy integration with your applications and deployment pipelines. You can use the API to execute most DDL and DML statements.  \\r\\n\\r\\nThere are some limitations you need to be aware of however, for example interactions with stages (using PUT and GET aren\u2019t supported via the Snowflake API) or stored procedure operations (using CALL), you can read more on this [here](https://docs.snowflake.com/en/developer-guide/sql-api/intro.html#limitations-of-the-sql-api).  \\r\\n\\r\\n\\r\\n## Endpoints\\r\\n\\r\\nThere are three endpoints provided:  \\r\\n\\r\\n- `/api/v2/statements/`\\r\\n- `/api/v2/statement/<statementHandle>`\\r\\n- `/api/v2/statements/<statementHandle/cancel`\\r\\n\\r\\nWe will be looking at the first two in this article.  \\r\\n\\r\\n## Authentication Methods\\r\\n\\r\\nThere are two types of Authentication methods for the API, __OAuth__ and __Key Pair__. For OAuth method, you can choose to use `X-Snowflake-Authorization-Token-Type` header, if this header is not present, Snowflake assumes that the token in the `Authorization` header is an OAuth token. For Key Pair method, the JWT token will be in the `Authorization` header as `Bearer <your token>`.  \\r\\n\\r\\nLet\u2019s walk through how to generate and use the JWT.  \\r\\n\\r\\n## Generating the JWT\\r\\n\\r\\nHere\'s whats needed:  \\r\\n\\r\\n[![Snowflake JWT](images/snowflake-jwt.png)](images/snowflake-jwt.png)\\r\\n\\r\\n### the Code\\r\\n\\r\\n<Gist id=\\"6fbe63cace2ad993ac06b324954b7daa\\" \\r\\n/>\\r\\n\\r\\n## Request Body\\r\\n\\r\\nNow we need a request body:  \\r\\n\\r\\n<Gist id=\\"ae0ebbedf51f232e7147e72f11007b68\\" \\r\\n/>\\r\\n\\r\\n## Submitting the Request\\r\\n\\r\\nWe will need to include the __region__ and __account identifier__, for instance if your account identifier includes a region (e.g. xy12345.us-east2.aws.snowflakecomputing.com).  \\r\\n\\r\\n<Gist id=\\"535123dda1536b5a48c6213470e83d6f\\" \\r\\n/>\\r\\n\\r\\n## Response Handling\\r\\n\\r\\nWhen making a `SELECT` query, there are three things worth noting:  \\r\\n1.\\t`rowType` fields in the `resultSetMetaData` represent the columns\\r\\n2.\\tdata without column names is in the format of `string[][]`\\r\\n3.\\t`partitionInfo` is an array of object representing different partitions\\r\\n\\r\\nFor more information see [Handling Responses from the SQL API - Snowflake Documentation](https://docs.snowflake.com/en/developer-guide/sql-api/handling-responses.html).  \\r\\n\\r\\n### Parsing data\\r\\n\\r\\nHere is a Typescript code snippet demonstrating parsing return data:  \\r\\n\\r\\n<Gist id=\\"d397621879b063ea0761233984aafe69\\" \\r\\n/>\\r\\n\\r\\n### Handling multiple partitions\\r\\n\\r\\nLarge result sets are paginated into *partitions*, each partition is a set of rows.\\r\\n\\r\\n:::note\\r\\n\\r\\nNote that the pages (referred to as partitions) are __NOT__ based on row count, instead they are based on the compressed batch size, so they will not be uniform in terms of the number of rows.\\r\\n\\r\\n:::\\r\\n\\r\\nTo get a partition, send a `GET` request with Url `https://<accountIdentifier>.snowflakecomputing.com/api/v2/statements/?partition=<partitionId>`.  \\r\\n\\r\\n<Gist id=\\"7f2b0443a9ca5e8284b987a9e84ca301\\" \\r\\n/>\\r\\n\\r\\nThanks!"},{"id":"split-a-large-swagger-openapi-specification-into-smaller-documents","metadata":{"permalink":"/split-a-large-swagger-openapi-specification-into-smaller-documents","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-05-02-split-a-large-swagger-openapi-specification-into-smaller-documents/index.md","source":"@site/blog/2022-05-02-split-a-large-swagger-openapi-specification-into-smaller-documents/index.md","title":"Split a large Open API or Swagger Specification into smaller documents","description":"Simple utility to split a large Open API or Swagger specification into smaller documents.","date":"2022-05-02T00:00:00.000Z","formattedDate":"May 2, 2022","tags":[{"label":"openapi","permalink":"/tags/openapi"},{"label":"swagger","permalink":"/tags/swagger"},{"label":"api","permalink":"/tags/api"},{"label":"stackql","permalink":"/tags/stackql"}],"readingTime":1.415,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"split-a-large-swagger-openapi-specification-into-smaller-documents","title":"Split a large Open API or Swagger Specification into smaller documents","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/split-up-large-openapi-docs.png","tags":["openapi","swagger","api","stackql"],"keywords":["openapi","swagger","api","stackql"],"description":"Simple utility to split a large Open API or Swagger specification into smaller documents."},"prevItem":{"title":"Using the Snowflake SQL API with TypeScript","permalink":"/using-the-snowflake-sql-api-with-typescript"},"nextItem":{"title":"Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python","permalink":"/stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python"}},"content":"Open API specifications can get quite large, especially for providers with upwards of 500 routes or operations.  \\r\\n\\r\\nThe challenge is to create standalone documents scoped by a service or path within the parent API specification and include only the components (schemas, responses, etc.) that pertain to operations included in the child document.  \\r\\n\\r\\nWhen I went looking for library or utility to do this, I couldn\u2019t find one... so I have developed one myself.  \\r\\n\\r\\nIt\'s a simple command (nodejs based but can be run in a bash terminal or from the Windows command line) which requires a few options, including:  \\r\\n\\r\\n- the __*provider name*__ (e.g. `github`)\\r\\n- a __*provider version*__ which is a version you set - allowing you to make minor modifications to the output documents (e.g. `v0.1.0`)\\r\\n- a __*service discriminator*__ which is a JSONPath expression to identify a service name within each route in the parent file, this is used to assign operations to services in separate documents (e.g. `\'$[\\"x-github\\"].category\'`)\\r\\n- an __*output directory*__ (e.g. `./dev`)\\r\\n\\r\\nand of course, the __*openapi spec document*__ you are splitting up.\\r\\n\\r\\nan example is shown here:\\r\\n\\r\\n```\\r\\nopenapi-doc-util split \\\\\\r\\n-n github \\\\\\r\\n-v v0.1.0 \\\\\\r\\n-s \'$[\\"x-github\\"].category\' \\\\\\r\\n-o ./dev \\\\\\r\\nref/github/api.github.com.yaml\\r\\n```\\r\\n\\r\\nHelp for the command is available using `openapi-doc-util split`.  \\r\\n\\r\\nThe net result is 59 self-contained, service scoped documents, containing only the components referenced by routes in the service document.\\r\\n\\r\\nYou can access this utility via [__NPMJS__](https://www.npmjs.com/package/@stackql/openapi-doc-util) or via [__GitHub__](https://github.com/stackql/openapi-doc-util).  \\r\\n\\r\\nSplitting up a large open API spec document, is the first stage in developing a [__StackQL__](https://github.com/stackql/stackql) provider which we will discuss next time!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python","metadata":{"permalink":"/stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-04-28-stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python/index.md","source":"@site/blog/2022-04-28-stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python/index.md","title":"Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python","description":"Simple demonstration of stream processing with Spark Structured Streaming, Kafka and Snowflake using Python","date":"2022-04-28T00:00:00.000Z","formattedDate":"April 28, 2022","tags":[{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"kafka","permalink":"/tags/kafka"},{"label":"spark","permalink":"/tags/spark"},{"label":"sql","permalink":"/tags/sql"},{"label":"streaming","permalink":"/tags/streaming"}],"readingTime":3.65,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python","title":"Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/kafka-spark-snowflake.png","tags":["snowflake","kafka","spark","sql","streaming"],"keywords":["snowflake","kafka","spark","sql","streaming"],"description":"Simple demonstration of stream processing with Spark Structured Streaming, Kafka and Snowflake using Python"},"prevItem":{"title":"Split a large Open API or Swagger Specification into smaller documents","permalink":"/split-a-large-swagger-openapi-specification-into-smaller-documents"},"nextItem":{"title":"Simple CLI Application to Login to Okta using PKCE","permalink":"/simple-cli-pkce-auth-using-okta"}},"content":"Structured Streaming in Spark provides a powerful framework for stream processing an analysis, such as streaming transformations, stateful streaming or sliding window operations.  \\r\\n\\r\\nKafka is a common streaming source and sink for Spark Streaming and Structured Streaming operations.  However, there may be situations where a data warehouse (such as Snowflake) is a more appropriate target for streaming operations, especially where there is a reporting or long-term storage requirement on the data derived from the streaming source.  \\r\\n\\r\\nThis article will demonstrate just how easy this is to implement using Python.  \\r\\n\\r\\n## Design\\r\\n\\r\\nThe following diagram illustrates the ingestion design for this example:  \\r\\n\\r\\n[![Spark Structured Streaming using Kafka and Snowflake](images/spark-streaming-kafka-snowflake.png)](images/spark-streaming-kafka-snowflake.png)\\r\\n\\r\\n## Snowflake Setup\\r\\n\\r\\nSome prerequisites for Snowflake:  \\r\\n\\r\\n1.\\tYou will need to create a user (or use an existing user), in either case the user will need to be identified by a private key.  You will need to generate a key pair as follows:  \\r\\n\\r\\n```bash\\r\\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\\r\\nopenssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\\r\\n```\\r\\ncopy the contents of the `rsa_key.pub` file, remove the `-----BEGIN PUBLIC KEY-----` and `-----END PUBLIC KEY-----` strings, then remove the line breaks to form one string, use this string as the `RSA_PUBLIC_KEY` in a `CREATE USER` or `ALTER USER` statement in Snowflake, like:  \\r\\n\\r\\n```sql\\r\\nALTER USER youruser SET RSA_PUBLIC_KEY=\'MIIBI...\';\\r\\n```\\r\\n\\r\\n2.  Now setup the target database, schema and table you will use to write out your stream data (the schema for the table must match the schema for the Data Stream you will use the `DataStreamWriter` to emit records to Snowflake  \\r\\n\\r\\nThe user you will be using (that you setup the key pair authentication for) will need to be assigned a default role to which the appropriate write permissions are granted to the target objects in Snowflake.  You will also need to designate a virtual warehouse (which your user must have `USAGE` permissions to.  \\r\\n\\r\\n## The Code\\r\\n\\r\\nNow that we have the objects and user setup in Snowflake, we can construct our Spark application.  \\r\\n\\r\\nFirst, you will need to start your Spark session (either using `pyspark` or `spark-submit`) including the packages that Spark will need to connect to Kafka and to Snowflake.  \\r\\n\\r\\nThe Snowflake packages include a JDBC driver and the Snowflake Connector for Spark, see [Snowflake Connector for Spark](https://docs.snowflake.com/en/user-guide/spark-connector.html).  \\r\\n\\r\\nAn example is shown here (package versions may vary depending upon the version of Spark you are using):  \\r\\n\\r\\n```bash\\r\\npyspark \\\\\\r\\n--packages \\\\\\r\\nnet.snowflake:snowflake-jdbc:3.13.14,\\\\\\r\\nnet.snowflake:spark-snowflake_2.12:2.10.0-spark_3.1,\\\\\\r\\norg.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\\r\\n```\\r\\n\\r\\nNow that we have a spark session with the necessary packages, lets go...  \\r\\n\\r\\n```python\\r\\n# import any required functions, set the checkpoint directory, and log level (optional)\\r\\nfrom pyspark.sql.functions import split\\r\\nspark.sparkContext.setLogLevel(\\"ERROR\\")\\r\\nspark.conf.set(\\"spark.sql.streaming.checkpointLocation\\", \\"file:///tmp\\")\\r\\n```\\r\\n\\r\\nsetup connection options for Snowflake by creating an `sfOptions` dictionary  \\r\\n\\r\\n```python\\r\\nsfOptions = {\\r\\n\\t  \\"sfURL\\" : sfUrl,\\r\\n\\t  \\"sfUser\\" : \\"avensolutions\\",\\r\\n\\t  \\"pem_private_key\\": private_key,\\r\\n\\t  \\"sfDatabase\\" : \\"SPARK_SNOWFLAKE_DEMO\\",\\r\\n\\t  \\"sfSchema\\" : \\"PUBLIC\\",\\r\\n\\t  \\"sfWarehouse\\" : \\"COMPUTE_WH\\",\\r\\n\\t  \\"streaming_stage\\" : \\"mystage\\"\\r\\n}\\r\\n```\\r\\n\\r\\nset a variable for the Snowflake Spark connector  \\r\\n\\r\\n```python\\r\\nSNOWFLAKE_SOURCE_NAME = \\"net.snowflake.spark.snowflake\\"\\r\\n```\\r\\n\\r\\nread messages from Kafka:    \\r\\n\\r\\n```python\\r\\nlines = spark \\\\\\r\\n  .readStream \\\\\\r\\n  .format(\\"kafka\\") \\\\\\r\\n  .option(\\"kafka.bootstrap.servers\\", \\"kafkabroker:9092\\") \\\\\\r\\n  .option(\\"subscribe\\", \\"weblogs\\") \\\\\\r\\n  .load()\\r\\n```\\r\\n\\r\\nperform necessary transformations (the fields and data types in the resultant data structure must match the target table you created in Snowflake:  \\r\\n\\r\\n```python\\r\\nlog_recs = lines.select(\\r\\n    split(lines.value.cast(\\"string\\"), \\" \\").alias(\\"data\\")\\r\\n    )\\r\\n    \\r\\nlog_data = log_recs.selectExpr(\\r\\n  \\"CAST(data[0] as string) as date\\",\\r\\n  \\"CAST(data[1] as string) as time\\",\\r\\n  \\"CAST(data[2] as string) as c_ip\\",  \\r\\n  \\"CAST(data[3] as string) as cs_username\\",\\r\\n  \\"CAST(data[4] as string) as s_sitename\\",  \\r\\n  \\"CAST(data[5] as string) as s_computername\\",\\r\\n  \\"CAST(data[6] as string) as s_ip\\",    \\r\\n  \\"CAST(data[7] as int) as s_port\\",  \\r\\n  \\"CAST(data[8] as string) as cs_method\\",    \\r\\n  \\"CAST(data[9] as string) as cs_uri_stem\\",  \\r\\n  \\"CAST(data[10] as string) as cs_uri_query\\",  \\r\\n  \\"CAST(data[11] as int) as sc_status\\",\\r\\n  \\"CAST(data[12] as int) as time_taken\\",    \\r\\n  \\"CAST(data[13] as string) as cs_version\\",    \\r\\n  \\"CAST(data[14] as string) as cs_host\\",\\r\\n  \\"CAST(data[15] as string) as User_Agent\\",\\r\\n  \\"CAST(data[16] as string) as Referer\\",    \\r\\n)\\r\\n```\\r\\n\\r\\nwrite to Snowflake!  \\r\\n\\r\\n```python\\r\\nquery = log_data\\\\\\r\\n\\t.writeStream\\\\\\r\\n\\t.format(SNOWFLAKE_SOURCE_NAME) \\\\\\r\\n\\t.options(**sfOptions) \\\\\\r\\n\\t.option(\\"dbtable\\", \\"WEB_LOGS\\") \\\\\\r\\n\\t.trigger(processingTime=\'30 seconds\') \\\\\\r\\n\\t.start()\\r\\n\\t\\r\\nquery.awaitTermination()\\r\\n```\\r\\n\\r\\n:::info\\r\\n\\r\\nNote that I have included the `processingTime` trigger of `30 seconds` (this is akin to the `batchInterval` in the DStream API), you should tune this to get a balance between batch sizes to ingest into Snowflake (which will benefit from larger batches) and latency.\\r\\n\\r\\n:::\\r\\n\\r\\n## The Results\\r\\n\\r\\n[![Spark Structured Streaming into Snowflake](images/snowflake-screenshot.png)](images/snowflake-screenshot.png)\\r\\n\\r\\nEnjoy!  \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"simple-cli-pkce-auth-using-okta","metadata":{"permalink":"/simple-cli-pkce-auth-using-okta","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-04-17-simple-cli-pkce-auth-using-okta/index.md","source":"@site/blog/2022-04-17-simple-cli-pkce-auth-using-okta/index.md","title":"Simple CLI Application to Login to Okta using PKCE","description":"Simple command line application to login to an Okta authorization server using a PKCE flow.","date":"2022-04-17T00:00:00.000Z","formattedDate":"April 17, 2022","tags":[{"label":"okta","permalink":"/tags/okta"},{"label":"oauth2","permalink":"/tags/oauth-2"},{"label":"cli","permalink":"/tags/cli"},{"label":"golang","permalink":"/tags/golang"},{"label":"pkce","permalink":"/tags/pkce"}],"readingTime":2.185,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"simple-cli-pkce-auth-using-okta","title":"Simple CLI Application to Login to Okta using PKCE","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/okta-pkce-cli-blog-image.png","tags":["okta","oauth2","cli","golang","pkce"],"keywords":["okta","oauth2","cli","golang","pkce"],"description":"Simple command line application to login to an Okta authorization server using a PKCE flow."},"prevItem":{"title":"Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python","permalink":"/stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python"},"nextItem":{"title":"Scaling up Prefect with GitStorage","permalink":"/scaling-up-prefect-with-gitstorage"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nThis article demonstrates a simple command line utility to login to an authorization server (Okta in this case) using a PKCE (Proof Key for Code Exchange) flow.  This is the preferred flow for public clients (such as Single Page Applications).  \\r\\n\\r\\n> The code for this article is available on [__GitHub__](https://github.com/stackql/okta-pkce-login)\\r\\n\\r\\n## Example\\r\\n\\r\\n[![Okta PKCE cli login example](images/okta-pkce-cli-login.png)](images/okta-pkce-cli-login.png)\\r\\n\\r\\n## Overview\\r\\n\\r\\nThis application can be used to illustrate the authorization/authentication flow discussed in [Simple SSO with an external IdP using Active Directory and Okta](https://fullstackchronicles.io/simple-sso-with-an-external-idp-using-active-directory-and-okta).  A flow which is pictured here:  \\r\\n\\r\\n[![PKCE Authorization t Okta using an AD IdP](images/seqdiagram.png)](images/seqdiagram.png)\\r\\n\\r\\n## Steps\\r\\n\\r\\nThe steps involved in the implementation of a PKCE login flow are as follows:\\r\\n\\r\\n### Generate a `code_challenge`\\r\\n\\r\\nTo implement a PKCE flow, you first need to generate a *Code Verifier* (which is a random value you create), the *Code Verifier* is then hashed using a SHA256 algorithm.  The hash is then used as the *Code Challenge*.  An example function to generate a code challenge is shown below:  \\r\\n \\r\\n<Gist id=\\"9a2a162813d77b83821d821b6a4a390a\\" \\r\\n/>\\r\\n\\r\\nFor more information see [Use PKCE to Make Your Apps More Secure](https://developer.okta.com/blog/2019/08/22/okta-authjs-pkce#:~:text=PKCE%20works%20by%20having%20the,is%20called%20the%20Code%20Challenge). \\r\\n\\r\\n### Build the `authorize` url\\r\\n\\r\\nThe `authorize` url is used to initiate the authorization flow with the authorization server.  An example function to construct the `authorize` url is shown below:  \\r\\n\\r\\n<Gist id=\\"9e628b905a532e5bd59f022a4adca340\\" \\r\\n/>\\r\\n\\r\\n### Get the authorization code via redirect uri\\r\\n\\r\\nThe `redirecturi` parameter supplied in the `authorize` url is used to retrieve the authorization code from the authorization server.  In order to get this code using a front end flow, you need to define a handler that will get the authorization code, call the token endpoint, and close the HTTP server, as shown here:  \\r\\n\\r\\n<Gist id=\\"617417bdcc54efcea9d37d27228f7f2a\\" \\r\\n/>\\r\\n\\r\\n### Exchange the code for an access token\\r\\n\\r\\nThe access token is what you ultimatly want, as this is the token that will be used to access protected resources.  An example function to exchange the authorization code for an access token is shown below:  \\r\\n\\r\\n<Gist id=\\"0a990674d8bde2baffc0b0231f52ed52\\" \\r\\n/>\\r\\n\\r\\n### (Optional) Get the user profile\\r\\n\\r\\nThe access token can be used to get the user profile, this is done by calling the `userinfo` endpoint using the token.  An example function to get the user profile is shown below:  \\r\\n\\r\\n<Gist id=\\"f04e8b018417a73986d3696c58f735cb\\" \\r\\n/>\\r\\n\\r\\n## with inspiration from...\\r\\n\\r\\n- [Auth0 PKCE flow for a CLI built in golang](https://gist.github.com/ogazitt/f749dad9cca8d0ac6607f93a42adf322)\\r\\n- [Golang sample for a CLI obtaining an access token using the PKCE flow](https://community.auth0.com/t/golang-sample-for-a-cli-obtaining-an-access-token-using-the-pkce-flow/40922)\\r\\n- [oktadev/okta-node-cli-example](https://github.com/oktadev/okta-node-cli-example)\\r\\n- [Build a Command Line Application with Node.js](https://developer.okta.com/blog/2019/06/18/command-line-app-with-nodejs)\\r\\n- [About the Authorization Code grant with PKCE](https://developer.okta.com/docs/guides/implement-grant-type/authcodepkce/main/#about-the-authorization-code-grant-with-pkce)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"scaling-up-prefect-with-gitstorage","metadata":{"permalink":"/scaling-up-prefect-with-gitstorage","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-02-28-scaling-up-prefect-with-gitstorage/index.md","source":"@site/blog/2022-02-28-scaling-up-prefect-with-gitstorage/index.md","title":"Scaling up Prefect with GitStorage","description":"Prefect.io is a python based Data Engineering toolbox for building and","date":"2022-02-28T00:00:00.000Z","formattedDate":"February 28, 2022","tags":[{"label":"prefect","permalink":"/tags/prefect"},{"label":"gitlab","permalink":"/tags/gitlab"},{"label":"docker","permalink":"/tags/docker"},{"label":"etl","permalink":"/tags/etl"}],"readingTime":6.12,"hasTruncateMarker":false,"authors":[{"name":"Chris Ottinger","title":"Senior Technologist","url":"https://github.com/datwiz","imageURL":"http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80","key":"chrisottinger"}],"frontMatter":{"slug":"scaling-up-prefect-with-gitstorage","title":"Scaling up Prefect with GitStorage","authors":["chrisottinger"],"draft":false,"image":"/img/blog/scaling-up-prefect-with-gitstorage-featured-image.png","tags":["prefect","gitlab","docker","etl"],"keywords":["prefect","gitlab","docker","etl"]},"prevItem":{"title":"Simple CLI Application to Login to Okta using PKCE","permalink":"/simple-cli-pkce-auth-using-okta"},"nextItem":{"title":"Implementing a Serverless SFTP Gateway using the AWS Transfer Family","permalink":"/implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\n\\r\\n[Prefect.io](https://prefect.io) is a python based Data Engineering toolbox for building and\\r\\noperating Data Pipelines.  Out of the box, Prefect provides an initial workflow for managing data\\r\\npipelines that results in a container image per data pipeline job.\\r\\n\\r\\nThe one-to-one relationship between data pipeline jobs and container images enables data engineers to\\r\\ncraft pipelines that are loosely coupled and don\'t require a shared runtime environment configuration.\\r\\nHowever, as the number of data pipeline jobs grow the default container per job approach starts to\\r\\nintroduce workflow bottlenecks and lifecycle management overheads.  For example, in order\\r\\nto update software components used by flows, such as upgrading the version of Prefect, all the data\\r\\npipeline job images have to be rebuilt and redeployed.  Additionally the container image per job workflow\\r\\nintroduces a wait time for data engineers to re-build data pipeline container images and test flows\\r\\ncentrally on Prefect Server or Prefect Cloud environment.\\r\\n\\r\\nFortunately, Prefect comes to its own rescue with the ability to open up the box, exposing the flexibility\\r\\nin the underlying framework.\\r\\n\\r\\n## Out of the box - Prefect DockerStorage\\r\\nOut of the box, Prefect provides a simple workflow for defining and deploying data pipelines as container images.\\r\\nAfter getting a first data pipeline running in a local environment, the attention turns to scaling up development\\r\\nand deploying flows into a managed environment, using either the Prefect Cloud service or a Prefect Server.\\r\\n\\r\\nCombining Prefect Cloud or Prefect Server with Kubernetes provides a flexible and scalable platform\\r\\nsolution for moving data pipelines into production.  There are a number of options for packaging\\r\\ndata pipeline flow code for execution on kubernetes clusters.  The Docker Storage option provides\\r\\nthe workflow for bundling the data pipeline job code into container images, enabling a common\\r\\ncontrolled execution environment and well understood distribution mechanism.  The data pipeline runs as\\r\\na pod using the flow container image.\\r\\n\\r\\nPrefect Docker Storage workflow steps for building and deploying data pipeline flows include:\\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"steps\\"\\r\\n  values={[\\r\\n    { label: \'Steps\', value: \'steps\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"steps\\">\\r\\n\\r\\n[![Workflow Steps](images/image1.png)](images/image1.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```\\r\\n@startuml \\"docker-storage-workflow\\"\\r\\n(*) --\x3e \\"package flow code\\r\\ninto a container image\\" \\r\\n--\x3e \\"register Prefect flow\\r\\nusing image reference\\"\\r\\n--\x3e \\"push image to container registry\\"\\r\\n--\x3e \\"run flow in Prefect Server or Cloud\\r\\n(new image pulled from registry)\\"\\r\\n--\x3e (*)\\r\\n@enduml\\r\\n```\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n* packaging a flow (python code) as a serialised/pickled object into a container image\\r\\n* registering the flow using the container image name\\r\\n* pushing the container image to a container repository accessible from the kubernetes cluster\\r\\n* running the flow by running an instance of the named container image as a kubernetes pod\\r\\n\\r\\nThis is relatively simple immutable workflow.  Each data pipeline flow version is effectively a unique and\\r\\nself contained \'point-in-time\' container image.  This initial workflow can also be extended to package\\r\\nmultiple related flows into a single container image, reducing the number of resulting container images.\\r\\nBut, as the number of data pipeline jobs grow, there issues of container image explosion and data engineering\\r\\nproductivity remain.\\r\\n\\r\\nUsing Prefect GitStorage for flows addresses both container image proliferation as well as development\\r\\nbottlenecks.\\r\\n\\r\\n## Prefect Git Storage\\r\\nPrefect [Git Storage](https://docs.prefect.io/orchestration/flow_config/storage.html#git) provides a workflow for developing and deploying data pipelines directly from git repositories,\\r\\nsuch as Gitlab or Github.  The data pipeline code (python) is pulled from the git repository on each invocation\\r\\nwith the ability to reference git branches and git tags.  This approach enables:\\r\\n* reducing the number of container images to the number of different runtime configurations to be supported.\\r\\n* improving the data engineering development cycle time by removing the need to build and push container images\\r\\non each code change.\\r\\n* when combined with kubernetes Prefect Run Configs and Job templates, enables selection of specific runtime environment images\\r\\n\\r\\nNote that the GitStorage option does required access from the runtime kubernetes cluster to the central git storage\\r\\nservice, e.g. gitlab, github, etc.\\r\\n\\r\\nPrefect Git Storage workflow steps for \'building\' and deploying data pipeline flows include:\\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"steps\\"\\r\\n  values={[\\r\\n    { label: \'Steps\', value: \'steps\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"steps\\">\\r\\n\\r\\n[![Workflow Steps](images/image2.png)](images/image2.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```\\r\\n@startuml \\"git-storage-workflow\\"\\r\\n(*) --\x3e \\"push commited flow code\\r\\nchanges to git service\\"\\r\\n--\x3e \\"register PrefectFlow\\r\\nusing branch or tag reference\\"\\r\\n--\x3e \\"run flow in Prefect Server or Cloud\\r\\n(code pulled from git service)\\"\\r\\n--\x3e (*)\\r\\n@enduml\\r\\n```\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n* pushing the committed code to the central git service\\r\\n* registering the flow using the git repository url and branch or tag reference\\r\\n* running the flow by pulling the reference code from the git service in a kubernetes pod\\r\\n\\r\\nThe container image build and push steps are removed from the developer feedback cycle time.\\r\\nDepending on network bandwidth and image build times, this can save remove 5 to 10 minutes from each deployment iteration.\\r\\n\\r\\n### Pushing the flow code\\r\\nOnce a set of changes to the data pipeline code has been committed, push to the central git service.\\r\\n```bash\\r\\n$ git commit\\r\\n$ git push\\r\\n```\\r\\n\\r\\n### Registering the flow\\r\\nThe flow can be registered with Prefect using either a branch (HEAD or latest) or tag reference.  Assuming\\r\\na workflow with feature branches:\\r\\n* feature branches: register the flow code using the feature branch.  This enables the latest version (HEAD)\\r\\nof the pushed flow code to be used for execution.  It also enables skipping re-registration of the flow on new\\r\\nchanges as the HEAD of the branch is pulled on each flow run\\r\\n* main line branches: register pinned versions of the flow using git tags.  This enables the use of a \\r\\nspecific version of the flow code to be pulled on each flow run, regardless of future changes.\\r\\n\\r\\nDetermining the which reference to use:\\r\\n```python\\r\\n# using gitpython module to work with repo info\\r\\nfrom git import Repo\\r\\n\\r\\n# presidence for identifing where to find the flow code\\r\\n# BUILD_TAG => GIT_BRANCH => active_branch\\r\\nbuild_tag = branch_name = None\\r\\nbuild_tag = os.getenv(\\"BUILD_TAG\\", \\"\\")\\r\\nif build_tag == \\"\\":\\r\\n  branch_name = os.getenv(\\"GIT_BRANCH\\", \\"\\")\\r\\n  if branch_name == \\"\\":\\r\\n    branch_name = str(Repo(os.getcwd()).active_branch)\\r\\n```\\r\\n\\r\\nConfiguring Prefect Git storage:\\r\\n```python\\r\\nfrom prefect.storage import Git\\r\\nimport my_flows.hello_flow as flow # assuming flow is defined in ./my_flows/flow.py\\r\\n\\r\\n# example using Gitlab\\r\\n# either branch_name or tag must be empty string \\"\\" or None\\r\\nstorage = Git(\\r\\n    repo_host=git_hostname,\\r\\n    repo=repo_path,\\r\\n    flow_path=f\\"{flow.__name__.replace(\'.\',\'/\')}.py\\",\\r\\n    flow_name=flow.flow.name,\\r\\n    branch_name=branch_name,\\r\\n    tag=build_tag,\\r\\n    git_token_secret_name=git_token_secret_name,\\r\\n    git_token_username=git_token_username\\r\\n)\\r\\n\\r\\nstorage.add_flow(flow.flow)\\r\\nflow.flow.storage = storage\\r\\n\\r\\nflow.flow.regsiter(build=False)\\r\\n```\\r\\n\\r\\nOnce registered, the flow storage details can be viewed in the Prefect Server or Prefect Cloud UI.  In this example, Prefect will use the `HEAD` version of the `main` branch on each flow run.\\r\\n\\r\\n[![hello flow storage details](images/flow-storage-details.png)](images/flow-storage-details.png)\\r\\n\\r\\n## Next Steps - Run Config\\r\\nWith Prefect Git Storage the runtime configuration and environment management is decoupled from the\\r\\ndata pipeline development workflow.  Unlike with Docker Storage, with Git Storage, the runtime\\r\\nexecution environment and data pipeline development workflows are defined and managed separately.\\r\\nAs an added benefit, the developer feedback loop cycle time is also reduced.\\r\\n\\r\\nWith the data engineering workflow addressed, the next step in scaling out the Prefect solution\\r\\nturns to configuration and lifecycle management of the runtime environment for data pipelines.\\r\\nPrefect Run Configs and Job templates provide the tools retaining the flexibility on container\\r\\nimage based runtime environments with improved manageability."},{"id":"implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family","metadata":{"permalink":"/implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-02-23-implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family/index.md","source":"@site/blog/2022-02-23-implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family/index.md","title":"Implementing a Serverless SFTP Gateway using the AWS Transfer Family","description":"An example implementation of an SFTP gateway using the AWS Transfer Family service, to store client data in an encrypted S3 bucket.","date":"2022-02-23T00:00:00.000Z","formattedDate":"February 23, 2022","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"aws transfer family","permalink":"/tags/aws-transfer-family"},{"label":"serverless","permalink":"/tags/serverless"},{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"snowpipe","permalink":"/tags/snowpipe"},{"label":"sftp","permalink":"/tags/sftp"},{"label":"managed file transfer","permalink":"/tags/managed-file-transfer"}],"readingTime":6.03,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family","title":"Implementing a Serverless SFTP Gateway using the AWS Transfer Family","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/aws-transfer-for-sftp.png","tags":["aws","aws transfer family","serverless","snowflake","snowpipe","sftp","managed file transfer"],"keywords":["aws","aws transfer family","serverless","snowflake","snowpipe","sftp","managed file transfer"],"description":"An example implementation of an SFTP gateway using the AWS Transfer Family service, to store client data in an encrypted S3 bucket."},"prevItem":{"title":"Scaling up Prefect with GitStorage","permalink":"/scaling-up-prefect-with-gitstorage"},"nextItem":{"title":"Simple SSO with an external IdP using Active Directory and Okta","permalink":"/simple-sso-with-an-external-idp-using-active-directory-and-okta"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\nimport Gist from \'react-gist\';\\r\\n\\r\\n> When you want the SFTP service without the SFTP Server.  \\r\\n\\r\\nIn implementing data platforms with external data providers, it is common to use a managed file transfer platform or an SFTP gateway as an entry point for providers to supply data to your system.  \\r\\n\\r\\nOften in past implementations this would involve deploying a sever (typically a Linux VM) and provisioning and configuring an SFTP service.  If you wanted the data sent by clients to be copied to another storage medium (such as S3 or EFS) you would need to roll your own code or subscribe to a marketplace offering to do so.  \\r\\n\\r\\nI recently trialled the [AWS Transfer Family SFTP gateway](https://docs.aws.amazon.com/transfer/index.html) offering from AWS and sharing my adventures here.  \\r\\n\\r\\n## Architecture\\r\\n\\r\\nIn this reference architecture, we are deploying an SFTP service which uses a path in an S3 bucket as a user\u2019s home directory.  Objects in the bucket are encrypted with a customer managed KMS key.  The SFTP server front end address is mapped to a vanity URL using Route53.  The bucket and path are integrated with a `STORAGE INTEGRATION`, `STAGE` and `PIPE` definition in Snowflake.  The Snowflake bits are covered in more detail in this blog: __[Automating Snowflake Role Based Storage Integration for AWS](automating-snowflake-role-based-storage-integration-for-aws)__.  This article just details the AWS Transfer Family SFTP setup.\\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"architecture\\"\\r\\n  values={[\\r\\n    { label: \'Architecture\', value: \'architecture\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"architecture\\">\\r\\n\\r\\n[![AWS Transfer SFTP Architecture](images/aws-transfer-sftp-architecture.png)](images/aws-transfer-sftp-architecture.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```\\r\\n@startuml\\r\\n\\r\\nskinparam rectangle<<boundary>> {\\r\\n    Shadowing false\\r\\n    StereotypeFontSize 0\\r\\n    FontColor #444444\\r\\n    BorderColor #444444\\r\\n    BorderStyle dashed\\r\\n}\\r\\n\\r\\nskinparam defaultTextAlignment center\\r\\n\\r\\n!$imgroot = \\"https://github.com/avensolutions/plantuml-cloud-image-library/raw/main/images\\"\\r\\n\\r\\n!unquoted procedure $AwsS3($alias, $label, $techn, $descr=\\"\\", $stereo=\\"AWS S3\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/aws/Storage/S3.png>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $Kms($alias, $label, $techn, $descr=\\"\\", $stereo=\\"AWS KMS\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/aws/SecurityIdentityCompliance/kms.png{scale=0.80}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $Route53($alias, $label, $techn, $descr=\\"\\", $stereo=\\"AWS Route53\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/aws/Networking/route53.png{scale=0.80}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $AwsTransferFamily($alias, $label, $techn, $descr=\\"\\", $stereo=\\"AWS Transfer Family\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/aws/MigrationTransfer/TransferFamily.png>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $Data($alias, $label, $techn, $descr=\\"\\", $stereo=\\"Data\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/general/documents.png>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $Snowpipe($alias, $label, $techn, $descr=\\"\\", $stereo=\\"Snowpipe\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/snowflake/snowpipe.png{scale=0.60}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $SnowflakeDb($alias, $label, $techn, $descr=\\"\\", $stereo=\\"Snowflake DB\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/snowflake/snowflakeDB.png{scale=0.70}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n$Data(supplier, Data Supplier, External Client)\\r\\n\\r\\nrectangle \\"AWS Environment\\" <<boundary>> {\\r\\n    $AwsTransferFamily(sftpgw, SFTP/FTPS Gateway, AWS Transfer Family)\\r\\n    $AwsS3(s3staging, Staging Bucket, AWS S3 Bucket)\\r\\n    $Kms(kms, KMS Key, Customer Managed Key)\\r\\n    $Route53(r53, CNAME Record, Route53 Record)\\r\\n}\\r\\n\\r\\nrectangle \\"Snowflake Environment\\" <<boundary>> {\\r\\n    $Snowpipe(snowpipe, Snowpipe, Snowpipe)\\r\\n    $SnowflakeDb(db, Snowflake DB, Snowflake DB)\\r\\n}\\r\\n\\r\\nr53 -[hidden]D- sftpgw\\r\\nsupplier -> r53 : resolves name\\r\\nr53 -> supplier : gets address\\r\\nsupplier -RIGHT-> sftpgw : SFTP\\r\\nsftpgw -DOWN-> kms : uses\\r\\nsftpgw -RIGHT-> s3staging: writes to\\r\\ns3staging -RIGHT-> snowpipe: writes to\\r\\nsnowpipe -DOWN-> kms: uses\\r\\nsnowpipe -RIGHT-> db: writes to\\r\\n\\r\\n@enduml\\r\\n```\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n## Setup\\r\\n\\r\\nThe steps to set up this pattern are detailed below.  \\r\\n\\r\\n:::info\\r\\n\\r\\nThis example uses the Jsonnet/CloudFormation pattern described in this article: __[Simplifying Large CloudFormation Templates using Jsonnet](simplifying-large-cloudformation-templates-using-jsonnet)__.  This is a useful pattern for breaking up a monolithic CloudFormation template at design time to more manageable resource scoped documents, then pre-processing these in a CI routine (GitLab CI, GitHub Actions, etc) to create a complete template.\\r\\n\\r\\n:::\\r\\n\\r\\n## Setup the Service\\r\\n\\r\\nTo setup the SFTP transfer service use the `AWS::Transfer::Server` resource type as shown below:  \\r\\n\\r\\n<Gist id=\\"c8b4ce8ab478715753aab73d478f4fcd\\" \\r\\n/>\\r\\n\\r\\n:::note\\r\\n\\r\\nUse the `tags` shown to display the custom hostname (used as a vanity url) in the Transfer UI in the AWS console.\\r\\n\\r\\n:::\\r\\n\\r\\n## Create the S3 Bucket\\r\\n\\r\\nCreate a bucket which will be used to store incoming files sent via SFTP.  \\r\\n\\r\\n<Gist id=\\"82eb106bc13f1a888f823cc71a7ff933\\" \\r\\n/>\\r\\n\\r\\n:::note\\r\\n\\r\\nThis example logs to a logging bucket, not shown for brevity.\\r\\n\\r\\n:::\\r\\n\\r\\n## Create a Customer Managed KMS Key\\r\\n\\r\\nCreate a customer managed KMS key which will be used to encrypt data stored in the S3 bucket created in the previous step.  \\r\\n\\r\\n<Gist id=\\"2c563411442c4541584815389de8a3b5\\" \\r\\n/>\\r\\n\\r\\n# Create an IAM role to access the bucket\\r\\n\\r\\nCreate an IAM role which will be assumed by the AWS Transfer Service to read and write to the S3 staging bucket.  \\r\\n\\r\\n<Gist id=\\"57e23a5c99c22f5550e99b086db5f9f1\\" \\r\\n/>\\r\\n\\r\\n:::important\\r\\n\\r\\nYou must assign permissions to use the KMS key created previously, failure to do so will result in errors such as:\\r\\n\\r\\n```\\r\\nremote readdir(): Permission denied\\r\\n```\\r\\n\\r\\n:::\\r\\n\\r\\n## User Directory Mappings\\r\\n\\r\\nAn SFTP users home directory is mapped to a path in your S3 bucket.  It is recommended to use the `LOGICAL` `HomeDirectoryType`.  This will prevent SFTP users from:\\r\\n\\r\\n- seeing or being able to access other users home directories\\r\\n- seeing the bucket name or paths in the bucket above their home directory\\r\\n\\r\\nThere are some trade offs for this which can make deployment a little more challenging but we will cover off the steps from here.\\r\\n\\r\\n### Create a Scoped Down Policy\\r\\n\\r\\nA \\"scoped down\\" policy prevents users from seeing or accessing objects in other users home directories.  This is a text file that will be sourced as a string into the `Policy` parameter of each SFTP user you create.\\r\\n\\r\\n<Gist id=\\"5e876bbf95b1b36355fa8af868572a26\\" \\r\\n/>\\r\\n\\r\\n:::important\\r\\n\\r\\nUsing the `LOGICAL` `HomeDirectoryType` you don\'t have access to variables which represent the bucket, so this needs to be hard coded in the `policy.txt` document.  \\r\\n\\r\\nAlso if you are using a customer managed KMS key to encrypt the data in the bucket (which you should be), you need to add permissions to the key - which again cannot be represented by a variable.  \\r\\n\\r\\nFailure to do so will result in errors when trying to `ls`, `put`, etc into the user\'s home directory such as:  \\r\\n\\r\\n```\\r\\nCouldn\'t read directory: Permission denied\\r\\nCouldn\'t close file: Permission denied\\r\\n```\\r\\n\\r\\nSince these properties are unlikely to change for the lifetime of your service this should not be an issue.  \\r\\n\\r\\n:::\\r\\n\\r\\n### Create a user\\r\\n\\r\\nUsers are identified by a username and an SSH key, providing the public key to the server.  A sample user is shown here:  \\r\\n\\r\\n<Gist id=\\"1b946b07374b78e0aca380317729bfa9\\" \\r\\n/>\\r\\n\\r\\n:::tip\\r\\n\\r\\nAs discussed previously, it is recommended to use `LOGICAL` home directory mappings, which prevents users from seeing information about the bucket or other directories on the SFTP server (including other users directories).\\r\\n\\r\\n:::\\r\\n\\r\\n## Create a Route 53 CNAME record\\r\\n\\r\\nIdeally you want to use a vanity url for users to access your SFTP service, such as `sftp.yourcompany.com`.  This can be accomplished by using a Route 53 CNAME record as shown here:  \\r\\n\\r\\n<Gist id=\\"0098851edc8d60b45534f6b1134be8cd\\" \\r\\n/>\\r\\n\\r\\n## Create some shared Tags\\r\\n\\r\\nYou would have noticed a shared `Tags` definition in many of the `libsonnet` files shown, an example `Tags` source file is shown here:  \\r\\n\\r\\n<Gist id=\\"8323d49f1045d2cd8c874d5a00e82a5e\\" \\r\\n/>\\r\\n\\r\\n## Pull it all together!\\r\\n\\r\\nNow that we have all of the input files, lets pull them all together in a `jsonnet` file, which will be preprocessed in a CI process to create a template we can deploy with AWS CloudFormation.  \\r\\n\\r\\n<Gist id=\\"f56065c075af9cc33853b0624f6ef636\\" \\r\\n/>\\r\\n\\r\\nYour customers would now connect to your service using they private key which corresponds to the public key they supplied to you in one of the previous steps, for example:    \\r\\n\\r\\n```bash\\r\\nsftp -i mysftpkey jeffrey_aven@sftp.yourdomain.com\\r\\n```\\r\\n\\r\\nAdd more users and enjoy!  \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"simple-sso-with-an-external-idp-using-active-directory-and-okta","metadata":{"permalink":"/simple-sso-with-an-external-idp-using-active-directory-and-okta","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-02-04-simple-sso-with-an-external-idp-using-active-directory-and-okta/index.md","source":"@site/blog/2022-02-04-simple-sso-with-an-external-idp-using-active-directory-and-okta/index.md","title":"Simple SSO with an external IdP using Active Directory and Okta","description":"A simple SSO pattern for authenticating and authorizing users from an external AD and to your application without requiring federation.","date":"2022-02-04T00:00:00.000Z","formattedDate":"February 4, 2022","tags":[{"label":"okta","permalink":"/tags/okta"},{"label":"azure","permalink":"/tags/azure"},{"label":"active directory","permalink":"/tags/active-directory"},{"label":"azure active directory","permalink":"/tags/azure-active-directory"},{"label":"sso","permalink":"/tags/sso"},{"label":"single sign on","permalink":"/tags/single-sign-on"},{"label":"identity","permalink":"/tags/identity"}],"readingTime":3.575,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"simple-sso-with-an-external-idp-using-active-directory-and-okta","title":"Simple SSO with an external IdP using Active Directory and Okta","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/okta-ad-sso-featured-image.png","tags":["okta","azure","active directory","azure active directory","sso","single sign on","identity"],"keywords":["okta","azure","active directory","azure active directory","sso","single sign on","identity"],"description":"A simple SSO pattern for authenticating and authorizing users from an external AD and to your application without requiring federation."},"prevItem":{"title":"Implementing a Serverless SFTP Gateway using the AWS Transfer Family","permalink":"/implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family"},"nextItem":{"title":"Converting to local time in AWS Lambda using Node.js","permalink":"/converting-to-local-time-in-aws-lambda-using-nodejs"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\nimport Gist from \'react-gist\';\\r\\n\\r\\nThis article describes a simple SSO pattern for authenticating and authorizing users from an external AD and to your application without requiring federation.  \\r\\n\\r\\n## the Challenge  \\r\\n\\r\\nYou need to authenticate external users to use your application, these users belong to an organization using Azure Active Directory with specific login policies (such as password strength and expiry, multi factor authentication, etc).  Your requirements (if you choose to accept them) are:\\r\\n\\r\\n1.\\tYou are required to provide SSO to these users using their home AD tenant and policies\\r\\n2.\\tThe solution does not include SAML based federation between directories (yours and theirs)\\r\\n3.\\tThe solution does not require any changes on the external AD tenant (no new AAD applications, client secrets, etc)\\r\\n\\r\\n## the Solution\\r\\n\\r\\nUsing an IDAM/IDaaS platform (such as Okta in this case), along with an AAD application (in your AD tenant in your Azure subscription), you can create a local AD app using this magic property to accomplish all of the above requirements (requiring zero changes on the third-party AD).  \\r\\n\\r\\n[![Azure AD App Registration](images/azure-ad-app-registration.png)](images/azure-ad-app-registration.png) \\r\\n\\r\\nThis is what it looks like using the `az` cli:\\r\\n\\r\\n<Gist id=\\"8b70fbe242da02ca844bf2fe53355743\\" \\r\\n/>\\r\\n\\r\\nthe `--available-to-other-tenants` property is Microsoft\'s way of allowing you to implicitly trust other AAD/Office 365 tenants, meaning the authentication request is passed to the target AD tenant from your application.  \\r\\n\\r\\nHere is a context diagram which explains the interactions in the context of a Jamstack application (using a library such as Auth.js).  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"overview\\"\\r\\n  values={[\\r\\n    { label: \'Overview\', value: \'overview\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"overview\\">\\r\\n\\r\\n[![Okta AD SSO Context Diagram](images/okta-ad-sso-context-diagram.png)](images/okta-ad-sso-context-diagram.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```plantuml\\r\\n@startuml\\r\\n\\r\\n!define C4Puml https://raw.githubusercontent.com/RicardoNiepel/C4-PlantUML/master\\r\\n!includeurl C4Puml/C4_Context.puml\\r\\n!includeurl C4Puml/C4_Component.puml\\r\\n!includeurl C4Puml/C4_Container.puml\\r\\n\\r\\n\'left to right direction\\r\\n\\r\\n!define Rel_NoRank(e_from,e_to, e_label=\\" \\") Rel_(e_from,e_to, e_label, \\"-[norank]->\\")\\r\\n\\r\\n!$imgroot = \\"https://github.com/avensolutions/plantuml-cloud-image-library/raw/main/images\\"\\r\\n\\r\\n!unquoted procedure $AzureActiveDirectory($alias, $label, $techn, $descr=\\"\\", $stereo=\\"Azure Active Directory\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/azure/AzureActiveDirectory.png{scale=0.75}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $Okta($alias, $label, $techn, $descr=\\"\\", $stereo=\\"Okta\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/okta/okta.png{scale=1}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\nPerson(user, User\\\\n<i>UserAgent (Browser) )\\r\\nPerson(admin, Application Admin)\\r\\nnote right\\r\\nCreate users in the Okta org with the same email as the users email address in their AD (external AD)\\r\\nend note\\r\\nrectangle \\"Application Environment\\" <<boundary>> as app{\\r\\n    $AzureActiveDirectory(localad, Local AD Tenant, Azure Active Directory)\\r\\n    $Okta(okta, Local Okta Org, Okta)\\r\\n}\\r\\n$AzureActiveDirectory(otherad, Azure AD Tenant\\\\n<i>(External AD), Azure Active Directory)\\r\\n\\r\\nLay_D(user, okta)\\r\\nLay_R(okta, localad)\\r\\nLay_R(localad, otherad)\\r\\nLay_D(okta, admin)\\r\\n\\r\\nRel_U(okta, user, access code)\\r\\nRel_D(user, okta, authorize request)\\r\\nRel_R(okta, localad, routes to)\\r\\nRel_R(localad, otherad, forwards to)\\r\\nRel_U(admin, okta, creates users)\\r\\n\\r\\n@enduml\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n## Setup and Configuration  \\r\\n\\r\\nThe following flowchart explains the steps involved in setting this up.  The highlighted nodes are part of normal application lifecycle operations as users get created and deactivated.  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"flowchart\\"\\r\\n  values={[\\r\\n    { label: \'Flowchart\', value: \'flowchart\', },\\r\\n    { label: \'Mermaid\', value: \'mermaidCode\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"flowchart\\">\\r\\n\\r\\n[![Okta AD SSO Setup Flowchart](images/okta-ad-sso-setup-flowchart.svg)](images/okta-ad-sso-setup-flowchart.svg) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"mermaidCode\\">\\r\\n\\r\\n```mermaid\\r\\nflowchart TD;\\r\\n  subgraph Local Azure AD;\\r\\n    a1(1. Create AD App);\\r\\n  end;\\r\\n  subgraph Okta;\\r\\n    b1(2. Create IdP)--\x3eb2(3. Create Application);\\r\\n    b2--\x3eb3(4. Create IdP\\\\nRouting Rule\\\\nfor Application);\\r\\n    b3--\x3eb4(5. Create Group);\\r\\n    b4--\x3eb5(6. Assign Group\\\\nto Application);\\r\\n    b5--\x3ec1(7. Create User);\\r\\n    c1--\x3ec2(8. Add User to Group);\\r\\n    style c1 fill:#f9f,stroke:#333,stroke-width:4px;\\r\\n    style c2 fill:#f9f,stroke:#333,stroke-width:4px;\\r\\n  end;\\r\\n  subgraph Application;\\r\\n    d1(9. Configure ISSUER\\\\nand CLIENTID);\\t\\t\\r\\n  end;\\r\\n  a1--\x3eOkta;\\r\\n  b3--\x3eApplication;\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n## Authorisation flow\\r\\n\\r\\nThe authorization flow for a public client (SPA) using PKCE (Proof Key for Code Exchange) is shown here:    \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"sequence\\"\\r\\n  values={[\\r\\n    { label: \'Sequence\', value: \'sequence\', },\\r\\n    { label: \'Mermaid\', value: \'mermaidCode\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"sequence\\">\\r\\n\\r\\n[![Okta AD SSO Authorization Flow](images/okta-ad-sso-authorization-flow.svg)](images/okta-ad-sso-authorization-flow.svg)\\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"mermaidCode\\">\\r\\n\\r\\n```mermaid\\r\\nsequenceDiagram;\\r\\n  %%{init: {\'theme\': \'base\', \'themeVariables\': { \'primaryColor\': \'#AACCFF\', \'primaryBorderColor\': \'#999000\', \'actorLineColor\': \'#000000\' }}}%%;\\r\\n  participant user as User;\\r\\n  participant spa as User Agent (SPA);\\r\\n  participant be as Back End APIs;\\r\\n  participant okta as Okta;\\r\\n  participant msft as Microsoft Login;\\r\\n  user->>spa: ;\\r\\n  spa->>okta: local.okta.com/../authorize;\\r\\n  Note over spa,okta: includes client_id (okta), code_challenge (PKCE), redirect_uri (to app), response_type, scope\\r\\n  okta--\x3e>spa: 302 REDIRECT; \\r\\n  spa->>msft: login.microsoftonline.com/../authorize;\\r\\n  Note over spa,msft: includes client_id (msft app id), state, redirect_uri (to okta), response_type, scope\\r\\n  msft->>msft: authenticate;\\r\\n  msft--\x3e>okta: local.okta.com/../authorize/callback;\\r\\n  okta--\x3e>msft:  302 REDIRECT;\\r\\n  msft--\x3e>spa: app/callback?code=xxx;\\r\\n  spa->>okta: exchange code for an access token;\\r\\n  okta->>spa: token;\\r\\n  spa->>be: present token to access resources;\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n## Next up\\r\\n\\r\\n`Code!`  Stay tuned...\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"converting-to-local-time-in-aws-lambda-using-nodejs","metadata":{"permalink":"/converting-to-local-time-in-aws-lambda-using-nodejs","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2022-01-29-converting-to-local-time-in-aws-lambda-using-nodejs/index.md","source":"@site/blog/2022-01-29-converting-to-local-time-in-aws-lambda-using-nodejs/index.md","title":"Converting to local time in AWS Lambda using Node.js","description":"A simple pattern for converting dates in AWS Lambda using a Node.js runtime from GMT/UTC (the default) to a local time observing daylight savings time.","date":"2022-01-29T00:00:00.000Z","formattedDate":"January 29, 2022","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"lambda","permalink":"/tags/lambda"},{"label":"nodejs","permalink":"/tags/nodejs"},{"label":"javascript","permalink":"/tags/javascript"}],"readingTime":4.715,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"converting-to-local-time-in-aws-lambda-using-nodejs","title":"Converting to local time in AWS Lambda using Node.js","authors":["jeffreyaven"],"draft":false,"image":"/img/fullstackchronicles-cover-image.png","tags":["aws","lambda","nodejs","javascript"],"keywords":["aws","lambda","nodejs","javascript"],"description":"A simple pattern for converting dates in AWS Lambda using a Node.js runtime from GMT/UTC (the default) to a local time observing daylight savings time."},"prevItem":{"title":"Simple SSO with an external IdP using Active Directory and Okta","permalink":"/simple-sso-with-an-external-idp-using-active-directory-and-okta"},"nextItem":{"title":"Automating Snowflake Role Based Storage Integration for AWS","permalink":"/automating-snowflake-role-based-storage-integration-for-aws"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\n\\r\\n## Background  \\r\\n\\r\\nAWS Lambda instances will return UTC/GMT time for any date time object created using the `Date.now()` function in JavaScript as shown here:  \\r\\n\\r\\n```javascript\\r\\nlet now = new Date();\\r\\nconst tzOffset = now.getTimezoneOffset();\\r\\nconsole.log(`Default Timezone Offset: ${tzOffset}`);\\r\\n// results in ...\\r\\n// Default Timezone Offset: 0\\r\\n```\\r\\nMoreover, Lambda instances are stateless and have no concept of local time.  This can make dealing with dates more challenging.  \\r\\n\\r\\nThis is compounded for localities which have legislated Daylight Savings Time during part of the year.  \\r\\n\\r\\n## Solution\\r\\n\\r\\nA simple (vanilla JavaScript - no third party libraries or external API calls) to adjust the time to local time adjusted for Daylight Savings Time is provided here:  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"commented\\"\\r\\n  values={[\\r\\n    { label: \'Commented\', value: \'commented\', },\\r\\n    { label: \'Uncommented\', value: \'uncommented\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"commented\\">\\r\\n\\r\\n```javascript\\r\\nfunction getGmtDstTransitionDate(year, month, transitionDay, hour){\\r\\n    const firstDayOfTheMonth = new Date(year, month, 1);\\r\\n    let transitionDate = new Date(firstDayOfTheMonth);\\r\\n    // find the first transition day of the month if the first day of the month is not a transition day\\r\\n    if (firstDayOfTheMonth.getDay() !== transitionDay) {\\r\\n        transitionDate = new Date(firstDayOfTheMonth.setDate(firstDayOfTheMonth.getDate() + (transitionDay - firstDayOfTheMonth.getDay())));\\r\\n    };\\r\\n    // return the transition date and time\\r\\n    return new Date(transitionDate.getTime() + (hour * 60 * 60000));\\r\\n};\\r\\n\\r\\nfunction getLocalDateTime(date) {\\r\\n    // default to GMT+11 for AEDT\\r\\n    let offsetInHours = 11;\\r\\n    // if month is between April and October check further, if not return AEDT offset\\r\\n    // remeber getMonth is zero based!\\r\\n    if (date.getMonth() >= 3 && date.getMonth() <= 9) {\\r\\n        // DST starts at 0200 on the First Sunday in October, which is 1600 (16) on the First Saturday (6) in October (9) GMT\\r\\n        const dstStartDate = getGmtDstTransitionDate(date.getFullYear(), 9, 6, 16);\\r\\n        // DST ends at 0300 on the First Sunday in April, which is 1600 (16) on the First Saturday (6) in April (3) GMT\\r\\n        const dstEndDate = getGmtDstTransitionDate(date.getFullYear(), 3, 6, 16);\\r\\n        if (date >= dstEndDate && date < dstStartDate) {\\r\\n            offsetInHours = 10;\\r\\n        };\\r\\n    };\\r\\n    // return the date and time in local time\\r\\n    return new Date(date.getTime() + (offsetInHours * 60 * 60000));\\r\\n}\\r\\n\\r\\n// get current timestamp\\r\\nlet now = new Date();\\r\\nconsole.log(`UTC Date: ${now}`);\\r\\nnow = getLocalDateTime(now);\\r\\nconsole.log(`Local toLocaleString: ${now.toLocaleString()}`);\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"uncommented\\">\\r\\n\\r\\n```javascript\\r\\nfunction getGmtDstTransitionDate(year, month, transitionDay, hour){\\r\\n    const firstDayOfTheMonth = new Date(year, month, 1);\\r\\n    let transitionDate = new Date(firstDayOfTheMonth);\\r\\n    if (firstDayOfTheMonth.getDay() !== transitionDay) {\\r\\n        transitionDate = new Date(firstDayOfTheMonth.setDate(firstDayOfTheMonth.getDate() + (transitionDay - firstDayOfTheMonth.getDay())));\\r\\n    };\\r\\n    return new Date(transitionDate.getTime() + (hour * 60 * 60000));\\r\\n};\\r\\n\\r\\nfunction getLocalDateTime(date) {\\r\\n    let offsetInHours = 11;\\r\\n    if (date.getMonth() >= 3 && date.getMonth() <= 9) {\\r\\n        const dstStartDate = getGmtDstTransitionDate(date.getFullYear(), 9, 6, 16);\\r\\n        const dstEndDate = getGmtDstTransitionDate(date.getFullYear(), 3, 6, 16);\\r\\n        if (date >= dstEndDate && date < dstStartDate) {\\r\\n            offsetInHours = 10;\\r\\n        };\\r\\n    };\\r\\n    return new Date(date.getTime() + (offsetInHours * 60 * 60000));\\r\\n}\\r\\n\\r\\nlet now = new Date();\\r\\nconsole.log(`UTC Date: ${now}`);\\r\\nnow = getLocalDateTime(now);\\r\\nconsole.log(`Local toLocaleString: ${now.toLocaleString()}`);\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n### Breaking it down\\r\\n\\r\\nThis solution is comprised of two functions for DRY purposes.  \\r\\n\\r\\nThe main function `getLocalDateTime` takes a date object representing the current time in UTC and returns a date object representing the local (DST adjusted) time.  \\r\\n\\r\\nThe `getLocalDateTime` function sets a default DST adjusted offset in hours (11 in the case of AEDT), if the month is between April and October the `getGmtDstTransitionDate` is used to determine the exact boundaries between Standard Time and Daylight Savings Time.  \\r\\n\\r\\nIn the case of AEST/AEDT this is the first Sunday in October at 0200 to enter Daylight Savings Time and the first Sunday in April at 0300 to end Daylight Savings Time (both dates and times are adjusted to their equivalent GMT times) and return to Standard Time (10 hours in the cases of AEST).  \\r\\n\\r\\nThe `offsetInHours` variable and the arguments for `getGmtDstTransitionDate` can be easily modified for other timezones.  \\r\\n\\r\\n### Tests\\r\\n\\r\\nSome simple tests to run to check if the code is working correctly, to help with this I have set up the following unit test function:  \\r\\n\\r\\n```javascript\\r\\nfunction unitTest(inputDate, expOutputDate, testCase) {\\r\\n\\tif (getLocalDateTime(inputDate).toUTCString() === expOutputDate.toUTCString()) {\\r\\n\\t\\tconsole.log(`TEST PASSED ${testCase}`)\\r\\n\\t} else {\\r\\n\\t\\tconsole.log(`TEST FAILED ${testCase} : input date in GMT ${inputDate} should equal ${expOutputDate}`)\\r\\n\\t};\\r\\n};\\r\\n```\\r\\n\\r\\nfirst create dates representing the beginning of Daylight Savings Time (immediately before the beginning, at the beginning and immediately after the beginning):  \\r\\n\\r\\n```javascript\\r\\nunitTest(new Date(2022, 9, 1, 15, 59, 59, 999), new Date(2022, 9, 2, 1, 59, 59, 999), \\"one ms before dst start\\");\\r\\n// returns...\\r\\n// ...\\tINFO\\tTEST PASSED one ms before dst start\\r\\nunitTest(new Date(2022, 9, 1, 16, 0, 0, 0), new Date(2022, 9, 2, 3, 0, 0, 0), \\"dst start\\");\\r\\n// returns...\\r\\n// ...\\tINFO\\tTEST PASSED dst start    \\r\\nunitTest(new Date(2022, 9, 1, 16, 0, 0, 1), new Date(2022, 9, 2, 3, 0, 0, 1), \\"one ms after dst start\\");\\r\\n// returns...\\r\\n// ...\\tINFO\\tTEST PASSED one ms after dst start\\r\\n```\\r\\n\\r\\nnext create dates similar tests representing the end of Daylight Savings Time (or beginning of Standard Time):  \\r\\n\\r\\n```javascript\\r\\nunitTest(new Date(2022, 3, 2, 15, 59, 59, 999), new Date(2022, 3, 3, 2, 59, 59, 999), \\"one ms before dst end\\");\\r\\n// returns...\\r\\n// ...\\tINFO\\tTEST PASSED one ms before dst end    \\r\\nunitTest(new Date(2022, 3, 2, 16, 0, 0, 0), new Date(2022, 3, 3, 2, 0, 0, 0), \\"dst end\\");\\r\\n// returns...\\r\\n// ...\\tINFO\\tTEST PASSED dst end    \\r\\nunitTest(new Date(2022, 3, 2, 16, 0, 0, 1), new Date(2022, 3, 3, 2, 0, 0, 1), \\"one ms after dst end\\");\\r\\n// returns...\\r\\n// ...\\tINFO\\tTEST PASSED one ms after dst end\\r\\n```\\r\\n\\r\\nEnjoy  \\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"automating-snowflake-role-based-storage-integration-for-aws","metadata":{"permalink":"/automating-snowflake-role-based-storage-integration-for-aws","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-12-18-automating-snowflake-role-based-storage-integration-for-aws/index.md","source":"@site/blog/2021-12-18-automating-snowflake-role-based-storage-integration-for-aws/index.md","title":"Automating Snowflake Role Based Storage Integration for AWS","description":"Automate the creation of a Storage Integration in Snowflake which allows a Snowflake External Stage to access objects in your AWS S3 bucket.","date":"2021-12-18T00:00:00.000Z","formattedDate":"December 18, 2021","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"snowpipe","permalink":"/tags/snowpipe"},{"label":"powershell","permalink":"/tags/powershell"},{"label":"snowsql","permalink":"/tags/snowsql"},{"label":"infrastructureascode","permalink":"/tags/infrastructureascode"},{"label":"iac","permalink":"/tags/iac"},{"label":"cloudautomation","permalink":"/tags/cloudautomation"}],"readingTime":4.18,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"automating-snowflake-role-based-storage-integration-for-aws","title":"Automating Snowflake Role Based Storage Integration for AWS","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/snowflake-storage-integration-aws-image.png","tags":["aws","snowflake","snowpipe","powershell","snowsql","infrastructureascode","iac","cloudautomation"],"keywords":["aws","snowflake","snowpipe","powershell","snowsql","infrastructureascode","iac","cloudautomation"],"description":"Automate the creation of a Storage Integration in Snowflake which allows a Snowflake External Stage to access objects in your AWS S3 bucket."},"prevItem":{"title":"Converting to local time in AWS Lambda using Node.js","permalink":"/converting-to-local-time-in-aws-lambda-using-nodejs"},"nextItem":{"title":"Simplifying Large CloudFormation Templates using Jsonnet","permalink":"/simplifying-large-cloudformation-templates-using-jsonnet"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\nimport Gist from \'react-gist\';\\r\\n\\r\\nI have used the instructions [here](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-auto-s3.html) to configure Snowpipe for several projects.  \\r\\n\\r\\nAlthough it is accurate, it is entirely click-ops oriented.  I like to automate (and script) everything, so I have created a fully automated implementation using PowerShell, the `aws` and `snowsql` CLIs.  \\r\\n\\r\\nThe challenge is that you need to go back and forth between AWS and Snowflake, exchanging information from each platform with the other.  \\r\\n\\r\\n## Overview  \\r\\n\\r\\nA Role Based Storage Integration in Snowflake allows a user (an AWS user arn) in your Snowflake account to use a role in your AWS account, which in turns enables access to S3 and KMS resources used by Snowflake for an external stage.  \\r\\n\\r\\nThe following diagram explains this (along with the PlantUML code used to create the diagram..):  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"overview\\"\\r\\n  values={[\\r\\n    { label: \'Overview\', value: \'overview\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"overview\\">\\r\\n\\r\\n[![Snowflake S3 Storage Integration](images/snowflake-aws-storage-integration.png)](images/snowflake-aws-storage-integration.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```plantuml\\r\\n@startuml\\r\\n\\r\\nskinparam rectangle<<boundary>> {\\r\\n    Shadowing false\\r\\n    StereotypeFontSize 0\\r\\n    FontColor #444444\\r\\n    BorderColor #444444\\r\\n    BorderStyle dashed\\r\\n}\\r\\n\\r\\nskinparam defaultTextAlignment center\\r\\n\\r\\n!$imgroot = \\"https://github.com/avensolutions/plantuml-cloud-image-library/raw/main/images\\"\\r\\n\\r\\n!unquoted procedure $AwsIam($alias, $label, $techn, $descr=\\"\\", $stereo=\\"AWS IAM\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/aws/SecurityIdentityCompliance/Iam.png>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $AwsS3($alias, $label, $techn, $descr=\\"\\", $stereo=\\"AWS S3\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/aws/Storage/S3.png>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\n!unquoted procedure $Snowflake($alias, $label, $techn, $descr=\\"\\", $stereo=\\"Snowflake\\")\\r\\n    rectangle \\"==$label\\\\n\\\\n<img:$imgroot/snowflake/snowflakeDB.png{scale=0.70}>\\\\n//<size:12>[$techn]</size>//\\" <<$stereo>> as $alias #white\\r\\n!endprocedure\\r\\n\\r\\nrectangle \\"Snowflake\\" <<boundary>> {\\r\\n    $AwsIam(user, Snowflake IAM User, AWS IAM User)\\r\\n    $Snowflake(int, Storage Integration, Storage Integration)\\r\\n    $Snowflake(stage, External Stage, Stage)\\r\\n}\\r\\n\\r\\nrectangle \\"AWS\\" <<boundary>> {\\r\\n    $AwsS3(bucket, Stage Bucket, AWS S3 Bucket)\\r\\n    $AwsIam(role, Snowflake Access Role, IAM Role)\\r\\n    $AwsIam(policy, Snowflake Access Policy, IAM Policy)\\r\\n}\\r\\n\\r\\nstage -UP-> int : uses\\r\\nint -RIGHT-> user : uses\\r\\nuser -RIGHT-> role : uses\\r\\npolicy -UP-> role : attached to\\r\\nrole -RIGHT-> bucket : allows access to\\r\\n\\r\\n@enduml\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\n## Setup  \\r\\n\\r\\nSome prerequisites (removed for brevity):  \\r\\n\\r\\n1.\\tset the following variables in your script:  \\r\\n- `$accountid` \u2013 your AWS account ID\\r\\n- `$bucketname` \u2013 the bucket you are letting Snowflake use as an External Stage\\r\\n- `$bucketarn` \u2013 used in policy statements (you could easily derive this from the bucket name)\\r\\n- `$kmskeyarn` \u2013 assuming you are used customer managed encryption keys, your Snowflake storage integration will need to use these to decrypt data in the stage\\r\\n- `$prefix` \u2013 if you want to set up granular access (on a key/path basis)\\r\\n2.\\tConfigure Snowflake access credentials using environment variables or using the `~/.snowsql/config` file (you should definitely use the `SNOWSQL_PWD` env var for your password however)\\r\\n3.\\tConfigure access to AWS using `aws configure`\\r\\n\\r\\n:::note\\r\\n\\r\\nThe actions performed in both AWS and Snowflake required privileged access on both platforms.\\r\\n\\r\\n:::\\r\\n\\r\\n## The Code  \\r\\n\\r\\nI have broken this into steps, the complete code is included at the end of the article.  \\r\\n\\r\\n### Create Policy Documents  \\r\\n\\r\\nYou will need to create the policy documents to allow the role you will create to access objects in the target S3 bucket, you will also need an initial \u201cAssume Role\u201d policy document which will be used to create the role and then updated with information you will get from Snowflake later.  \\r\\n\\r\\n<Gist id=\\"73d507126c114e6ee7398226cf004f55\\" \\r\\n/>\\r\\n\\r\\n### Create Snowflake Access Policy  \\r\\n\\r\\nUse the `snowflake_policy_doc.json` policy document created in the previous step to create a managed policy, you will need the `arn` returned in a subsequent statement.  \\r\\n\\r\\n<Gist id=\\"65be4f7c104f92fa3dbf9342813b3fd2\\" \\r\\n/>\\r\\n\\r\\n### Create Snowflake IAM Role  \\r\\n\\r\\nUse the initial `assume_role_policy_doc.json` created to create a new Snowflake access role, you will need the `arn` for this resource when you configure the Storage Integration in Snowflake.  \\r\\n\\r\\n<Gist id=\\"e1bdd5316fe7cb106de1edcff77d8e2b\\" \\r\\n/>\\r\\n\\r\\n### Attach S3 Access Policy to the Role  \\r\\n\\r\\nNow you will attach the `snowflake-access-policy` to the `snowflake-access-role` using the `$policyarn` captured from the policy creation statement.  \\r\\n\\r\\n<Gist id=\\"d2d54b43e379a26bd264a4c97939250c\\" \\r\\n/>\\r\\n\\r\\n### Create Storage Integration in Snowflake  \\r\\n\\r\\nUse the `snowsql` CLI to create a Storage Integration in Snowflake supplying the `$rolearn` captured from the role creation statement.  \\r\\n\\r\\n<Gist id=\\"8e4617227bcd68be74c2a5d694c85f91\\" \\r\\n/>\\r\\n\\r\\n### Get `STORAGE_AWS_IAM_USER_ARN` and `STORAGE_AWS_EXTERNAL_ID`  \\r\\n\\r\\nYou will need the `STORAGE_AWS_IAM_USER_ARN` and `STORAGE_AWS_EXTERNAL_ID` values for the storage integration you created in the previous statement, these will be used to updated the assume role policy in your `snowflake-access-role`.  \\r\\n\\r\\n<Gist id=\\"14dbf570030cad1a46d88d2e87006c8e\\" \\r\\n/>\\r\\n\\r\\n### Update Snowflake Access Policy  \\r\\n\\r\\nUsing the `STORAGE_AWS_IAM_USER_ARN` and `STORAGE_AWS_EXTERNAL_ID` values retrieved in the previous statements, you will update the `assume-role-policy` for the `snowflake-access-role`.  \\r\\n\\r\\n<Gist id=\\"944c39205e142de9a76266f7f3cd260b\\" \\r\\n/>\\r\\n\\r\\n### Test the Storage Integration  \\r\\n\\r\\nTo test the connectivity between your Snowflake account and your AWS external stage using the Storage Integartion just created, create a stage as shown here:  \\r\\n\\r\\n<Gist id=\\"99c24e8c80c6556fe381cf64c841f739\\" \\r\\n/>\\r\\n\\r\\nNow list objects in the stage (assuming there are any).  \\r\\n\\r\\n```js\\r\\nlist @my_stage;\\r\\n```\\r\\nThis should just work!  You can use your storage integration to create different stages for different paths in your External Stage bucket and use both of these objects to create Snowpipes for automated ingestion.  Enjoy!  \\r\\n\\r\\n### Complete Code  \\r\\n\\r\\nThe complete code for this example is shown here:  \\r\\n\\r\\n<Gist id=\\"5f4cba25f4eac380d63f5829c56d0306\\" \\r\\n/>\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"simplifying-large-cloudformation-templates-using-jsonnet","metadata":{"permalink":"/simplifying-large-cloudformation-templates-using-jsonnet","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-11-21-simplifying-large-cloudformation-templates-using-jsonnet/index.md","source":"@site/blog/2021-11-21-simplifying-large-cloudformation-templates-using-jsonnet/index.md","title":"Simplifying Large CloudFormation Templates using Jsonnet","description":"A simple pattern to break up large CloudFormation templates into smaller, more manageable modules using Jsonnet and GitLab CI.","date":"2021-11-21T00:00:00.000Z","formattedDate":"November 21, 2021","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"awscloudformation","permalink":"/tags/awscloudformation"},{"label":"jsonnet","permalink":"/tags/jsonnet"},{"label":"gitlab","permalink":"/tags/gitlab"},{"label":"gitlabci","permalink":"/tags/gitlabci"},{"label":"infrastructureascode","permalink":"/tags/infrastructureascode"},{"label":"iac","permalink":"/tags/iac"},{"label":"cloudautomation","permalink":"/tags/cloudautomation"}],"readingTime":2.565,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"simplifying-large-cloudformation-templates-using-jsonnet","title":"Simplifying Large CloudFormation Templates using Jsonnet","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/cloudformation-jsonnet-featured-image.png","tags":["aws","awscloudformation","jsonnet","gitlab","gitlabci","infrastructureascode","iac","cloudautomation"],"keywords":["aws","awscloudformation","jsonnet","gitlab","gitlabci","infrastructureascode","iac","cloudautomation"],"description":"A simple pattern to break up large CloudFormation templates into smaller, more manageable modules using Jsonnet and GitLab CI."},"prevItem":{"title":"Automating Snowflake Role Based Storage Integration for AWS","permalink":"/automating-snowflake-role-based-storage-integration-for-aws"},"nextItem":{"title":"Simplified AWS Deployments with CloudFormation and GitLab CI","permalink":"/aws-deployments-with-cloudformation-and-gitlab-ci"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nCloudFormation templates in large environments can grow beyond a manageable point.  This article provides one approach to breaking up CloudFormation templates into modules which can be imported and used to create a larger template to deploy a complex AWS stack \u2013 using Jsonnet.  \\r\\n\\r\\nJsonnet is a json pre-processing and templating library which includes features including user defined and built-in functions, objects, and inheritance amongst others.  If you are not familiar with Jsonnet, here are some good resources to start with:  \\r\\n\\r\\n- [Jsonnet](https://jsonnet.org/)\\r\\n- [Blog Article: Using Jsonnet to Configure Multiple Environments](https://cloudywithachanceofbigdata.com/using-jsonnet-to-configure-multiple-environments)\\r\\n- [Blog Article: Using the Jsonnet Map Function](https://docs.infraql.io/blog/using-the-jsonnet-map-function)\\r\\n\\r\\n## Advantages\\r\\n\\r\\nUsing Jsonnet you can use imports to break up large stacks into smaller files scoped for each resource.  This approach makes CloudFormation template easier to read and write and allows you to apply the DRY (Do Not Repeat Yourself) coding principle (not possible with native CloudFormation templates.  \\r\\n\\r\\nAdditionally, although as the template fragments are in Jsonnet format, you can add annotations or comments to your code similar to YAML (not possible with a JSON template alone), although the rendered template is in legal CloudFormation Json format.  \\r\\n\\r\\n## Process Overview\\r\\n\\r\\nThe process is summarised here: \\r\\n\\r\\n[![CloudFormation and Jsonnet](images/cloudformation-jsonnet.png)](images/cloudformation-jsonnet.png) \\r\\n\\r\\n## Code\\r\\n\\r\\nThis example will deploy a stack with a VPC and an S3 bucket with logging.  The project directory structure would look like this:  \\r\\n\\r\\n```bash\\r\\ntemplates/\\r\\n\u251c\u2500 includes/\\r\\n\u2502  \u251c\u2500 vpc.libsonnet\\r\\n\u2502  \u251c\u2500 s3landingbucket.libsonnet\\r\\n\u2502  \u251c\u2500 s3loggingbucket.libsonnet\\r\\n\u2502  \u251c\u2500 tags.libsonnet\\r\\n\u251c\u2500 template.jsonnet\\r\\n```\\r\\n\\r\\nLets look at all of the constituent files:  \\r\\n\\r\\n### `template.jsonnet`\\r\\nThis is the root document which will be processed by Jsonnet to render a legal CloudFormation JSON template.  It will import the other files in the includes directory.  \\r\\n\\r\\n<Gist id=\\"8f2cc0c464de762f73b3f81c75a13832\\" \\r\\n/>\\r\\n\\r\\n### `includes/tags.libsonnet`\\r\\n\\r\\nThis code module is used to generate re-usable tags for other resources (DRY).  \\r\\n\\r\\n<Gist id=\\"82e21743e845355ba0ef7240f1f7327a\\" \\r\\n/>\\r\\n\\r\\n### `includes/vpc.libsonnet`\\r\\nThis code module defines a VPC resource to be created with CloudFormation.  \\r\\n\\r\\n<Gist id=\\"e79189bbc1cfb8b72bd860c6381f6130\\" \\r\\n/>\\r\\n\\r\\n### `includes/s3loggingbucket.libsonnet`\\r\\nThis code module defines an S3 bucket resource to be created in the stack which will be used for logging for other buckets.  \\r\\n\\r\\n<Gist id=\\"187c97deca224617b064c4028ebbbee2\\" \\r\\n/>\\r\\n\\r\\n### `includes/s3landingbucket.libsonnet`\\r\\nThis code module defines an S3 landing bucket resource to be created in the stack.  \\r\\n\\r\\n<Gist id=\\"c0dc5d868809f98ef672aca738bb1e5e\\" \\r\\n/>\\r\\n\\r\\n## Testing\\r\\nTo test the pre-processing, you will need a Jsonnet binary/executable for your environment.  You can find Docker images which include this for you, or you could build it yourself.  \\r\\n\\r\\nOnce you have a compiled binary, you can run the following to generate a rendered CloudFormation template.  \\r\\n\\r\\n```bash\\r\\njsonnet template.jsonnet -o template.json\\r\\n```\\r\\nYou can validate this template using the AWS CLI as shown here:  \\r\\n\\r\\n```bash\\r\\naws cloudformation validate-template --template-body file://template.json\\r\\n```\\r\\n## Deployment\\r\\nIn a previous article, [Simplified AWS Deployments with CloudFormation and GitLab CI](https://cloudywithachanceofbigdata.com/aws-deployments-with-cloudformation-and-gitlab-ci), I demonstrated an end-to-end deployment pipeline using GitLab CI.  Jsonnet pre-processing can be added to this pipeline as an initial \u2018preprocess\u2019 stage and job.  A snippet from the `.gitlab-ci.yml` file is included here:  \\r\\n\\r\\n<Gist id=\\"14c4c2fdccb27884c69c31f7b3a17a99\\" \\r\\n/>\\r\\n\\r\\nEnjoy!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"aws-deployments-with-cloudformation-and-gitlab-ci","metadata":{"permalink":"/aws-deployments-with-cloudformation-and-gitlab-ci","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-11-11-aws-deployments-with-cloudformation-and-gitlab-ci/index.md","source":"@site/blog/2021-11-11-aws-deployments-with-cloudformation-and-gitlab-ci/index.md","title":"Simplified AWS Deployments with CloudFormation and GitLab CI","description":"A simple pattern for deploying stacks in AWS using CloudFormation templates using GitLab CI which allows you to review changes before deploying.","date":"2021-11-11T00:00:00.000Z","formattedDate":"November 11, 2021","tags":[{"label":"gitlab","permalink":"/tags/gitlab"},{"label":"gitlabci","permalink":"/tags/gitlabci"},{"label":"aws","permalink":"/tags/aws"},{"label":"awscloudformation","permalink":"/tags/awscloudformation"},{"label":"infrastructureascode","permalink":"/tags/infrastructureascode"},{"label":"iac","permalink":"/tags/iac"},{"label":"cloudautomation","permalink":"/tags/cloudautomation"}],"readingTime":2.22,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"aws-deployments-with-cloudformation-and-gitlab-ci","title":"Simplified AWS Deployments with CloudFormation and GitLab CI","authors":["jeffreyaven"],"draft":false,"image":"/img/blog/gitlabci-cloudformation-featured-image.png","tags":["gitlab","gitlabci","aws","awscloudformation","infrastructureascode","iac","cloudautomation"],"keywords":["gitlab","gitlabci","aws","awscloudformation","infrastructureascode","iac","cloudautomation"],"description":"A simple pattern for deploying stacks in AWS using CloudFormation templates using GitLab CI which allows you to review changes before deploying."},"prevItem":{"title":"Simplifying Large CloudFormation Templates using Jsonnet","permalink":"/simplifying-large-cloudformation-templates-using-jsonnet"},"nextItem":{"title":"Using Jsonnet to Configure Multiple Environments","permalink":"/using-jsonnet-to-configure-multiple-environments"}},"content":"import Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\nimport Gist from \'react-gist\';\\r\\n\\r\\nManaging cloud deployments and IaC pipelines can be challenging.  I\u2019ve put together a simple pattern for deploying stacks in AWS using CloudFormation templates using GitLab CI.  \\r\\n\\r\\nThis deployment framework enables you to target different environments based upon refs (branches or tags) for instance deploy to a dev environment for a push or merge into develop and deploy to prod on a push or merge into main, otherwise just lint/validate (e.g., for a push to a non-protected feature branch).  Templates are uploaded to a designated S3 bucket and staged for use in the pipeline and can be retained as an additional audit trail (in addition to the GitLab project history).  \\r\\n\\r\\nFurthermore, you can review changes (by inspecting change set contents) before deploying, saving you from fat finger deployments \ud83d\ude0a.  \\r\\n \\r\\n## How it works\\r\\nThe logic is described here:  \\r\\n\\r\\n<Tabs\\r\\n  defaultValue=\\"flow\\"\\r\\n  values={[\\r\\n    { label: \'Flow\', value: \'flow\', },\\r\\n    { label: \'PlantUML\', value: \'plantuml\', },\\r\\n  ]\\r\\n}>\\r\\n<TabItem value=\\"flow\\">\\r\\n\\r\\n[![GitLab CI](images/gitlabci-cloudformation-flow.png)](images/gitlabci-cloudformation-flow.png) \\r\\n\\r\\n</TabItem>\\r\\n<TabItem value=\\"plantuml\\">\\r\\n\\r\\n```plantuml\\r\\n@startuml\\r\\n\\r\\npartition prepare {\\r\\n  (*) --\x3e === S1 ===\\r\\n  === S1 === --\x3e \\"Validate Template\\"\\r\\n  --\x3e === S2 ===\\r\\n  === S1 === --\x3e \\"Check Stack State\\"\\r\\n  --\x3e === S2 ===\\r\\n}\\r\\n\\r\\npartition publish {\\r\\n  --\x3e \\"Publish Template to S3\\"\\r\\n}\\r\\n\\r\\npartition plan {\\r\\n  --\x3e \\"Stack Exists?\\"\\r\\n  --\x3e === S3 ===\\r\\n  === S3 === --\x3e [Yes] \\"Create Change Set\\"\\r\\n  === S3 === --\x3e [No] === S4 ===\\r\\n  \\"Create Change Set\\" --\x3e === S4 ===\\r\\n}\\r\\n\\r\\npartition deploy {\\r\\n  --\x3e \\"MANUAL: Review Changes\\"\\r\\n  --\x3e \\"Deploy Change Set\\"\\r\\n}\\r\\n\\r\\n--\x3e(*)\\r\\n\\r\\n@enduml\\r\\n```\\r\\n\\r\\n</TabItem>\\r\\n</Tabs>\\r\\n\\r\\nThe pipleline looks like this in GitLab:  \\r\\n\\r\\n[![GitLab CI](images/gitlab-ci.png)](images/gitlab-ci.png)  \\r\\n\\r\\n## Prerequisites\\r\\nYou will need to set up GitLab CI variables for `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and optionally `AWS_DEFAULT_REGION`.  You can do this via __Settings -> CI/CD -> Variables__ in your GitLab project.   As `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are secrets, they should be configured as `protected` (as they are only required for protected branches) and `masked` so they are not printed in job logs.\\r\\n\\r\\n## `.gitlab-ci.yml` code\\r\\nThe GitLab CI code is shown here:  \\r\\n\\r\\n<Gist id=\\"d561e9f002048b4e4be4043cf185d1bd\\" \\r\\n/>\\r\\n\\r\\n## Reviewing change sets (plans) and applying\\r\\nOnce a pipeline is triggered for an existing stack it will run hands off until a change set (plan) is created.  You can inspect the plan by clicking on the Plan GitLab CI job where you would see output like this:  \\r\\n\\r\\n[![Change Set](images/gitlab-ci-cloudformation-plan.png)](images/gitlab-ci-cloudformation-plan.png)  \\r\\n\\r\\nIf you are OK with the changes proposed, you can simply hit the play button on the last stage of the pipeline (Deploy).  Voil\xe0, stack deployed, enjoy!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"using-jsonnet-to-configure-multiple-environments","metadata":{"permalink":"/using-jsonnet-to-configure-multiple-environments","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-06-24-using-jsonnet-to-configure-multiple-environments/index.md","source":"@site/blog/2021-06-24-using-jsonnet-to-configure-multiple-environments/index.md","title":"Using Jsonnet to Configure Multiple Environments","description":"Everytime I start a new project I try and optimise how the application can work across multiple envronments. For those who don\'t have the luxury of developing everything in docker containers or isolated spaces, you will know my pain. How do I write code that can run on my local dev environment, migrate to the shared test and ci environment and ultimately still work in production.","date":"2021-06-24T00:00:00.000Z","formattedDate":"June 24, 2021","tags":[{"label":"ci-cd","permalink":"/tags/ci-cd"},{"label":"configuration","permalink":"/tags/configuration"},{"label":"envconfig","permalink":"/tags/envconfig"},{"label":"environments","permalink":"/tags/environments"},{"label":"hocon","permalink":"/tags/hocon"},{"label":"json","permalink":"/tags/json"},{"label":"jsonnet","permalink":"/tags/jsonnet"}],"readingTime":3.07,"hasTruncateMarker":false,"authors":[{"name":"Mark Stella","title":"Senior Cloud Engineer","url":"https://github.com/mpstella","imageURL":"http://1.gravatar.com/avatar/9a7465656212285f24f64326cd38d6c9?s=80","key":"markstella"}],"frontMatter":{"slug":"using-jsonnet-to-configure-multiple-environments","title":"Using Jsonnet to Configure Multiple Environments","authors":["markstella"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["ci-cd","configuration","envconfig","environments","hocon","json","jsonnet"],"keywords":["ci-cd","configuration","envconfig","environments","hocon","json","jsonnet"]},"prevItem":{"title":"Simplified AWS Deployments with CloudFormation and GitLab CI","permalink":"/aws-deployments-with-cloudformation-and-gitlab-ci"},"nextItem":{"title":"Use BigQuery to trigger Cloud Run","permalink":"/use-bigquery-to-trigger-cloud-run"}},"content":"Everytime I start a new project I try and optimise how the application can work across multiple envronments. For those who don\'t have the luxury of developing everything in docker containers or isolated spaces, you will know my pain. How do I write code that can run on my local `dev` environment, migrate to the shared `test` and `ci` environment and ultimately still work in `production`.\\r\\n\\r\\nIn the past I tried exotic options like dynamically generating `YAML` or `JSON` using Jinja. I then graduated to `HOCON` which made my life so much easier. This was until I stumbled across [Jsonnet](https://jsonnet.org/). For those who have not seen this in action, think JSON meets Jinja meets HOCON (a Frankenstein creation that I have actually built in the past)\\r\\n\\r\\nTo get a feel for how it looks, below is a contrived example where I require 3 environments (dev, test and production) that have different paths, databases and vault configuration.\\r\\n\\r\\nEssentially, when this config is run through the Jsonnet templating engine, it will expect a variable \'`ENV`\' to ultimately refine the `environment` entry to the one we specifically want to use.\\r\\n\\r\\nA helpful thing I like to do with my programs is give users a bit of information as to what environments can be used. For me, running a cli that requires args should be as informative as possible - so listing out all the environments is mandatory. I achieve this with a little trickery and a lot of help from the [click](https://click.palletsprojects.com/) package!\\r\\n\\r\\n```jsonnet\\r\\nlocal exe = \\"application.exe\\";\\r\\n\\r\\nlocal Environment(prefix) = {\\r\\n  root: \\"/usr/\\" + prefix + \\"/app\\",\\r\\n  path: self.root + \\"/bin/\\" + exe,\\r\\n  database: std.asciiUpper(prefix) + \\"_DB\\",\\r\\n  tmp_dir: \\"/tmp/\\" + prefix\\r\\n};\\r\\n\\r\\nlocal Vault = {\\r\\n  local uri = \\"http://127.0.0.1:8200/v1/secret/app\\",\\r\\n  _: {},\\r\\n  dev: {\\r\\n      secrets_uri: uri,\\r\\n      approle: \\"local\\"\\r\\n  },\\r\\n  tst: {\\r\\n      secrets_uri: uri,\\r\\n      approle: \\"local\\"\\r\\n  },\\r\\n  prd: {\\r\\n      secrets_uri: \\"https://vsrvr:8200/v1/secret/app\\",\\r\\n      approle: \\"sa_user\\"\\r\\n  }\\r\\n};\\r\\n\\r\\n{\\r\\n\\r\\n  environments: {\\r\\n    _: {},\\r\\n    dev: Environment(\\"dev\\") + Vault[std.extVar(\\"ENV\\")],\\r\\n    tst: Environment(\\"tst\\") + Vault[std.extVar(\\"ENV\\")],\\r\\n    prd: Environment(\\"prd\\") + Vault[std.extVar(\\"ENV\\")]\\r\\n  },\\r\\n\\r\\n  environment: $[\\"environments\\"][std.extVar(\\"ENV\\")],\\r\\n}\\r\\n```\\r\\n\\r\\nThe trick I perform is to have a placeholder entry \'`_`\' that I use to initially render the template. I then use the generated JSON file and get all the environment keys so I can feed that directly into click.\\r\\n\\r\\n```python\\r\\nfrom typing import Any, Dict\\r\\nimport click\\r\\nimport json\\r\\nimport _jsonnet\\r\\nfrom pprint import pprint\\r\\n\\r\\nENV_JSONNET = \'environment.jsonnet\'\\r\\nENV_PFX_PLACEHOLDER = \'_\'\\r\\n\\r\\ndef parse_environment(prefix: str) -> Dict[str, Any]:\\r\\n    _json_str = _jsonnet.evaluate_file(ENV_JSONNET, ext_vars={\'ENV\': prefix})\\r\\n    return json.loads(_json_str)\\r\\n\\r\\n_config = parse_environment(prefix=ENV_PFX_PLACEHOLDER)\\r\\n\\r\\n_env_prefixes = [k for k in _config[\'environments\'].keys() if k != ENV_PFX_PLACEHOLDER]\\r\\n\\r\\n\\r\\n@click.command(name=\\"EnvMgr\\")\\r\\n@click.option(\\r\\n    \\"-e\\",\\r\\n    \\"--environment\\",\\r\\n    required=True,\\r\\n    type=click.Choice(_env_prefixes, case_sensitive=False),\\r\\n    help=\\"Which environment this is executing on\\",\\r\\n)\\r\\ndef cli(environment: str) -> None:\\r\\n    config = parse_environment(environment)\\r\\n    pprint(config[\'environment\'])\\r\\n\\r\\n\\r\\nif __name__ == \\"__main__\\":\\r\\n    cli()\\r\\n```\\r\\n\\r\\nThis now allows me to execute the application with both list checking (has the user selected an allowed environment?) and the autogenerated help that click provides.\\r\\n\\r\\nBelow shows running the cli with no arguments:\\r\\n\\r\\n```shell\\r\\n$> python cli.py\\r\\n\\r\\nUsage: cli.py [OPTIONS]\\r\\nTry \'cli.py --help\' for help.\\r\\n\\r\\nError: Missing option \'-e\' / \'--environment\'. Choose from:\\r\\n        dev,\\r\\n        prd,\\r\\n        tst\\r\\n```\\r\\n\\r\\nExecuting the application with a valid environment:\\r\\n\\r\\n```shell\\r\\n$> python cli.py -e dev\\r\\n\\r\\n{\'approle\': \'local\',\\r\\n \'database\': \'DEV_DB\',\\r\\n \'path\': \'/usr/dev/app/bin/application.exe\',\\r\\n \'root\': \'/usr/dev/app\',\\r\\n \'secrets_uri\': \'http://127.0.0.1:8200/v1/secret/app\',\\r\\n \'tmp_dir\': \'/tmp/dev\'}\\r\\n```\\r\\n\\r\\nExecuting the application with an invalid environment:\\r\\n\\r\\n```shell\\r\\n$> python cli.py -e prd3\\r\\n\\r\\nUsage: cli.py [OPTIONS]\\r\\nTry \'cli.py --help\' for help.\\r\\n\\r\\nError: Invalid value for \'-e\' / \'--environment\': \'prd3\' is not one of \'dev\', \'prd\', \'tst\'.\\r\\n```\\r\\n\\r\\nThis is only the tip of what Jsonnet can provide, I am continually learning more about the templating engine and the tool.\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"use-bigquery-to-trigger-cloud-run","metadata":{"permalink":"/use-bigquery-to-trigger-cloud-run","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-06-19-use-bigquery-to-trigger-cloud-run/index.md","source":"@site/blog/2021-06-19-use-bigquery-to-trigger-cloud-run/index.md","title":"Use BigQuery to trigger Cloud Run","description":"So you\'re using BigQuery (BQ). It\'s all set up and humming perfectly. Maybe now, you want to run an ELT job whenever a new table partition is created, or maybe you want to retrain your ML model whenever new rows are inserted into the BQ table.","date":"2021-06-19T00:00:00.000Z","formattedDate":"June 19, 2021","tags":[{"label":"big-query","permalink":"/tags/big-query"},{"label":"bigquery","permalink":"/tags/bigquery"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"}],"readingTime":3.795,"hasTruncateMarker":false,"authors":[{"name":"Tom Klimovski","title":"Senior Cloud Engineer","url":"https://github.com/tomklimovskigamma","imageURL":"http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80","key":"tomklimovski"}],"frontMatter":{"slug":"use-bigquery-to-trigger-cloud-run","title":"Use BigQuery to trigger Cloud Run","authors":["tomklimovski"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["big-query","bigquery","gcp","google-cloud-platform","googlecloudplatform"],"keywords":["big-query","bigquery","gcp","google-cloud-platform","googlecloudplatform"]},"prevItem":{"title":"Using Jsonnet to Configure Multiple Environments","permalink":"/using-jsonnet-to-configure-multiple-environments"},"nextItem":{"title":"Azure Static Web App Review","permalink":"/azure-static-web-app-review"}},"content":"So you\'re using BigQuery (BQ). It\'s all set up and humming perfectly. Maybe now, you want to run an ELT job whenever a new table partition is created, or maybe you want to retrain your ML model whenever new rows are inserted into the BQ table.\\r\\n\\r\\nIn my previous article on [EventArc](https://cloudywithachanceofbigdata.com/eventarc-the-state-of-eventing-in-google-cloud/), we went through how Logging can help us create eventing-type functionality in your application. Let\'s take it a step further and walk through how we can couple BigQuery and Cloud Run.\\r\\n\\r\\nIn this article you will learn how to\\r\\n\\r\\n- Tie together BigQuery and Cloud Run\\r\\n- Use BigQuery\'s audit log to trigger Cloud Run\\r\\n- With those triggers, run your required code\\r\\n\\r\\n## Let\'s go!\\r\\n\\r\\nLet\'s create a temporary dataset within BigQuery named `tmp_bq_to_cr`.\\r\\n\\r\\nIn that same dataset, let\'s create a table in which we will insert some rows to test our BQ audit log. Let\'s grab some rows from a BQ public dataset to create this table:\\r\\n\\r\\n```sql\\r\\nCREATE OR REPLACE TABLE tmp_bq_to_cr.cloud_run_trigger AS\\r\\nSELECT\\r\\n date, country_name, new_persons_vaccinated, population\\r\\n from `bigquery-public-data.covid19_open_data.covid19_open_data`\\r\\n where country_name=\'Australia\'\\r\\n AND\\r\\n date > \'2021-05-31\'\\r\\nLIMIT 100\\r\\n```\\r\\n\\r\\nFollowing this, let\'s run an insert query that will help us build our mock database trigger:\\r\\n\\r\\n```sql\\r\\nINSERT INTO tmp_bq_to_cr.cloud_run_trigger\\r\\nVALUES(\'2021-06-18\', \'Australia\', 3, 1000)\\r\\n```\\r\\n\\r\\nNow, in another browser tab let\'s navigate to [BQ Audit Events](https://console.cloud.google.com/logs/query;query=bigquery.v2?_ga=2.187390252.-505923201.1592376029) and look for our `INSERT INTO` event:\\r\\n\\r\\n[![BQ-insert-event](images/bq-insert-event.png)](images/bq-insert-event.png)\\r\\n\\r\\nThere will be several audit logs for any given BQ action. Only after a query is parsed does BQ know which table we want to interact with, so the initial log will, for e.g., not have the table name.\\r\\n\\r\\nWe don\'t want any old audit log, so we need to ensure we look for a unique set of attributes that clearly identify our action, such as in the diagram above.\\r\\n\\r\\nIn the case of inserting rows, the attributes are a combination of\\r\\n\\r\\n- The method is `google.cloud.bigquery.v2.JobService.InsertJob`\\r\\n- The name of the table being inserted to is the `protoPayload.resourceName`\\r\\n- The dataset id is available as `resource.labels.dataset_id`\\r\\n- The number of inserted rows is `protoPayload.metadata.tableDataChanged.insertedRowsCount`\\r\\n\\r\\n## Time for some code\\r\\n\\r\\nNow that we\'ve identified the payload that we\'re looking for, we can write the action for Cloud Run. We\'ve picked Python and Flask to help us in this instance. ([full code is on GitHub](https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/blob/master/blogs/cloud_run/main.py)).\\r\\n\\r\\nFirst, let\'s filter out the noise and find the event we want to process\\r\\n\\r\\n```python\\r\\n@app.route(\'/\', methods=[\'POST\'])\\r\\ndef index():\\r\\n    # Gets the Payload data from the Audit Log\\r\\n    content = request.json\\r\\n    try:\\r\\n        ds = content[\'resource\'][\'labels\'][\'dataset_id\']\\r\\n        proj = content[\'resource\'][\'labels\'][\'project_id\']\\r\\n        tbl = content[\'protoPayload\'][\'resourceName\']\\r\\n        rows = int(content[\'protoPayload\'][\'metadata\']\\r\\n                   [\'tableDataChange\'][\'insertedRowsCount\'])\\r\\n        if ds == \'cloud_run_tmp\' and \\\\\\r\\n           tbl.endswith(\'tables/cloud_run_trigger\') and rows > 0:\\r\\n            query = create_agg()\\r\\n            return \\"table created\\", 200\\r\\n    except:\\r\\n        # if these fields are not in the JSON, ignore\\r\\n        pass\\r\\n    return \\"ok\\", 200\\r\\n```\\r\\n\\r\\nNow that we\'ve found the event we want, let\'s execute the action we need. In this example, we\'ll aggregate and write out to a new table `created_by_trigger`:\\r\\n\\r\\n```python\\r\\ndef create_agg():\\r\\n    client = bigquery.Client()\\r\\n    query = \\"\\"\\"\\r\\n    CREATE OR REPLACE TABLE tmp_bq_to_cr.created_by_trigger AS\\r\\n    SELECT\\r\\n      count_name, SUM(new_persons_vaccinated) AS n\\r\\n    FROM tmp_bq_to_cr.cloud_run_trigger\\r\\n    \\"\\"\\"\\r\\n    client.query(query)\\r\\n    return query\\r\\n```\\r\\n\\r\\nThe Dockerfile for the container is simply a basic Python container into which we install Flask and the BigQuery client library:\\r\\n\\r\\n```docker\\r\\nFROM python:3.9-slim\\r\\nRUN pip install Flask==1.1.2 gunicorn==20.0.4 google-cloud-bigquery\\r\\nENV APP_HOME /app\\r\\nWORKDIR $APP_HOME\\r\\nCOPY *.py ./\\r\\nCMD exec gunicorn --bind :$PORT main:app\\r\\n```\\r\\n\\r\\n## Now we Cloud Run\\r\\n\\r\\nBuild the container and deploy it using a couple of gcloud commands:\\r\\n\\r\\n```bash\\r\\nSERVICE=bq-cloud-run\\r\\nPROJECT=$(gcloud config get-value project)\\r\\nCONTAINER=\\"gcr.io/${PROJECT}/${SERVICE}\\"\\r\\ngcloud builds submit --tag ${CONTAINER}\\r\\ngcloud run deploy ${SERVICE} --image $CONTAINER --platform managed\\r\\n```\\r\\n\\r\\n## I always forget about the permissions\\r\\n\\r\\nIn order for the trigger to work, the Cloud Run service account will need the following permissions:\\r\\n\\r\\n```bash\\r\\ngcloud projects add-iam-policy-binding $PROJECT \\\\\\r\\n    --member=\\"serviceAccount:service-${PROJECT_NO}@gcp-sa-pubsub.iam.gserviceaccount.com\\"\\\\\\r\\n    --role=\'roles/iam.serviceAccountTokenCreator\'\\r\\n\\r\\ngcloud projects add-iam-policy-binding $PROJECT \\\\\\r\\n    --member=serviceAccount:${SVC_ACCOUNT} \\\\\\r\\n    --role=\'roles/eventarc.admin\'\\r\\n```\\r\\n\\r\\n### Finally, the event trigger\\r\\n\\r\\n```bash\\r\\ngcloud eventarc triggers create ${SERVICE}-trigger \\\\\\r\\n  --location ${REGION} --service-account ${SVC_ACCOUNT} \\\\\\r\\n  --destination-run-service ${SERVICE}  \\\\\\r\\n  --event-filters type=google.cloud.audit.log.v1.written \\\\\\r\\n  --event-filters methodName=google.cloud.bigquery.v2.JobService.InsertJob \\\\\\r\\n  --event-filters serviceName=bigquery.googleapis.com\\r\\n```\\r\\n\\r\\nImportant to note here is that we\'re triggering on _any_ Insert log created by BQ That\'s why in this action we had to filter these events based on the payload.\\r\\n\\r\\n# Take it for a spin\\r\\n\\r\\nNow, try out the BigQuery -> Cloud Run trigger and action. Go to the BigQuery console and insert a row or two:\\r\\n\\r\\n```sql\\r\\nINSERT INTO tmp_bq_to_cr.cloud_run_trigger\\r\\nVALUES(\'2021-06-18\', \'Australia\', 5, 25000)\\r\\n```\\r\\n\\r\\nWatch as a new table called `created_by_trigger` gets created! You have successfully triggered a Cloud Run action on a database event in BigQuery.\\r\\n\\r\\nEnjoy!"},{"id":"azure-static-web-app-review","metadata":{"permalink":"/azure-static-web-app-review","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-06-18-azure-static-web-app-review/index.md","source":"@site/blog/2021-06-18-azure-static-web-app-review/index.md","title":"Azure Static Web App Review","description":"Azure Static WebApp","date":"2021-06-18T00:00:00.000Z","formattedDate":"June 18, 2021","tags":[{"label":"azure","permalink":"/tags/azure"},{"label":"jamstack","permalink":"/tags/jamstack"},{"label":"microsoft-azure","permalink":"/tags/microsoft-azure"},{"label":"netlify","permalink":"/tags/netlify"},{"label":"progressive-web-application","permalink":"/tags/progressive-web-application"},{"label":"pwa","permalink":"/tags/pwa"},{"label":"react","permalink":"/tags/react"},{"label":"single-page-application","permalink":"/tags/single-page-application"},{"label":"spa","permalink":"/tags/spa"},{"label":"vercel","permalink":"/tags/vercel"},{"label":"vue-js","permalink":"/tags/vue-js"}],"readingTime":2.7,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"azure-static-web-app-review","title":"Azure Static Web App Review","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/AzureStaticWebApp.png","tags":["azure","jamstack","microsoft-azure","netlify","progressive-web-application","pwa","react","single-page-application","spa","vercel","vue-js"],"keywords":["azure","jamstack","microsoft-azure","netlify","progressive-web-application","pwa","react","single-page-application","spa","vercel","vue-js"]},"prevItem":{"title":"Use BigQuery to trigger Cloud Run","permalink":"/use-bigquery-to-trigger-cloud-run"},"nextItem":{"title":"Introducing the Metadata Hub (MDH)","permalink":"/introducing-the-metadata-hub-mdh"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Azure Static WebApp](images/AzureStaticWebApp.png)\\r\\n\\r\\nThe Azure Static Web App feature is relatively new in the Azure estate which has recently become generally available, I thought I would take it for a test drive and discuss my findings.\\r\\n\\r\\nI am a proponent of the JAMStack architecture for front end applications and a user of CD enabled CDN services like Netlify, so this Azure feature was naturally appealing to me.\\r\\n\\r\\nAzure SWAs allow you to serve static assets (like JavaScript) without a origin server, meaning you don\u2019t need a web server, are able to streamline content distribution and web app performance, and reduce the attack surface area of your application.\\r\\n\\r\\nThe major advantage to using is simplicity, no scaffolding or infra requirements and it is seamlessly integrated into your CI/CD processes (natively if you are using GitHub).\\r\\n\\r\\n## Deploying Static Web Apps in Azure\\r\\n\\r\\nPretty simple to setup, aside from a name and a resource group, you just need to supply:\\r\\n\\r\\n- a **location** (Azure region to be used for serverless back end APIs via Azure Function Apps) note that this is not a location where the static web is necessarily running\\r\\n- a GitHub or GitLab **repo URL**\\r\\n- the **branch** you wish to use to trigger production deployments (e.g. `main`)\\r\\n- a **path** to your code within your app (e.g. where your `package.json` file is located)\\r\\n- an **output folder** (e.g. `dist`) this should not exist in your repo\\r\\n- a project or personal access **token** for your GitHub account (alternatively you can perform an interactive OAuth2.0 consent if using the portal)\\r\\n\\r\\nAn example is shown here:\\r\\n\\r\\n<Gist id=\\"eef5a25ed01327a180711fd64370c457\\" \\r\\n/>\\r\\n\\r\\n## GitHub Actions\\r\\n\\r\\nUsing the consent provided (either using the OAuth flow or by providing a token), Azure Static Web Apps will automagically create the GitHub Actions workflow to deploy your application on a push or merge event to your repo. This includes providing scoped API credentials to Azure to allow access to the Static Web App resource using secrets in GitHub (which are created automagically as well). An example workflow is shown here:\\r\\n\\r\\n<Gist id=\\"8e7ad2bdd9ba351368c5aedad289e972\\" \\r\\n/>\\r\\n\\r\\n## Preview or Staging Releases\\r\\n\\r\\nSimilar to the functionality in analogous services like Netlify, you can configure preview releases of your application to be deployed from specified branches on pull request events.\\r\\n\\r\\n## Routes and Authorization\\r\\n\\r\\nRoutes (for SPAs) need to be provided to Azure by using a file named `staticwebapp.config.json` located in the application root of your repo (same level as you `package.json` file). You can also specify response codes and whether the rout requires authentication as shown here:\\r\\n\\r\\n<Gist id=\\"7dd3bcf05474da551b3d311ae0729e18\\" \\r\\n/>\\r\\n\\r\\n## Pros\\r\\n\\r\\n- Globally distributed CDN\\r\\n- Increased security posture, reduced attack surface area\\r\\n- Simplified architecture and deployment\\r\\n- No App Service Plan required \u2013 cost reduction\\r\\n- Enables Continuous Deployment \u2013 incl preview/staging environments\\r\\n- TLS and DNS can be easily configured for your app\\r\\n\\r\\n## Cons\\r\\n\\r\\n- Serverless API locations are limited\\r\\n- Integration with other VCS/CI/CD systems like GitLab would need to be custom built (GitHub and Azure DevOps is integrated)\\r\\n\\r\\nOverall, this is a good feature for deploying SPAs or PWAs in Azure.\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"introducing-the-metadata-hub-mdh","metadata":{"permalink":"/introducing-the-metadata-hub-mdh","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-06-15-introducing-the-metadata-hub-mdh/index.md","source":"@site/blog/2021-06-15-introducing-the-metadata-hub-mdh/index.md","title":"Introducing the Metadata Hub (MDH)","description":"Metadata Hub (MDH) is intended to be the source of truth for metadata around the Company\u2019s platform. It has the ability to load metadata configuration from yaml, and serve that information up via API. It will also be the store of information for pipeline information while ingesting files into the platform.","date":"2021-06-15T00:00:00.000Z","formattedDate":"June 15, 2021","tags":[{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"metadata","permalink":"/tags/metadata"}],"readingTime":9.37,"hasTruncateMarker":false,"authors":[{"name":"Tom Klimovski","title":"Senior Cloud Engineer","url":"https://github.com/tomklimovskigamma","imageURL":"http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80","key":"tomklimovski"}],"frontMatter":{"slug":"introducing-the-metadata-hub-mdh","title":"Introducing the Metadata Hub (MDH)","authors":["tomklimovski"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["gcp","google-cloud-platform","metadata"],"keywords":["gcp","google-cloud-platform","metadata"]},"prevItem":{"title":"Azure Static Web App Review","permalink":"/azure-static-web-app-review"},"nextItem":{"title":"Masking Private Keys in CI/CD Pipelines in GitLab","permalink":"/masking-private-keys-in-ci-cd-pipelines-in-gitlab"}},"content":"import ImageWithCaption from \'/js/ImageWithCaption/ImageWithCaption.js\';\\r\\nimport MdhImage from \'./images/mdhoverview.png\';\\r\\n\\r\\nMetadata Hub (MDH) is intended to be the source of truth for metadata around the Company\u2019s platform. It has the ability to load metadata configuration from yaml, and serve that information up via API. It will also be the store of information for pipeline information while ingesting files into the platform.\\r\\n\\r\\n## Key philosophies:\\r\\n\\r\\n> **Config-Driven**. Anyone who has been authorized to do so, should be able to add another \u2018table-info.yaml\u2019 in to MDH without the need to update any code in the system\\r\\n\\r\\nHere\u2019s how table information makes its way into MDH:  \\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={MdhImage}\\r\\naltText=\\"Metadata Hub\\"\\r\\n/>\\r\\n\\r\\n### Paths\\r\\n\\r\\n|  |  |  |  |\\r\\n| --- | --- | --- | --- |\\r\\n| /tables | get: | summary: All tables in MDH | description: get the title of all tables that exist in MDH |\\r\\n|  | post: | summary: Creates a new table in MDH | description: Creates a new table in MDH |\\r\\n| /tables/{id} | get | summary: Obtain information about specific table |  |\\r\\n| /tables/{id}/columns | get | summary: All columns for a particular table | description: Obtain information on columns for a particular table |\\r\\n| /run | get | summary: All information about a particular end-to-end batch run of file ingestion |  |\\r\\n|  | post | summary: Update metadata on a batch load | description: Update metadata on a batch load |\\r\\n| /calendar | get | summary: Use this to save on calculation of business days. | description: This base response gives you today\'s date in a string |\\r\\n| /calendar/previousBusinessDay | get | summary: Will return a string of the previous business day | description: Will return a string of the previous business day, based on the date on when it\'s called |\\r\\n| /calendar/nextBusinessDay | get | summary: Will return a string of the next business day | description: Will return a string of the next business day, based on the date on when it\'s called |\\r\\n|  |  |  |  |\\r\\n\\r\\n# Yaml to Datastore - Entity/Kind design\\r\\n\\r\\n### Datastore Primer\\r\\n\\r\\nBefore we jump right into Entity Groups in Datastore, it is important to first go over the basics and establish a common vocabulary. Datastore holds entities, which are objects, that can contain various key/value pairs, called properties. Each entity must contain a unique identifier, known as a key. When creating an entity, a user can choose to specify a custom key or let Datastore create a key. If a user decides to specify a custom key, it will contain two fields: a kind, which represents a category such as \u2018Toy\u2019 or \u2018Marital Status\u2019, and a name, which is the identifying value. If a user decides to only specify a kind when creating a key, and does not specify a unique identifier, Datastore automatically generates an ID behind the scenes. Below is an example of a Python3 script which illustrates this identifier concept.\\r\\n\\r\\n```python\\r\\nfrom google.cloud import datastore\\r\\n\\r\\nclient = datastore.Client()\\r\\n#Custom key- specify my kind=item and a unique_id of broker\\r\\ncustom_key_entry = datastore.Entity(client.key(\\"table\\",\\"broker\\"))\\r\\nclient.put(custom_key_entry)\\r\\n\\r\\n#Only specify kind=item, let datastore generate unique_id\\r\\ndatastore_gen_key_entry = datastore.Entity(client.key(\\"table\\"))\\r\\nclient.put(datastore_gen_key_entry)\\r\\n```\\r\\n\\r\\nIn your GCP Console under Datastore, you will then see your two entities of kind \u201ctable\u201d. One will contain your custom key and one will contain the automatically generated key.\\r\\n\\r\\nAncestors and Entity Groups\\r\\n\\r\\nFor highly related or hierarchical data, Datastore allows entities to be stored in a parent/child relationship. This is known as an entity group or ancestor/descendent relationship.\\r\\n\\r\\n### Entity Group\\r\\n\\r\\n[![erd](images/erd.png)](images/erd.png)\\r\\n\\r\\n_This is an example of an entity group with kinds of types: table, column, and classification. The \u2018Grandparent\u2019 in this relationship is the \u2018table\u2019. In order to configure this, one must first create the table entity. Then, a user can create a column, and specify that the parent is a table key. In order to create the grandchild, a user then creates a classification and sets its parent to be a column key. To further add customizable attributes, a user can specify additional key-value pairs such as pii and data_type. These key-value pairs are stored as properties. We model this diagram in Datastore in our working example below._\\r\\n\\r\\nOne can create entity groups by setting the \u2018parent\u2019 parameter while creating an entity key for a child. This command adds the parent key to be part of the child entity key. The child\u2019s key is represented as a tuple (\u2018parent_key\u2019, \u2018child_key\u2019), such that the parents\u2019 key is the prefix of the key, which is followed by its own unique identifier. For example, follow the diagram above:\\r\\n\\r\\n```python\\r\\ntable_key = datastore_client.key(\\"table\\",\\"broker\\")\\r\\ncolumn_key = datastore_client.key(\\"column\\",\\"broker_legal_name\\", parent=table_key)\\r\\n```\\r\\n\\r\\nPrinting the variable `table_key` will display: `(\\"table\\", \\"broker\\",\\"column\\", \\"broker_legal_name\\")`\\r\\n\\r\\nDatastore also supports chaining of parents, which can lead to very large keys for descendants with a long lineage of ancestors. Additionally, parents can have multiple children (representing a one-to-many relationship). However, there is no native support for entities to have multiple parents (representing a many-to-many relationship). Once you have configured this ancestral hierarchy, it is easy to retrieve all descendants for a given parent. You can do this by querying on the parent key by using the \u2018ancestor\u2019 parameter. For example, given the entity table_key created above, I can query for all of the tables\\r\\n\\r\\n```python\\r\\ncolumns: my_query = client.query(kind=\\"table\\", ancestor = column_key) .\\r\\n```\\r\\n\\r\\n# A Full Working Example for MDH\\r\\n\\r\\nAs per our Key Philosophies - **_Config-Driven_** - anyone should be able to add a new `table` to be processed and landed in a target-table somewhere within MDH with our yaml syntax. Below is a full working python3 example of the table/column/classification hierarchical model described above.\\r\\n\\r\\n```python\\r\\nfrom google.cloud import datastore\\r\\n\\r\\ndatastore_client = datastore.Client()\\r\\n\\r\\n# Entities with kinds- table, column, classification\\r\\nmy_entities = [\\r\\n{\\"kind\\": \\"table\\", \\"table_id\\": \\"broker\\", \\"table_type\\": \\"snapshot\\",\\r\\n    \\"notes\\": \\"describes mortgage brokers\\"},\\r\\n{\\"kind\\": \\"column\\", \\"column_id\\": \\"broker_legal_name\\", \\"table_id\\": \\"broker\\",\\r\\n    \\"data_type\\": \\"string\\", \\"size\\": 20, \\"nullable\\": 1},\\r\\n{\\"kind\\": \\"column\\", \\"column_id\\": \\"broker_short_code\\", \\"table_id\\": \\"broker\\",\\r\\n    \\"data_type\\": \\"string\\", \\"size\\": 3, \\"nullable\\": 1},\\r\\n{\\"kind\\": \\"classification\\", \\"classification_id\\":\\"classif_id_REQ_01\\",\\r\\n    \\"restriction_level\\": \\"public\\", \\"pii\\": 0, \\"if\\": \\"greater than 90 days\\",\\r\\n    \\"column_id\\": \\"broker_legal_name\\", \\"table_id\\": \\"broker\\"},\\r\\n{\\"kind\\": \\"classification\\", \\"classification_id\\":\\"classif_id_REQ_03\\",\\r\\n    \\"restriction_level\\": \\"restricted\\", \\"pii\\": 0, \\"if\\": \\"less than 90 days\\",\\r\\n    \\"column_id\\": \\"broker_legal_name\\", \\"table_id\\": \\"broker\\"},\\r\\n{\\"kind\\": \\"classification\\", \\"classification_id\\":\\"classif_id_REQ_214\\",\\r\\n    \\"restriction_level\\": \\"public\\", \\"pii\\": 0, \\"column_id\\": \\"broker_short_code\\",\\r\\n    \\"table_id\\": \\"broker\\"},\\r\\n]\\r\\n\\r\\n\\r\\n# traverse my_entities, set parents and add those to datastore\\r\\nfor entity in my_entities:\\r\\n    kind = entity[\'kind\']\\r\\n    parent_key = None\\r\\n    if kind == \\"column\\":\\r\\n        parent_key = datastore_client.key(\\"table\\", entity[\\"table_id\\"])\\r\\n    elif kind == \\"classification\\":\\r\\n        parent_key = datastore_client.key(\\"table\\", entity[\\"table_id\\"],\\r\\n                                          \\"column\\", entity[\\"column_id\\"])\\r\\n\\r\\n    key = datastore_client.key(kind, entity[kind+\\"_id\\"],\\r\\n        parent=parent_key)\\r\\n    datastore_entry = datastore.Entity(key)\\r\\n    datastore_entry.update(entity)\\r\\n\\r\\n    print(\\"Saving: {}\\".format(entity))\\r\\n\\r\\n    datastore_client.put(datastore_entry)\\r\\n```\\r\\n\\r\\nThe code above assumes that you\u2019ve set yourself up with a working Service Account or authorised yourself in, and that your GCP project has been set.\\r\\n\\r\\nNow let\u2019s do some digging around our newly minted Datastore model. Let\u2019s grab the column \u2018broker_legal_name\u2019\\r\\n\\r\\n```python\\r\\nquery1 = datastore_client.query(kind=\\"column\\")\\r\\nquery1.add_filter(\\"column_id\\", \\"=\\", \\"broker_legal_name\\")\\r\\n```\\r\\n\\r\\nNow that we have the column entity, let\u2019s locate it\u2019s parent id.\\r\\n\\r\\n```python\\r\\ncolumn = list(query1.fetch())[0]\\r\\nprint(\\"This column belongs to: \\" +str(column.key.parent.id_or_name))\\r\\n```\\r\\n\\r\\nFurther to this, we can also get all data classification elements attributed to a single column using the ancestor clause query.\\r\\n\\r\\n```python\\r\\nquery2 = datastore_client.query(kind=\\"classification\\", ancestor=column.key)\\r\\nfor classification in list(query2.fetch()):\\r\\n    print(classification.key)\\r\\n    print(classification[\\"restriction_level\\"])\\r\\n```\\r\\n\\r\\nFor more complex queries, Datastore has the concept of indexes being set, usually via it\u2019s index.yaml configuration. The following is an example of an `index.yaml` file:\\r\\n\\r\\n```yaml\\r\\nindexes:\\r\\n  - kind: Cat\\r\\n    ancestor: no\\r\\n    properties:\\r\\n      - name: name\\r\\n      - name: age\\r\\n        direction: desc\\r\\n\\r\\n  - kind: Cat\\r\\n    properties:\\r\\n      - name: name\\r\\n        direction: asc\\r\\n      - name: whiskers\\r\\n        direction: desc\\r\\n\\r\\n  - kind: Store\\r\\n    ancestor: yes\\r\\n    properties:\\r\\n      - name: business\\r\\n        direction: asc\\r\\n      - name: owner\\r\\n        direction: asc\\r\\n```\\r\\n\\r\\nIndexes are important when attempting to add filters on more than one particular attribute within a Datastore entity. For example, the following code will fail:\\r\\n\\r\\n```python\\r\\n# Adding a \'>\' filter will cause this to fail. Sidenote; it will work\\r\\n# without an index if you add another \'=\' filter.\\r\\nquery2 = datastore_client.query(kind=\\"classification\\", ancestor=column.key)\\r\\nquery2.add_filter(\\"pii\\", \\">\\", 0)\\r\\nfor classification in list(query2.fetch()):\\r\\n        print(classification.key)\\r\\n        print(classification[\\"classification_id\\"])\\r\\n```\\r\\n\\r\\nTo rectify this issue, you need to create an index.yaml that looks like the following:\\r\\n\\r\\n```yaml\\r\\nindexes:\\r\\n  - kind: classification\\r\\n    ancestor: yes\\r\\n    properties:\\r\\n      - name: pii\\r\\n```\\r\\n\\r\\nYou would usually upload the yaml file using the gcloud commands:\\r\\n\\r\\n`gcloud datastore indexes create path/to/index.yaml.`\\r\\n\\r\\nHowever, let\u2019s do this programmatically.\\r\\n\\r\\nThe official pypi package for google-cloud-datastore can be found here: https://pypi.org/project/google-cloud-datastore/. At the time of writing, Firestore in Datastore-mode will be the way forward, as per the release note from January 31, 2019.\\r\\n\\r\\n> Cloud Firestore is now Generally Available. Cloud Firestore is the new version of Cloud Datastore and includes a backwards-compatible Datastore mode.\\r\\n\\r\\n> If you intend to use the Cloud Datastore API in a new project, use Cloud Firestore in Datastore mode. Existing Cloud Datastore databases will be automatically upgraded to Cloud Firestore in Datastore mode.\\r\\n\\r\\n> Except where noted, the Cloud Datastore documentation now describes behavior for Cloud Firestore in Datastore mode.\\r\\n\\r\\nWe\u2019ve purposefully created MDH in Datastore to show you how it was done originally, and we\u2019ll be migrating the Datastore code to Firestore in an upcoming post.\\r\\n\\r\\nCreating and deleting indexes within Datastore will need to be done through the REST API via googleapiclient.discovery, as this function doesn\u2019t exist via the google-cloud-datastore API. Working with the discovery api client can be a bit daunting for a first-time user, so here\u2019s the code to add an index on Datastore:\\r\\n\\r\\n```python\\r\\nimport os\\r\\nfrom google.oauth2 import service_account\\r\\nfrom googleapiclient.discovery import build\\r\\nfrom google.cloud import datastore\\r\\n\\r\\n\\r\\nSCOPES = [\'https://www.googleapis.com/auth/cloud-platform\']\\r\\n\\r\\nSERVICE_ACCOUNT_FILE = os.getenv(\'GOOGLE_APPLICATION_CREDENTIALS\')\\r\\nPROJECT_ID = os.getenv(\\"PROJECT_ID\\")\\r\\n\\r\\ncredentials = service_account\\r\\n             .Credentials\\r\\n         .from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\\r\\n\\r\\ndatastore_api = build(\'datastore\', \'v1\', credentials=credentials)\\r\\n\\r\\nbody = {\\r\\n    \'ancestor\': \'ALL_ANCESTORS\',\\r\\n    \'kind\': \'classification\',\\r\\n    \'properties\': [{\\r\\n        \'name\': \'pii\',\\r\\n        \'direction\': \'DESCENDING\'\\r\\n    }]\\r\\n}\\r\\n\\r\\nresponse = datastore_api.projects()\\r\\n           .indexes()\\r\\n           .create(projectId=PROJECT_ID, body=body)\\r\\n           .execute()\\r\\n```\\r\\n\\r\\nHow did we craft this API request? We can use the Google API Discovery Service to build client libraries, IDE plugins, and other tools that interact with Google APIs. The Discovery API provides a list of Google APIs and a machine-readable \\"Discovery Document\\" for each API. Features of the Discovery API:\\r\\n\\r\\n- A directory of supported APIs schemas based on JSON Schema.\\r\\n- A machine-readable \\"Discovery Document\\" for each of the supported APIs. Each document contains:\\r\\n- A list of API methods and available parameters for each method.\\r\\n- A list of available OAuth 2.0 scopes.\\r\\n- Inline documentation of methods, parameters, and available parameter values.\\r\\n\\r\\nNavigating to the API reference page for Datastore and going to the \u2018Datastore Admin\u2019 API page, we can see references to the Indexes and RESTful endpoints we can hit for those Indexes. Therefore, looking at the link for the Discovery document for Datastore:\\r\\n\\r\\n> https://datastore.googleapis.com/$discovery/rest?version=v1\\r\\n\\r\\nFrom this, we can build out our instantiation for the google api discovery object build(\'datastore\', \'v1\', credentials=credentials)\\r\\n\\r\\nWith respect to building out the body aspect of the request, I\u2019ve found crafting that part within the \u2018Try this API\u2019 section of `https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects.indexes/create` pretty valuable.\\r\\n\\r\\nWith this code, your index should show up in your Datastore console! You can also retrieve them within gcloud with gcloud datastore indexes list if you\u2019d like to verify the indexes outside our python code. So there you have it: a working example of entity groups, ancestors, indexes and Metadata within Datastore. Have fun coding!"},{"id":"masking-private-keys-in-ci-cd-pipelines-in-gitlab","metadata":{"permalink":"/masking-private-keys-in-ci-cd-pipelines-in-gitlab","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-06-15-masking-private-keys-in-ci-cd-pipelines-in-gitlab/index.md","source":"@site/blog/2021-06-15-masking-private-keys-in-ci-cd-pipelines-in-gitlab/index.md","title":"Masking Private Keys in CI/CD Pipelines in GitLab","description":"Big fan of GitLab (and GitLab CI in particular). I had a recent requirement to push changes to a wiki repo associated with a GitLab project through a GitLab CI pipeline (using the SaaS version of GitLab) and ran into a conundrum\u2026","date":"2021-06-15T00:00:00.000Z","formattedDate":"June 15, 2021","tags":[{"label":"ci","permalink":"/tags/ci"},{"label":"gitlab","permalink":"/tags/gitlab"},{"label":"gitlab-ci","permalink":"/tags/gitlab-ci"},{"label":"private-keys","permalink":"/tags/private-keys"},{"label":"secrets","permalink":"/tags/secrets"}],"readingTime":1.405,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"masking-private-keys-in-ci-cd-pipelines-in-gitlab","title":"Masking Private Keys in CI/CD Pipelines in GitLab","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["ci","gitlab","gitlab-ci","private-keys","secrets"],"keywords":["ci","gitlab","gitlab-ci","private-keys","secrets"]},"prevItem":{"title":"Introducing the Metadata Hub (MDH)","permalink":"/introducing-the-metadata-hub-mdh"},"nextItem":{"title":"Simple Tasker: Configuration driven orchestration","permalink":"/simple-tasker-configuration-driven-orchestration"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nBig fan of GitLab (and GitLab CI in particular). I had a recent requirement to push changes to a wiki repo associated with a GitLab project through a GitLab CI pipeline (using the SaaS version of GitLab) and ran into a conundrum\u2026\\r\\n\\r\\nUsing the GitLab SaaS version - deploy tokens can\u2019t have write api access, so the next best solution is to use deploy keys, adding your public key as a deploy key and granting this key write access to repositories is relatively straightforward.\\r\\n\\r\\nThis issue is when you attempt to create a masked GitLab CI variable using the private key from your keypair, you get this\u2026\\r\\n\\r\\n[![](images/masked-variable.png)](images/masked-variable.png)\\r\\n\\r\\nI was a bit astonished to see this to be honest\u2026 Looks like it has been raised as an issue several times over the last few years but never resolved (the root cause of which is something to do with newline characters or base64 encoding or the overall length of the string).\\r\\n\\r\\nI came up with a solution! Not pretty but effective, masks the variable so that it cannot be printed in CI logs as shown here:\\r\\n\\r\\n[![](images/ci-ssh-key.png)](images/ci-ssh-key.png)\\r\\n\\r\\n## Setup\\r\\n\\r\\nAdd a masked and protected GitLab variable for each line in the private key, for example:\\r\\n\\r\\n[![](images/masked-vars.png)](images/masked-vars.png)\\r\\n\\r\\n## The Code\\r\\n\\r\\nAdd the following block to your `.gitlab-ci.yml` file:\\r\\n\\r\\n<Gist id=\\"b5260f14ecc0bf0d080c80297d0b475c\\" \\r\\n/>\\r\\n\\r\\nnow within Jobs in your pipeline you can simply do this to clone, push or pull from a remote GitLab repo:\\r\\n\\r\\n<Gist id=\\"c96e211544f7cb4ef3ca4e90dc8e36e3\\" \\r\\n/>\\r\\n\\r\\nas mentioned not pretty, but effective and no other cleaner options as I could see\u2026\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"simple-tasker-configuration-driven-orchestration","metadata":{"permalink":"/simple-tasker-configuration-driven-orchestration","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-06-15-simple-tasker-configuration-driven-orchestration/index.md","source":"@site/blog/2021-06-15-simple-tasker-configuration-driven-orchestration/index.md","title":"Simple Tasker: Configuration driven orchestration","description":"Recently I found myself at a client that were using a third party tool to scan all their enterprise applications in order to collate their data lineage. They had spent two years onboarding applications to the tool, resulting in a large technical mess that was hard to debug and impossible to extend. As new applications were integrated onto the platform, developers were forced to think of new ways of connecting and tranforming the data so it could be consumed.","date":"2021-06-15T00:00:00.000Z","formattedDate":"June 15, 2021","tags":[{"label":"data-lineage","permalink":"/tags/data-lineage"},{"label":"orchestration","permalink":"/tags/orchestration"},{"label":"python","permalink":"/tags/python"}],"readingTime":5.67,"hasTruncateMarker":false,"authors":[{"name":"Mark Stella","title":"Senior Cloud Engineer","url":"https://github.com/mpstella","imageURL":"http://1.gravatar.com/avatar/9a7465656212285f24f64326cd38d6c9?s=80","key":"markstella"}],"frontMatter":{"slug":"simple-tasker-configuration-driven-orchestration","title":"Simple Tasker: Configuration driven orchestration","authors":["markstella"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["data-lineage","orchestration","python"],"keywords":["data-lineage","orchestration","python"]},"prevItem":{"title":"Masking Private Keys in CI/CD Pipelines in GitLab","permalink":"/masking-private-keys-in-ci-cd-pipelines-in-gitlab"},"nextItem":{"title":"Okta Admin Command Line Interface","permalink":"/okta-admin-command-line-interface"}},"content":"Recently I found myself at a client that were using a third party tool to scan all their enterprise applications in order to collate their data lineage. They had spent two years onboarding applications to the tool, resulting in a large technical mess that was hard to debug and impossible to extend. As new applications were integrated onto the platform, developers were forced to think of new ways of connecting and tranforming the data so it could be consumed.\\r\\n\\r\\nThe general approach was: `setup scanner` -> `scan application` -> `modify results` -> `upload results` -> `backup results` -> `cleanup workspace` -> `delete anything older than \'X\' days`\\r\\n\\r\\nEach developer had their own style of doing this - involving shell scripts, python scripts, SQL and everything in between. Worse, there was slabs of code replicated across the entire repository, with variables and paths changed depending on the use case.\\r\\n\\r\\nMy tasks was to create a framework that could orchestrate the scanning and adhered to the following philosophies:\\r\\n\\r\\n- DRY (Don\'t Repeat Yourself)\\r\\n- Config driven\\r\\n- Version controlled\\r\\n- Simple to extend\\r\\n- Idempotent\\r\\n\\r\\nIt also had to be written in Python as that was all the client was skilled in.\\r\\n\\r\\nAfter looking at what was on the market (Airflow and Prefect being the main contenders) I decided to roll my own simplified orchestrator that required as little actual coding as possible and could be setup by configuration.\\r\\n\\r\\nIn choosing a configuration format, I settled on [HOCON](https://github.com/lightbend/config/blob/master/HOCON.md) as it closely resembled JSON but has advanced features such as interpolation, substitions and the ability to include other hocon files - this would drastically reduce the amount of boilerplate configuration required.\\r\\n\\r\\nBecause I had focused so heavily on being configuration driven, I also needed the following charecteristics to be delivered:\\r\\n\\r\\n- Self discovery of task types (more on this later)\\r\\n- Configuration validation at startup\\r\\n\\r\\n## Tasks and self discovery\\r\\n\\r\\nAs I wanted anyone to be able to rapidly extend the framework by adding tasks, I needed to reduce as much repetition and boilerplate as possible. Ideally, I wanted a developer to just have to think about writing code and not have to deal with how to integrate this.\\r\\n\\r\\nTo achieve this, we needed a way of registering new \'tasks\' that would become available to the framework. I wanted a developer to simply have to subclass the main Task class and implement a run function - the rest would be taken care of.\\r\\n\\r\\n```python\\r\\nclass TaskRegistry:\\r\\n\\r\\n    def __init__(self) -> None:\\r\\n        self._registry = {}\\r\\n\\r\\n    def register(self, cls: type) -> None:\\r\\n        n = getattr(cls, \'task_name\', cls.__name__).lower()\\r\\n        self._registry[n] = cls\\r\\n\\r\\n    def registered(self) -> List[str]:\\r\\n        return list(self._registry.keys())\\r\\n\\r\\n    def has(self, name: str) -> bool:\\r\\n        return name in self._registry\\r\\n\\r\\n    def get(self, name: str) -> type:\\r\\n        return self._registry[name]\\r\\n\\r\\n    def create(self, name: str, *args, **kwargs) -> object:\\r\\n        try:\\r\\n            return self._registry[name](*args, **kwargs)\\r\\n        except KeyError:\\r\\n            raise ClassNotRegisteredException(name)\\r\\n\\r\\n\\r\\nregistry = TaskRegistry()\\r\\n```\\r\\n\\r\\nOnce the registry was instantiated, any new Tasks that inherited from \'Task\' would automatically be added to the registry. We could then use the `create(name)` function to instantiate any class - essentially a pythonic [Factory Method](https://en.wikipedia.org/wiki/Factory_method_pattern)\\r\\n\\r\\n```python\\r\\nclass Task(ABC):\\r\\n\\r\\n    def __init__(self) -> None:\\r\\n        self.logger = logging.getLogger(self.__class__.__name__)\\r\\n\\r\\n    def __init_subclass__(cls) -> None:\\r\\n        registry.register(cls)\\r\\n\\r\\n    @abstractmethod\\r\\n    def run(self, **kwargs) -> bool:\\r\\n        raise NotImplementedError\\r\\n```\\r\\n\\r\\nFor the framework to automatically register the classes, it was important to follow the project structure. As long as the task resided in the \'tasks\' module, we could scan this at runtime and register each task.\\r\\n\\r\\n```text\\r\\n\u2514\u2500\u2500 simple_tasker\\r\\n    \u251c\u2500\u2500 __init__.py\\r\\n    \u251c\u2500\u2500 cli.py\\r\\n    \u2514\u2500\u2500 tasks\\r\\n        \u251c\u2500\u2500 __init__.py\\r\\n        \u251c\u2500\u2500 archive.py\\r\\n        \u2514\u2500\u2500 shell_script.py\\r\\n```\\r\\n\\r\\nThis was achieved with a simple dynamic module importer\\r\\n\\r\\n```python\\r\\nmodules = glob.glob(join(dirname(__file__), \\"*.py\\"))\\r\\n\\r\\nfor f in modules:\\r\\n    if isfile(f) and not f.endswith(\\"__init__.py\\"):\\r\\n        __import__(f\\"{Task.__module__}.{basename(f)[:-3]}\\")\\r\\n```\\r\\n\\r\\n## The configuration\\r\\n\\r\\nIn designing how the configuration would bind to the task, I needed to capture the `name` (what object to instanticate) and what `args` to pass to the instantiated run function. I decided to model it as below with everything under a \'tasks\' array\\r\\n\\r\\n```text\\r\\ntasks: [\\r\\n    {\\r\\n        name: shell_script\\r\\n        args: {\\r\\n            script_path: uname\\r\\n            script_args: -a\\r\\n        }\\r\\n    },\\r\\n    {\\r\\n        name: shell_script\\r\\n        args: {\\r\\n            script_path: find\\r\\n            script_args: [${CWD}/simple_tasker/tasks, -name, \\"*.py\\"]\\r\\n        }\\r\\n    },\\r\\n    {\\r\\n        name: archive\\r\\n        args: {\\r\\n            input_directory_path: ${CWD}/simple_tasker/tasks\\r\\n            target_file_path: /tmp/${PLATFORM}_${TODAY}.tar.gz\\r\\n        }\\r\\n    }\\r\\n]\\r\\n```\\r\\n\\r\\n## Orchestration and validation\\r\\n\\r\\nAs mentioned previously, one of the goals was to ensure the configuration was valid prior to any execution. This meant that the framework needed to validate whether tha task `name` referred to a registered task, and that all mandatory `arguments` were addressed in the configuration. Determining whether the task was registered was just a simple key check, however to validate the arguments to the run required some inspection - I needed to get all args for the run function and filter out \'self\' and any asterisk args (\\\\*args, \\\\*\\\\*kwargs)\\r\\n\\r\\n```python\\r\\ndef get_mandatory_args(func) -> List[str]:\\r\\n\\r\\n    mandatory_args = []\\r\\n    for k, v in inspect.signature(func).parameters.items():\\r\\n        if (\\r\\n            k != \\"self\\"\\r\\n            and v.default is inspect.Parameter.empty\\r\\n            and not str(v).startswith(\\"*\\")\\r\\n        ):\\r\\n            mandatory_args.append(k)\\r\\n\\r\\n    return mandatory_args\\r\\n```\\r\\n\\r\\nAnd finally onto the actual execution bit. The main functionality required here is to validate that the config was defined correctly, then loop through all tasks and execute them - passing in any args.\\r\\n\\r\\n```python\\r\\nclass Tasker:\\r\\n\\r\\n    def __init__(self, path: Path, env: Dict[str, str] = None) -> None:\\r\\n\\r\\n        self.logger = logging.getLogger(self.__class__.__name__)\\r\\n        self._tasks = []\\r\\n\\r\\n        with wrap_environment(env):\\r\\n            self._config = ConfigFactory.parse_file(path)\\r\\n\\r\\n\\r\\n    def __validate_config(self) -> bool:\\r\\n\\r\\n        error_count = 0\\r\\n\\r\\n        for task in self._config.get(\\"tasks\\", []):\\r\\n            name, args = task[\\"name\\"].lower(), task.get(\\"args\\", {})\\r\\n\\r\\n            if registry.has(name):\\r\\n                for arg in get_mandatory_args(registry.get(name).run):\\r\\n                    if arg not in args:\\r\\n                        print(f\\"Missing arg \'{arg}\' for task \'{name}\'\\")\\r\\n                        error_count += 1\\r\\n            else:\\r\\n                print(f\\"Unknown tasks \'{name}\'\\")\\r\\n                error_count += 1\\r\\n\\r\\n            self._tasks.append((name, args))\\r\\n\\r\\n        return error_count == 0\\r\\n\\r\\n    def run(self) -> bool:\\r\\n\\r\\n        if self.__validate_config():\\r\\n\\r\\n            for name, args in self._tasks:\\r\\n                exe = registry.create(name)\\r\\n                self.logger.info(f\\"About to execute: \'{name}\'\\")\\r\\n                if not exe.run(**args):\\r\\n                    self.logger.error(f\\"Failed tasks \'{name}\'\\")\\r\\n                    return False\\r\\n\\r\\n            return True\\r\\n        return False\\r\\n```\\r\\n\\r\\n## Putting it together - sample tasks\\r\\n\\r\\nBelow are two examples of how easy it is to configure the framework. We have a simple folder archiver that will tar/gz a directory based on 2 input parameters.\\r\\n\\r\\n```python\\r\\nclass Archive(Task):\\r\\n\\r\\n    def __init__(self) -> None:\\r\\n        super().__init__()\\r\\n\\r\\n    def run(self, input_directory_path: str, target_file_path: str) -> bool:\\r\\n\\r\\n        self.logger.info(f\\"Archiving \'{input_directory_path}\' to \'{target_file_path}\'\\")\\r\\n\\r\\n        with tarfile.open(target_file_path, \\"w:gz\\") as tar:\\r\\n            tar.add(\\r\\n                input_directory_path,\\r\\n                arcname=os.path.basename(input_directory_path)\\r\\n            )\\r\\n        return True\\r\\n```\\r\\n\\r\\nA more complex example would be the ability to execute shell scripts (or os functions) by passing in some optional variables and variables that can either be a string or list.\\r\\n\\r\\n```python\\r\\nclass ShellScript(Task):\\r\\n\\r\\n    task_name = \\"shell_script\\"\\r\\n\\r\\n    def __init__(self) -> None:\\r\\n        super().__init__()\\r\\n\\r\\n    def run(\\r\\n        self,\\r\\n        script_path: str,\\r\\n        script_args: Union[str, List[str]] = None,\\r\\n        working_directory_path: str = None\\r\\n    ) -> bool:\\r\\n\\r\\n        cmd = [script_path]\\r\\n\\r\\n        if isinstance(script_args, str):\\r\\n            cmd.append(script_args)\\r\\n        else:\\r\\n            cmd += script_args\\r\\n\\r\\n        try:\\r\\n\\r\\n            result = subprocess.check_output(\\r\\n                cmd,\\r\\n                stderr=subprocess.STDOUT,\\r\\n                cwd=working_directory_path\\r\\n            ).decode(\\"utf-8\\").splitlines()\\r\\n\\r\\n            for o in result:\\r\\n                self.logger.info(o)\\r\\n\\r\\n        except (subprocess.CalledProcessError, FileNotFoundError) as e:\\r\\n            self.logger.error(e)\\r\\n            return False\\r\\n\\r\\n        return True\\r\\n```\\r\\n\\r\\nYou can view the entire implementation [here](https://github.com/mpstella/simple_tasker)"},{"id":"okta-admin-command-line-interface","metadata":{"permalink":"/okta-admin-command-line-interface","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-05-30-okta-admin-command-line-interface/index.md","source":"@site/blog/2021-05-30-okta-admin-command-line-interface/index.md","title":"Okta Admin Command Line Interface","description":"Okta Admin CLI","date":"2021-05-30T00:00:00.000Z","formattedDate":"May 30, 2021","tags":[{"label":"cli","permalink":"/tags/cli"},{"label":"command-line","permalink":"/tags/command-line"},{"label":"golang","permalink":"/tags/golang"},{"label":"identity","permalink":"/tags/identity"},{"label":"oauth-2-0","permalink":"/tags/oauth-2-0"},{"label":"oidc","permalink":"/tags/oidc"},{"label":"okta","permalink":"/tags/okta"}],"readingTime":1.675,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"okta-admin-command-line-interface","title":"Okta Admin Command Line Interface","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/Dev_Logo-02_Large-thumbnail.png","tags":["cli","command-line","golang","identity","oauth-2-0","oidc","okta"],"keywords":["cli","command-line","golang","identity","oauth-2-0","oidc","okta"]},"prevItem":{"title":"Simple Tasker: Configuration driven orchestration","permalink":"/simple-tasker-configuration-driven-orchestration"},"nextItem":{"title":"Enumerating all roles for a user in Snowflake","permalink":"/enumerating-all-roles-for-a-user-in-snowflake"}},"content":"![Okta Admin CLI](images/Dev_Logo-02_Large-thumbnail.png)\\r\\n\\r\\nIdentity and Access Management is a critical component of any application or SaaS architecture. I\u2019m currently doing a spike of the Okta solution for an application development project I am on. Okta is a comprehensive solution built on the open OAuth2 and OIDC protocols, as well as supporting more conventional identity federation approaches such as SAML.\\r\\n\\r\\nOkta has a clean and easy to use web-based Admin interface which can be used to create applications, users, claims, identity providers and more.\\r\\n\\r\\nDuring my spike, which was done in a crash and burn test Okta organisation, I had associated my user account with a Microsoft Identity Provider for SSO, and subsequently had issues accessing the Microsoft Account my user was associated with, as a result I managed to lock myself (the super admin) out of the Okta Admin Console.\\r\\n\\r\\nFortunately, prior to doing this I had created an API token for my user. So, I went about looking at ways I could interact with Okta programmatically. My first inclination was to use a simple CLI for Okta to get me out of jail\u2026 but I found there wasn\u2019t one that suited. There are, however, a wealth of SDKs for Okta across multiple front-end and back-end oriented programming languages (such as JavaScript, Golang, Python and more).\\r\\n\\r\\nBeing in lockdown and having some free time on my hands, I decided to create a simple open source command line tool which could be used to administer an Okta organisation. The result of this weekend lockdown is `okta-admin`\u2026\\r\\n\\r\\n[![okta-admin cli](images/okta-admin-screenshot.png)](images/okta-admin-screenshot.png)\\r\\n\\r\\nFor this project I used the [Golang SDK for Okta](https://github.com/okta/okta-sdk-golang), along with the [Cobra](https://github.com/spf13/cobra) and [Viper](https://github.com/spf13/viper) Golang packages (used by `docker`, `kubectl` and other popular command line utilities). To provide a query interface to JSON response payloads I use [GJson](https://github.com/tidwall/gjson).\\r\\n\\r\\nWill keep adding to this so stay tuned...\\r\\n\\r\\n> Complete source code for this project is available at [https://github.com/gammastudios/okta-admin](https://github.com/gammastudios/okta-admin)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"enumerating-all-roles-for-a-user-in-snowflake","metadata":{"permalink":"/enumerating-all-roles-for-a-user-in-snowflake","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-03-23-enumerating-all-roles-for-a-user-in-snowflake/index.md","source":"@site/blog/2021-03-23-enumerating-all-roles-for-a-user-in-snowflake/index.md","title":"Enumerating all roles for a user in Snowflake","description":"Snowflake","date":"2021-03-23T00:00:00.000Z","formattedDate":"March 23, 2021","tags":[{"label":"javascript","permalink":"/tags/javascript"},{"label":"rbac","permalink":"/tags/rbac"},{"label":"roles","permalink":"/tags/roles"},{"label":"snowflake","permalink":"/tags/snowflake"},{"label":"sql","permalink":"/tags/sql"},{"label":"stored-procedure","permalink":"/tags/stored-procedure"},{"label":"tail-call-recursion","permalink":"/tags/tail-call-recursion"},{"label":"tailcall","permalink":"/tags/tailcall"}],"readingTime":1.1,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"enumerating-all-roles-for-a-user-in-snowflake","title":"Enumerating all roles for a user in Snowflake","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/snowflake.png","tags":["javascript","rbac","roles","snowflake","sql","stored-procedure","tail-call-recursion","tailcall"],"keywords":["javascript","rbac","roles","snowflake","sql","stored-procedure","tail-call-recursion","tailcall"]},"prevItem":{"title":"Okta Admin Command Line Interface","permalink":"/okta-admin-command-line-interface"},"nextItem":{"title":"EventArc: The state of eventing in Google Cloud","permalink":"/eventarc-the-state-of-eventing-in-google-cloud"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Snowflake](images/snowflake.png)\\r\\n\\r\\nSnowflake allows roles to be assigned to other roles, so when a user is assigned to a role, they may inherit the ability to use countless other roles.\\r\\n\\r\\n**Challenge:** recursively enumerate all roles for a given user\\r\\n\\r\\nOne solution would be to create a complex query on the `\u201cSNOWFLAKE\\".\\"ACCOUNT_USAGE\\".\\"GRANTS_TO_ROLES\\"` object.\\r\\n\\r\\nAn easier solution is to use a stored procedure to recurse through grants for a given user and return an `ARRAY` of roles for that user.\\r\\n\\r\\nThis is a good programming exercise in tail call recursion (sort of) in JavaScript. Here is the code:\\r\\n\\r\\n<Gist id=\\"9b9985dbf8163ade22b71f2ccf20cb51\\" \\r\\n/>\\r\\n\\r\\nTo call the stored proc, execute:\\r\\n\\r\\n<Gist id=\\"fbbfaa3b67af828e4d905411567cd031\\" \\r\\n/>\\r\\n\\r\\nOne drawback of stored procedures in Snowflake is that they can only have scalar or array return types and cannot be used directly in a SQL query, however you can use the `table(result_scan(last_query_id()))` trick to get around this, as shown below where we will pivot the `ARRAY` into a record set with the array elements as rows:\\r\\n\\r\\n<Gist id=\\"6a7e8bc552b87ab1e039f22bacf1b65f\\" \\r\\n/>\\r\\n\\r\\n__IMPORTANT__\\r\\n\\r\\nThis query **must** be the next statement run immediately after the `CALL` statement and cannot be run again until you run another `CALL` statement.\\r\\n\\r\\nMore adventures with Snowflake soon!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"eventarc-the-state-of-eventing-in-google-cloud","metadata":{"permalink":"/eventarc-the-state-of-eventing-in-google-cloud","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-02-28-eventarc-the-state-of-eventing-in-google-cloud/index.md","source":"@site/blog/2021-02-28-eventarc-the-state-of-eventing-in-google-cloud/index.md","title":"EventArc: The state of eventing in Google Cloud","description":"When defining event-driven architectures, it\'s always good to keep up with how the landscape is changing. How do you connect microservices in your architecture? Is Pub/Sub the end-game for all events? To dive a bit deeper, let\'s talk through the benefits of having a single\xa0orchestrator, or perhaps a choreographer is better?","date":"2021-02-28T00:00:00.000Z","formattedDate":"February 28, 2021","tags":[{"label":"eventarc","permalink":"/tags/eventarc"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"microservices","permalink":"/tags/microservices"},{"label":"gcp","permalink":"/tags/gcp"}],"readingTime":2.81,"hasTruncateMarker":false,"authors":[{"name":"Tom Klimovski","title":"Senior Cloud Engineer","url":"https://github.com/tomklimovskigamma","imageURL":"http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80","key":"tomklimovski"}],"frontMatter":{"slug":"eventarc-the-state-of-eventing-in-google-cloud","title":"EventArc: The state of eventing in Google Cloud","authors":["tomklimovski"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["eventarc","google-cloud-platform","microservices","gcp"],"keywords":["eventarc","google-cloud-platform","microservices","gcp"]},"prevItem":{"title":"Enumerating all roles for a user in Snowflake","permalink":"/enumerating-all-roles-for-a-user-in-snowflake"},"nextItem":{"title":"Microservices Concepts: Orchestration versus Choreography","permalink":"/microservices-concepts-orchestration-versus-choreography"}},"content":"When defining event-driven architectures, it\'s always good to keep up with how the landscape is changing. How do you connect microservices in your architecture? Is Pub/Sub the end-game for all events? To dive a bit deeper, let\'s talk through the benefits of having a single\xa0_orchestrator_, or perhaps a choreographer is better?\\r\\n\\r\\n## Orchestration versus choreography refresher\\r\\n\\r\\nMy colleague [@jeffreyaven](https://www.linkedin.com/in/jeffreyaven/) did a recent post explaining this concept in simple terms, which is worth reviewing, see:\\r\\n\\r\\n[__Microservices Concepts: Orchestration versus Choreography__](https://cloudywithachanceofbigdata.com/microservices-concepts-orchestration-versus-choreography/)\\r\\n\\r\\nShould there really be a central orchestrator controlling all interactions between services.....or, should each service work independently and only interact through events?\\r\\n\\r\\n- **Orchestration**\xa0is usually viewed as a domain-wide central service that defines the flow and control of communication between services. In this paradigm, in becomes easier to change and ultimately monitor policies across your org.\\r\\n- **Choreography**\xa0has each service registering and emitting events as they need to. It doesn\'t direct or define the flow of communication, but using this method usually has a central broker passing around messages and allows services to be truly independent.\\r\\n\\r\\nEnter\xa0[Workflows](https://cloud.google.com/workflows), which is suited for centrally orchestrated services. Not only Google Cloud service such as Cloud Functions and Cloud Run, but also external services.\\r\\n\\r\\nHow about choreography?\xa0[Pub/Sub](https://cloud.google.com/pubsub)\xa0and\xa0[Eventarc](https://cloud.google.com/blog/products/serverless/build-event-driven-applications-in-cloud-run)\xa0are both suited for this. We all know and love Pub/Sub,\xa0_but how do I use EventArc?_\\r\\n\\r\\n## What is Eventarc?\\r\\n\\r\\nAnnounced in October-2020, it was introduced as eventing functionality that enables you, the developer, to send events\xa0_to_\xa0Cloud Run from more than 60 Google Cloud sources.\\r\\n\\r\\n### But how does it work?\\r\\n\\r\\nEventing is done by reading those sweet sweet Audit Logs, from various sources, and sending them to Cloud Run services as events in\xa0[Cloud Events](https://cloudevents.io/)\xa0format. Quick primer on Cloud Events: its a specification for describing event data in a common way. The specification is now under the\xa0[Cloud Native Computing Foundation](https://cncf.io/). Hooray! It can also read events from Pub/Sub topics for custom applications. Here\'s a diagram I graciously ripped from\xa0[Google Cloud Blog](https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud):\\r\\n\\r\\n[![Eventarc](images/CloudEvents_fig1.png)](images/CloudEvents_fig1.png)\\r\\n\\r\\n### Why do I need Eventarc? I have the Pub/Sub\\r\\n\\r\\nGood question. Eventarc provides and easier path to receive events not only from Pub/Sub topics but from a number of Google Cloud sources with its Audit Log and Pub/Sub integration. Actually,\xa0_any_\xa0service that has Audit Log integration can be an event source for Eventarc. Beyond easy integration, it provides consistency and structure to how events are generated, routed and consumed. Things like:\\r\\n\\r\\n#### **Triggers**\\r\\n\\r\\nThey specify routing rules from events sources, to event sinks. Listen for new object creation in GCS and route that event to a service in Cloud Run by creating an Audit-Log-Trigger. Create triggers that also listen to Pub/Sub. Then list\xa0**all**\xa0triggers in one, central place in Eventarc:\\r\\n\\r\\n`gcloud beta eventarc triggers list`\\r\\n\\r\\n#### **Consistency with eventing format and libraries**\\r\\n\\r\\nUsing the CloudEvent-compliant specification will allow for event data in a common way, increasing the movement towards the goal of consistency, accessibility and portability. Makes it easier for different languages to read the event and Google Events Libraries to parse fields.\\r\\n\\r\\nThis means that the long-term vision of Eventarc to be the\xa0**hub**\xa0of events, enabling a unified eventing story for Google Cloud and beyond.\\r\\n\\r\\n[![Eventarc producers and consumers](images/CloudEvents_fig2.png)](images/CloudEvents_fig2.png)\\r\\n\\r\\nIn the future, you can excpect to forego Audit Log and read these events directly and send these out to even more sinks within GCP and any HTTP target.\\r\\n\\r\\n* * *\\r\\n\\r\\nThis article written on inspiration from\xa0[https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud](https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud). Thanks Mete Atamel!"},{"id":"microservices-concepts-orchestration-versus-choreography","metadata":{"permalink":"/microservices-concepts-orchestration-versus-choreography","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-02-26-microservices-concepts-orchestration-versus-choreography/index.md","source":"@site/blog/2021-02-26-microservices-concepts-orchestration-versus-choreography/index.md","title":"Microservices Concepts: Orchestration versus Choreography","description":"Orchestration versus Choreography","date":"2021-02-26T00:00:00.000Z","formattedDate":"February 26, 2021","tags":[{"label":"microservices","permalink":"/tags/microservices"}],"readingTime":1.82,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"microservices-concepts-orchestration-versus-choreography","title":"Microservices Concepts: Orchestration versus Choreography","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/service-mesh-1.png","tags":["microservices"],"keywords":["microservices"]},"prevItem":{"title":"EventArc: The state of eventing in Google Cloud","permalink":"/eventarc-the-state-of-eventing-in-google-cloud"},"nextItem":{"title":"Using the Azure CLI to Create an API using a Function App within API Management","permalink":"/using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management"}},"content":"![Orchestration versus Choreography](images/service-mesh-1.png)\\r\\n\\r\\nOne of the foundational concepts in microservices architecture and design patterns is the concept of Orchestration versus Choreography. Before we look at a reference implementation of each of these patterns, it is worthwhile starting with an analogy.\\r\\n\\r\\nThis is often likened to a Jazz band versus a Symphony Orchestra.\\r\\n\\r\\nA modern symphony orchestra is normally comprised of sections such as strings, brass, woodwind and percussion sections. The sections are orchestrated by a conductor, usually placed at a central point with respect to each of the sections. The conductor instructs each section to perform their components of the overall symphony.\\r\\n\\r\\nBy contrast, a Jazz band does not have a conductor and also features improvisation, with different musicians improvising based upon each other. Choreography, although more aligned to dance, can involve improvisation. In both cases there is still an intended output and a general framework as to how the composition will be executed, however unlike a symphony orchestra there is a degree of spontaneity.\\r\\n\\r\\n_Now back to technology and microservices\u2026_\\r\\n\\r\\nIn the Orchestration model, there is a central orchestration service which controls the interactions between other services, in other words the flow and control of communication and/or message passing between services is controlled by an orchestrator (much like the conductor in a symphony orchestra). On the plus side, this model enables easier monitoring and policy enforcement across the system. A generalisation of the Orchestration model is shown below:\\r\\n\\r\\n[![Orchestration model](images/orchestration.png)](images/orchestration.png)\\r\\n\\r\\nBy contrast, in the Choreography model, each service works independently and interacts with other services through events. In this model each service registers and emits events as they need to. The flow (of communication between services) is not predefined, much like a Jazz band. This model often includes a central broker for message passing between services, but the services operate independently of each other and are not controlled by a central service (an orchestrator). A generalisation of the Choreography model is shown below:\\r\\n\\r\\n[![Choreography model](images/choreography.png)](images/choreography.png)\\r\\n\\r\\nWe will post subsequent articles with implementations of these patterns, but it is worthwhile getting a foundational understanding first.\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management","metadata":{"permalink":"/using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2021-01-06-using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management/index.md","source":"@site/blog/2021-01-06-using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management/index.md","title":"Using the Azure CLI to Create an API using a Function App within API Management","description":"API Management Function App","date":"2021-01-06T00:00:00.000Z","formattedDate":"January 6, 2021","tags":[{"label":"api-management","permalink":"/tags/api-management"},{"label":"apis","permalink":"/tags/apis"},{"label":"azure","permalink":"/tags/azure"},{"label":"function-app","permalink":"/tags/function-app"},{"label":"microservices","permalink":"/tags/microservices"},{"label":"microsoft-azure","permalink":"/tags/microsoft-azure"},{"label":"serverless","permalink":"/tags/serverless"}],"readingTime":1.335,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management","title":"Using the Azure CLI to Create an API using a Function App within API Management","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/api-management-function-app.png","tags":["api-management","apis","azure","function-app","microservices","microsoft-azure","serverless"],"keywords":["api-management","apis","azure","function-app","microservices","microsoft-azure","serverless"]},"prevItem":{"title":"Microservices Concepts: Orchestration versus Choreography","permalink":"/microservices-concepts-orchestration-versus-choreography"},"nextItem":{"title":"Great Expectations (for your data...)","permalink":"/great-expectations-for-your-data"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![API Management Function App](images/api-management-function-app.png)\\r\\n\\r\\nFunction Apps, Logic Apps and App Services can be used to expose APIs within Azure API Management which is an easy way to deploy serverless microservices. You can see this capability in the Azure portal below within API Management:\\r\\n\\r\\n[![Add a new API using a Function App as a back end](images/apimamanagement-add-fnapp.png)](images/apimamanagement-add-fnapp.png)\\r\\n\\r\\nLike most readers, I like to script everything, so I was initially frustrated when I couldn\u2019t replicate this operation in the Azure cli, REST, PowerShell, or any of the other SDKs or IaC tools. Others shared my frustration as seen [here](https://feedback.azure.com/forums/248703-api-management/suggestions/36832033-programmatically-import-azure-function-into-apim).\\r\\n\\r\\nI was nearly resigned to using click ops in the portal (arrrgh) before I worked out this workaround.\\r\\n\\r\\n## The Solution\\r\\n\\r\\nThere is a bit more prep work required to automate this process, but it is well worth it.\\r\\n\\r\\n1. Create an OpenApi (or Swagger spec or WADL) specification document, as seen below (use the absolute URL for your Function App in the `url` parameter):\\r\\n\\r\\n<Gist id=\\"077e8f313e6f44393df71057c8af7850\\" \\r\\n/>\\r\\n\\r\\n2. Use the `az apim api import` function (not the `az apim api create` function), as shown here:\\r\\n\\r\\n<Gist id=\\"1f5eec542bd5ec01dbb9a06472e8e59b\\" \\r\\n/>\\r\\n\\r\\n3. Associate the API with a product (which is how you can rate limit APIs)\\r\\n\\r\\n<Gist id=\\"4ad9c81b97ee97fb2cb6f794c2ae820f\\" \\r\\n/>\\r\\n\\r\\nThat\u2019s it! You can now access your function via the API gateway using the gateway url or via the developer portal as seen below:\\r\\n\\r\\n[![Function App API in API Management in the Azure Portal](images/apimamanagement-test-api.png)](images/apimamanagement-test-api.png)\\r\\n\\r\\n[![Function App API in the Dev Portal](images/apimamanagement-dev-portal.png)](images/apimamanagement-dev-portal.png)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"great-expectations-for-your-data","metadata":{"permalink":"/great-expectations-for-your-data","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-11-27-great-expectations-for-your-data/index.md","source":"@site/blog/2020-11-27-great-expectations-for-your-data/index.md","title":"Great Expectations (for your data...)","description":"Great Expectations","date":"2020-11-27T00:00:00.000Z","formattedDate":"November 27, 2020","tags":[{"label":"data","permalink":"/tags/data"},{"label":"data-quality","permalink":"/tags/data-quality"},{"label":"data-quality-management","permalink":"/tags/data-quality-management"}],"readingTime":6.385,"hasTruncateMarker":false,"authors":[{"name":"Tom Klimovski","title":"Senior Cloud Engineer","url":"https://github.com/tomklimovskigamma","imageURL":"http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80","key":"tomklimovski"}],"frontMatter":{"slug":"great-expectations-for-your-data","title":"Great Expectations (for your data...)","authors":["tomklimovski"],"draft":false,"hide_table_of_contents":true,"image":"images/great-expectations.png","tags":["data","data-quality","data-quality-management"],"keywords":["data","data-quality","data-quality-management"]},"prevItem":{"title":"Using the Azure CLI to Create an API using a Function App within API Management","permalink":"/using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management"},"nextItem":{"title":"Multi Cloud Diagramming with PlantUML","permalink":"/multi-cloud-diagramming-with-plantuml"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Great Expectations](images/great-expectations.png)\\r\\n\\r\\nThis article provides an introduction to the Great Expectations Python library for data quality management ([https://github.com/great-expectations/great\\\\_expectations](https://github.com/great-expectations/great_expectations)).\\r\\n\\r\\nSo what are expectations when it comes to data (and data quality)...\\r\\n\\r\\nAn expectation is a falsifiable, verifiable statement about data. Expectations provide a language to talk about data characteristics and data quality - humans to humans, humans to machines and machines to machines.\\r\\n\\r\\nThe **great expectations** project includes predefined, codified expectations such as:\\r\\n\\r\\n```\\r\\nexpect_column_to_exist \\r\\nexpect_table_row_count_to_be_between \\r\\nexpect_column_values_to_be_unique \\r\\nexpect_column_values_to_not_be_null \\r\\nexpect_column_values_to_be_between \\r\\nexpect_column_values_to_match_regex \\r\\nexpect_column_mean_to_be_between \\r\\nexpect_column_kl_divergence_to_be_less_than\\r\\n```\\r\\n\\r\\n\u2026 and many more\\r\\n\\r\\nExpectations are both data tests and docs! Expectations can be presented in a machine-friendly JSON, for example:\\r\\n\\r\\n<Gist id=\\"317fa68cc27e4e364ab238a93f6ed361\\" \\r\\n/>\\r\\n\\r\\nGreat Expectations provides validation results of defined expectations, which can dramatically shorten your development cycle.\\r\\n\\r\\n[![validation results in great expectations](images/validation_failed_unexpected_values.gif)](validation results in great expectations)\\r\\n\\r\\nNearly 50 built in expectations allow you to express how you understand your data, and you can add custom expectations if you need a new one. A machine can test if a dataset conforms to the expectation.\\r\\n\\r\\n## OK, enough talk, let\'s go!\\r\\n\\r\\n```bash\\r\\npyenv virtualenv 3.8.2 ge38\\r\\npip install great-expectations\\r\\n```\\r\\n\\r\\ntried with Python 3.7.2, but had issues with library `lgzm` on my local machine\\r\\n\\r\\nonce installed, run the following in the python repl shell:\\r\\n\\r\\n<Gist id=\\"78211408899d5d0d4b1a088d039fe1d3\\" \\r\\n/>\\r\\n\\r\\nshowing the data in the dataframe should give you the following:\\r\\n\\r\\n<Gist id=\\"fbdeda83bfe9af7ceb33a36a3f4a29e0\\" \\r\\n/>\\r\\n\\r\\nas can be seen, a collection of random integers in each column for our initial testing. Let\'s pipe this data in to great-expectations...\\r\\n\\r\\n<Gist id=\\"0bf00ff1bfad316f83f1b458aa2ad01b\\" \\r\\n/>\\r\\n\\r\\nyields the following output...\\r\\n\\r\\n<Gist id=\\"83264e645d220ebb0f0a529a2a139be9\\" \\r\\n/>\\r\\n\\r\\nthis shows that there are 0 unexpected items in the data we are testing. Great!\\r\\n\\r\\nNow let\'s have a look at a negative test. Since we\'ve picked the values at random, there are bound to be duplicates. Let\'s test that:\\r\\n\\r\\n<Gist id=\\"0a75fe9700677d0329d96da44c54dca5\\" \\r\\n/>\\r\\n\\r\\nyields...\\r\\n\\r\\n<Gist id=\\"0c714a928064124ec15c9733d9e5ff29\\" \\r\\n/>\\r\\n\\r\\nThe JSON schema has metadata information about the result, of note is the result section which is specific to our query, and shows the percentage that failed the expectation.\\r\\n\\r\\nLet\'s progress to something more real-world, namely creating exceptions that are run on databases. Armed with our basic understanding of great-expectations, let\'s...\\r\\n\\r\\n- set up a postgres database\\r\\n- initiate a new Data Context within great-expectations\\r\\n- write test-cases for the data\\r\\n- group those test-cases and\\r\\n- run it\\r\\n\\r\\n## Setting up a Database\\r\\n\\r\\n<Gist id=\\"30dc4a230656f36c4c9b7b208b792329\\" \\r\\n/>\\r\\n\\r\\nif you don\'t have it installed,\\r\\n\\r\\n<Gist id=\\"b127a57c499e63c003ac6c8bb4408768\\" \\r\\n/>\\r\\n\\r\\nwait 15 minutes for download the internet. Verify postgres running with\xa0`docker ps`, then connect with\\r\\n\\r\\n<Gist id=\\"d33c81448fcfddb608f79a69a75202a1\\" \\r\\n/>\\r\\n\\r\\nCreate some data\\r\\n\\r\\n<Gist id=\\"13069c3111039ac60281f291dc1e6bd8\\" \\r\\n/>\\r\\n\\r\\nTake data for a spin\\r\\n\\r\\n<Gist id=\\"f62857c6dbaaaf1d784aa94c5914c9f5\\" \\r\\n/>\\r\\n\\r\\nshould yield\\r\\n\\r\\n<Gist id=\\"52e11ea7319f8344674999ec1b36ab0e\\" \\r\\n/>\\r\\n\\r\\nNow time for\xa0`great-expectations`\\r\\n\\r\\nGreat Expectations relies on the library\xa0`sqlalchemy`\xa0and\xa0`psycopg2`\xa0to connect to your data.\\r\\n\\r\\n<Gist id=\\"6bd28c03b4013f4301b1af2c74dcd947\\" \\r\\n/>\\r\\n\\r\\nonce done, let\'s set up\xa0`great-expectations`\\r\\n\\r\\n<Gist id=\\"4210011daa1fb3c738875f917ebafbf3\\" \\r\\n/>\\r\\n\\r\\nshould look like below:\\r\\n\\r\\n<Gist id=\\"1f1a698cf6a6785598db1133212f30fe\\" \\r\\n/>\\r\\n\\r\\nlet\'s set up a few other goodies while we\'re here\\r\\n\\r\\n<Gist id=\\"51f684d6db8149950f28cc32afa2f461\\" \\r\\n/>\\r\\n\\r\\n**Congratulations! Great Expectations is now set up**\\r\\n\\r\\nYou should see a file structure as follows:\\r\\n\\r\\n[![great expectations tree structure](images/ge_tree_structure.png)](images/ge_tree_structure.png)\\r\\n\\r\\nIf you didn\'t generate a suite during the set up based on `app.order`, you can do so now with\\r\\n\\r\\n`great_expectations suite new`\\r\\n\\r\\nwhen created, looking at\xa0`great_expectations/expectations/app/order/warning.json`\xa0should yield the following:\\r\\n\\r\\n<Gist id=\\"24d105be1cacacbd89b9e4cbac6f4d21\\" \\r\\n/>\\r\\n\\r\\nas noted in the content section, this expectation config is created by the tool by looking at 1000 rows of the data. We also have access to the data-doc site which we can open in the browser at\xa0`great_expectations/uncommitted/data_docs/local_site/index.html`\\r\\n\\r\\n[![great expectations index page](images/index-page.png)](images/index-page.png)\\r\\n\\r\\nClicking on\xa0`app.order.warning`, you\'ll see the sample expectation shown in the UI\\r\\n\\r\\n[![great expectations app order screen](images/ge-app.order-screen.png)](images/ge-app.order-screen.png)\\r\\n\\r\\nNow, let\'s create our own\xa0`expectation`\xa0file and take it for a spin. We\'ll call this one\xa0`error`.\\r\\n\\r\\n[![great expectations new suite](images/ge_suite_new.png)](images/ge_suite_new.png)\\r\\n\\r\\nThis should also start a\xa0`jupyter notebook`. If for some reason you need to start it back up again, you can do so with\\r\\n\\r\\n<Gist id=\\"b4c9fed39b89d939127e4b381c49f274\\" \\r\\n/>\\r\\n\\r\\nGo ahead and hit\xa0`run`\xa0on your first cell.\\r\\n\\r\\n[![Editing a suite with Jupyter](images/jupyter-edit-suite.png)](images/jupyter-edit-suite.png)\\r\\n\\r\\nLet\'s keep it simple and test the\xa0`customer_order_id`\xa0column is in a set with the values below:\\r\\n\\r\\n<Gist id=\\"fea43e4e566795213fc1c5bbcda317ad\\" \\r\\n/>\\r\\n\\r\\nusing the following expectations function in your Table Expectation(s). You may need to click the\xa0`+`\xa0sign in the toolbar to insert a new cell, as below:\\r\\n\\r\\n<Gist id=\\"38be05ccb27c21da2b4213d6a63afd83\\" \\r\\n/>\\r\\n\\r\\n[![Adding table expectation](images/add-table-expectation.png)](images/add-table-expectation.png)\\r\\n\\r\\nAs we can see, appropriate json output that describes the output of our expectation. Go ahead and run the final cell, which will save our work and open a newly minted data documentation UI page, where you\'ll see the expectations you defined in human readable form.\\r\\n\\r\\n[![Saved suite](images/saved-suite.png)](images/saved-suite.png)\\r\\n\\r\\n## Running the test cases\\r\\n\\r\\nIn Great Expectations, running a set of expectations (test cases) is called a\xa0`checkpoint`. Let\'s create a new checkpoint called\xa0`first_checkpoint`\xa0for our\xa0`app.order.error`\xa0expectation as shown below:\\r\\n\\r\\n<Gist id=\\"322483724633e8a4513c5d6ae67298ae\\" \\r\\n/>\\r\\n\\r\\nLet\'s take a look at our checkpoint definition.\\r\\n\\r\\n<Gist id=\\"08594ad6420d46fe60be41d9d949605c\\" \\r\\n/>\\r\\n\\r\\n<Gist id=\\"9cdd4da31f9fdef1246092fa5b65e90c\\" \\r\\n/>\\r\\n\\r\\nAbove you can see the\xa0`validation_operator_name`\xa0which points to a definition in\xa0`great_expectations.yml`, and the\xa0`batches`\xa0where we defined the data source and what expectations to run against.\\r\\n\\r\\nLet\'s have a look at\xa0`great_expectations.yml`. We can see the\xa0`action_list_operator`\xa0defined and all the actions it contains:\\r\\n\\r\\n[![List operators](images/ge_action_list_operator.png)](images/ge_action_list_operator.png)\\r\\n\\r\\nLet\'s run our checkpoint using\\r\\n\\r\\n<Gist id=\\"050501513ee0f5fa13ab522ea7b9242e\\" \\r\\n/>\\r\\n\\r\\n[![Validate checkpoint](images/validate-checkpoint.png)](images/validate-checkpoint.png)\\r\\n\\r\\nOkay cool, we\'ve set up an expectation, a checkpoint and shown a successful status! But what does a failure look like? We can introduce a failure by logging in to postgres and inserting a\xa0`customer_11`\xa0that we\'ll know will fail, as we\'ve specific our expectation that\xa0`customer_id`\xa0should only have two values..\\r\\n\\r\\n<Gist id=\\"5bce4ba20ab8bfae5910db5cd6cc66f4\\" \\r\\n/>\\r\\n\\r\\nHere are the commands to make that happen, as well as the command to re-run our checkpoint:\\r\\n\\r\\n<Gist id=\\"4ad6f1753d65a4f099467bd9fd760067\\" \\r\\n/>\\r\\n\\r\\n<Gist id=\\"fa94ded27b212e099177bee9d6a2cd36\\" \\r\\n/>\\r\\n\\r\\nRun checkpoint again, this time it should fail\\r\\n\\r\\n<Gist id=\\"12fb3e62a5bbcd205ee84ff7445e2657\\" \\r\\n/>\\r\\n\\r\\n[![Failed checkpoint](images/failed-checkpoint.png)](images/failed-checkpoint.png)\\r\\n\\r\\n**As expected, it failed.**\\r\\n\\r\\n## Supported Databases\\r\\n\\r\\nIn it\'s current implementation\xa0`version 0.12.9`, the supported databases our of the box are:\\r\\n\\r\\n<Gist id=\\"4816fb9a621d2d660f746b62ab54ba59\\" \\r\\n/>\\r\\n\\r\\nIt\'s great to be BigQuery supported out of the box, but what about Google Spanner and Google BigTable? Short-answer; currently not supported. See tickets\xa0[https://github.com/googleapis/google-cloud-python/issues/3022](https://github.com/googleapis/google-cloud-python/issues/3022).\\r\\n\\r\\nWith respect to BigTable, it may not be possible as SQLAlchemy can only manage SQL-based RDBSM-type systems, while BigTable (and HBase) are NoSQL non-relational systems.\\r\\n\\r\\n## Scheduling\\r\\n\\r\\nNow that we have seen how to run tests on our data, we can run our checkpoints from bash or a python script(generated using great_expectations checkpoint script first_checkpoint). This lends itself to easy integration with scheduling tools like airflow, cron, prefect, etc.\\r\\n\\r\\n## Production deployment\\r\\n\\r\\nWhen deploying in production, you can store any sensitive information(credentials, validation results, etc) which are part of the uncommitted folder in cloud storage systems or databases or data stores depending on your infratructure setup. Great Expectations has a lot of options\\r\\n\\r\\n## When not to use a data quality framework\\r\\n\\r\\nThis tool is great and provides a lot of advanced data quality validation functions, but it adds another layer of complexity to your infrastructure that you will have to maintain and trouble shoot in case of errors. It would be wise to use it only when needed.\\r\\n\\r\\n## In general\\r\\n\\r\\nDo not use a data quality framework, if simple SQL based tests at post load time works for your use case. Do not use a data quality framework, if you only have a few (usually < 5) simple data pipelines.\\r\\n\\r\\nDo use it when you have data that needs to be tested in an automated and a repeatable fashion. As shown in this article, Great Expectations has a number of options that can be toggled to suit your particular use-case.\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nGreat Expectations shows a lot of promise, and it\'s an active project so expect to see features roll out frequently. It\'s been quite easy to use, but I\'d like to see all it\'s features work in a locked-down enterprise environment.\\r\\n\\r\\nTom Klimovski  \\r\\nPrincipal Consultant, Gamma Data  \\r\\n[tom.klimovski@gammadata.io](mailto:tom.klimovski@gammadata.io)"},{"id":"multi-cloud-diagramming-with-plantuml","metadata":{"permalink":"/multi-cloud-diagramming-with-plantuml","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-10-26-multi-cloud-diagramming-with-plantuml/index.md","source":"@site/blog/2020-10-26-multi-cloud-diagramming-with-plantuml/index.md","title":"Multi Cloud Diagramming with PlantUML","description":"Mulitcloud Diagramming","date":"2020-10-26T00:00:00.000Z","formattedDate":"October 26, 2020","tags":[{"label":"amazonwebservices","permalink":"/tags/amazonwebservices"},{"label":"aws","permalink":"/tags/aws"},{"label":"azure","permalink":"/tags/azure"},{"label":"c4model","permalink":"/tags/c-4-model"},{"label":"diagramming","permalink":"/tags/diagramming"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"microsoft-azure","permalink":"/tags/microsoft-azure"},{"label":"multi-cloud","permalink":"/tags/multi-cloud"},{"label":"plantuml","permalink":"/tags/plantuml"},{"label":"software-architecture","permalink":"/tags/software-architecture"}],"readingTime":0.94,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"multi-cloud-diagramming-with-plantuml","title":"Multi Cloud Diagramming with PlantUML","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/multicloud.png","tags":["amazonwebservices","aws","azure","c4model","diagramming","gcp","google-cloud-platform","googlecloudplatform","microsoft-azure","multi-cloud","plantuml","software-architecture"],"keywords":["amazonwebservices","aws","azure","c4model","diagramming","gcp","google-cloud-platform","googlecloudplatform","microsoft-azure","multi-cloud","plantuml","software-architecture"]},"prevItem":{"title":"Great Expectations (for your data...)","permalink":"/great-expectations-for-your-data"},"nextItem":{"title":"Cloud Bigtable Primer Part II \u2013 Row Key Selection and Schema Design","permalink":"/cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Mulitcloud Diagramming](images/multicloud.png)\\r\\n\\r\\nFollowing on from the recent post [GCP Templates for C4 Diagrams using PlantUML](https://cloudywithachanceofbigdata.com/gcp-templates-for-c4-diagrams-using-plantuml/), cloud architects are often challenged with producing diagrams for architectures spanning multiple cloud providers, particularly as you elevate to enterprise level diagrams.\\r\\n\\r\\nIn this post, with the magic of `!includeurl` we have brought PlantUML template libraries together for AWS, Azure and GCP icon sets, allowing us to produce multi cloud C4 diagrams using PlantUML like this one:\\r\\n\\r\\n[![Multi Cloud Architecture Diagram using PlantUML](images/Example-Multi-Cloud-PlantUML-C4-Diagram.png)](images/Example-Multi-Cloud-PlantUML-C4-Diagram.png)\\r\\n\\r\\nCreating a multi cloud diagram is simple, start by adding the following `include` statements after the `@startuml` label in a new PlantUML C4 diagram:\\r\\n\\r\\n<Gist id=\\"5319b6b041f8b8f54c922a9a5b9b6e7c\\" \\r\\n/>\\r\\n\\r\\nThen add references to the required services from different providers\u2026\\r\\n\\r\\n<Gist id=\\"6ed55cd1b4e3b2e7027f8236af4aa112\\" \\r\\n/>\\r\\n\\r\\nThen include the predefined resources from your different cloud providers in your diagram as shown here (describing a client server application over a cloud to cloud VPN between Azure and GCP)...\\r\\n\\r\\n<Gist id=\\"600aecff7094d7843771770b7048cb2c\\" \\r\\n/>\\r\\n\\r\\nHappy multi-cloud diagramming!\\r\\n\\r\\n> Full source code is available at:\\r\\n> \\r\\n> [https://github.com/gamma-data/plantuml-multi-cloud-diagrams](https://github.com/gamma-data/plantuml-multi-cloud-diagrams)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design","metadata":{"permalink":"/cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-09-13-cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design/index.md","source":"@site/blog/2020-09-13-cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design/index.md","title":"Cloud Bigtable Primer Part II \u2013 Row Key Selection and Schema Design","description":"Cloud BigTable","date":"2020-09-13T00:00:00.000Z","formattedDate":"September 13, 2020","tags":[{"label":"bigtable","permalink":"/tags/bigtable"},{"label":"cloud-bigtable","permalink":"/tags/cloud-bigtable"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"nosql","permalink":"/tags/nosql"}],"readingTime":3.46,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design","title":"Cloud Bigtable Primer Part II \u2013 Row Key Selection and Schema Design","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/cbt-featured-image.png","tags":["bigtable","cloud-bigtable","gcp","google-cloud-platform","googlecloudplatform","nosql"],"keywords":["bigtable","cloud-bigtable","gcp","google-cloud-platform","googlecloudplatform","nosql"]},"prevItem":{"title":"Multi Cloud Diagramming with PlantUML","permalink":"/multi-cloud-diagramming-with-plantuml"},"nextItem":{"title":"GCP Templates for C4 Diagrams using PlantUML","permalink":"/gcp-templates-for-c4-diagrams-using-plantuml"}},"content":"![Cloud BigTable](images/cbt-featured-image.png)\\r\\n\\r\\nThis is a follow up to the original Cloud Bigtable primer where we discussed the basics of Cloud Bigtable:\\r\\n\\r\\n[__Cloud Bigtable Primer - Part I__](https://cloudywithachanceofbigdata.com/cloud-bigtable-primer-part-i/)\\r\\n\\r\\nIn this article we will cover schema design and row key selection in Bigtable \u2013 arguably the most critical design decision to make when employing Bigtable in a cloud data architecture.\\r\\n\\r\\n## Quick Review\\r\\n\\r\\nRecall from the previous post where the Bigtable data model was introduced that tables in Bigtable are comprised of rows and columns - much like a table in any other RDBMS. Every row is uniquely identified by a rowkey \u2013 again akin to a primary key in a table in an RDBMS. But this is where the similarities end.\\r\\n\\r\\nUnlike a table in an RDBMS, columns only ever exist when they are inserted, and `NULLs` are not stored. See the illustration below:\\r\\n\\r\\n[![](images/bigtable-data-model.png)](images/bigtable-data-model.png)\\r\\n\\r\\n## Row Key Selection\\r\\n\\r\\nData in Bigtable is distributed by row keys. Row keys are physically stored in tablets in lexographic order. Recall that row keys are your ONLY indexes to data in Bigtable.\\r\\n\\r\\n### Selection Considerations\\r\\n\\r\\nAs row keys are your only indexes to retrieve or update rows in Bigtable, row key design must take the access patterns for the data to be stored and served via Bigtable into consideration, specifically the following must be considered when designing a Bigtable application:\\r\\n\\r\\n- Search patterns (returning data for a specific entity)\\r\\n- Scan patterns (returning batches of data)\\r\\n\\r\\nQueries that use the row key, a row prefix, or a row range are the most efficient. Queries that do not include a row key will typically scan GB or TB of data and would not be suitable for operational use cases.\\r\\n\\r\\n### Row Key Performance\\r\\n\\r\\nRow key performance will be biased towards your specific access patterns and application functional requirements. For example if you are performing sequential reads or scan operations then sequential keys will perform the best, however their write performance will not be optimal. Conversely, random keys (such as a `uuid`) will perform best for writes but poor for scan or sequential read operations.\\r\\n\\r\\nAdding salts to keys (or additional data), similar to the use of salts in cryptography as well as promoting other field keys to be part of a composite row key can help achieve a \u201cGoldilocks\u201d scenario for both reads and writes, see the diagram below:\\r\\n\\r\\n[![](images/keys.png)](images/keys.png)\\r\\n\\r\\n### Using Reverse Timestamps\\r\\n\\r\\nUse reverse timestamps when your most common query is for the latest values. Typically you would append the reverse timestamp to the key, this will ensure that the same related records are grouped together, for instance if you are storing events for a customer using the customer id along with an appended reverse timestamp (for example `<customer_id>#<reverse_ts>`) would allow you to quickly serve the latest events for a customer in descending order as within each group (`customer_id`), rows will be sorted so most recent insert will be located at the top.  \\r\\nA reverse timestamp can be generalised as:\\r\\n\\r\\n`Long.MAX_VALUE - System.currentTimeMillis()`\\r\\n\\r\\n### Schema Design Tips\\r\\n\\r\\nSome general tips for good schema design using Bigtable are summarised below:\\r\\n\\r\\n- Group related data for more efficient reads using column families\\r\\n- Distribute data evenly for more efficient writes\\r\\n- Place identical values in the adjoining rows for more efficient compression using row keys\\r\\n\\r\\nFollowing these tips will give you the best possible performance using Bigtable.\\r\\n\\r\\n### Use the Key Visualizer to profile performance\\r\\n\\r\\nGoogle provides a neat tool to visualize your row key distribution in Cloud Bigtable. You need to have at least 30 GB of data in your table to enable this feature.\\r\\n\\r\\nThe Key Visualizer is shown here:\\r\\n\\r\\n[![Bigtable Key Visualizer](images/image.png)](images/image.png)\\r\\n\\r\\nThe Key Visualizer will help you find and prevent hotspots, find rows with too much data and show if your key schema is balanced.\\r\\n\\r\\n### Summary\\r\\n\\r\\nBigtable is one of the original and best (massively) distributed NoSQL platforms available. Schema and moreover row key design play a massive part in ensuring low latency and query performance. Go forth and conquer with Cloud Bigtable!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"gcp-templates-for-c4-diagrams-using-plantuml","metadata":{"permalink":"/gcp-templates-for-c4-diagrams-using-plantuml","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-08-14-gcp-templates-for-c4-diagrams-using-plantuml/index.md","source":"@site/blog/2020-08-14-gcp-templates-for-c4-diagrams-using-plantuml/index.md","title":"GCP Templates for C4 Diagrams using PlantUML","description":"GCP C4 Diagramming","date":"2020-08-14T00:00:00.000Z","formattedDate":"August 14, 2020","tags":[{"label":"c4model","permalink":"/tags/c-4-model"},{"label":"diagramming","permalink":"/tags/diagramming"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"plantuml","permalink":"/tags/plantuml"},{"label":"software-architecture","permalink":"/tags/software-architecture"}],"readingTime":1.96,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"gcp-templates-for-c4-diagrams-using-plantuml","title":"GCP Templates for C4 Diagrams using PlantUML","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/gcp-c4.png","tags":["c4model","diagramming","gcp","google-cloud-platform","googlecloudplatform","plantuml","software-architecture"],"keywords":["c4model","diagramming","gcp","google-cloud-platform","googlecloudplatform","plantuml","software-architecture"]},"prevItem":{"title":"Cloud Bigtable Primer Part II \u2013 Row Key Selection and Schema Design","permalink":"/cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design"},"nextItem":{"title":"Cloud Bigtable Primer - Part I","permalink":"/cloud-bigtable-primer-part-i"}},"content":"![GCP C4 Diagramming](images/gcp-c4.png)\\r\\n\\r\\nI am a believer in the mantra of _**\u201cEverything-as-Code\u201d**_, this includes diagrams and other architectural artefacts. Enter PlantUML\u2026\\r\\n\\r\\n## PlantUML\\r\\n\\r\\n[PlantUML](https://plantuml.com/) is an open-source tool which allows users to create UML diagrams from an intuitive DSL (Domain Specific Language). PlantUML is built on top of Graphviz and enables software architects and designers to use code to create Sequence Diagrams, Use Case Diagrams, Class Diagrams, State and Activity Diagrams and much more.\\r\\n\\r\\n## C4 Diagrams\\r\\n\\r\\nPlantUML can be extended to support the [C4 model](https://c4model.com/) for visualising software architecture. Which describes software in different layers including Context, Container, Component and Code diagrams.\\r\\n\\r\\n## GCP Architecture Diagramming using C4\\r\\n\\r\\nPlantUML and C4 can be used to produce cloud architectures, there are official libraries available through PlantUML for Azure and AWS service icons, however these do not exist for GCP yet. There are several open source libraries available, however I have made an attempt to simplify the implementation.\\r\\n\\r\\nThe code below can be used to generate a C4 diagram describing a GCP architecture including official GCP service icons:\\r\\n\\r\\n```\\r\\n@startuml\\r\\n!define GCPPuml https://raw.githubusercontent.com/gamma-data/GCP-C4-PlantUML/master/templates\\r\\n\\r\\n!includeurl GCPPuml/C4\\\\_Context.puml\\r\\n!includeurl GCPPuml/C4\\\\_Component.puml\\r\\n!includeurl GCPPuml/C4\\\\_Container.puml\\r\\n!includeurl GCPPuml/GCPC4Integration.puml\\r\\n!includeurl GCPPuml/GCPCommon.puml\\r\\n\\r\\n!includeurl GCPPuml/Networking/CloudDNS.puml\\r\\n!includeurl GCPPuml/Networking/CloudLoadBalancing.puml\\r\\n!includeurl GCPPuml/Compute/ComputeEngine.puml\\r\\n!includeurl GCPPuml/Storage/CloudStorage.puml\\r\\n!includeurl GCPPuml/Databases/CloudSQL.puml\\r\\n\\r\\ntitle Sample C4 Diagram with GCP Icons\\r\\n\\r\\nPerson(publisher, \\"Publisher\\")\\r\\nSystem\\\\_Ext(device, \\"User\\")\\r\\n\\r\\nBoundary(gcp,\\"gcp-project\\") {\\r\\n  CloudDNS(dns, \\"Managed Zone\\", \\"Cloud DNS\\")\\r\\n  CloudLoadBalancing(lb, \\"L7 Load Balancer\\", \\"Cloud Load Balancing\\")\\r\\n  CloudStorage(bucket, \\"Static Content Bucket\\", \\"Cloud Storage\\")\\r\\n  Boundary(region, \\"gcp-region\\") {\\r\\n    Boundary(zonea, \\"zone a\\") {\\r\\n      ComputeEngine(gcea, \\"Content Server\\", \\"Compute Engine\\")\\r\\n      CloudSQL(csqla, \\"Dynamic Content\\", \\"Cloud SQL\\")\\r\\n    }\\r\\n    Boundary(zoneb, \\"zone b\\") {\\r\\n      ComputeEngine(gceb, \\"Content Server\\", \\"Compute Engine\\")\\r\\n      CloudSQL(csqlb, \\"Dynamic Content\\\\\\\\n(Read Replica)\\", \\"Cloud SQL\\")\\r\\n    }\\r\\n  }\\r\\n}\\r\\n\\r\\nRel(device, dns, \\"resolves name\\")\\r\\nRel(device, lb, \\"makes request\\")\\r\\nRel(lb, gcea, \\"routes request\\")\\r\\nRel(lb, gceb, \\"routes request\\")\\r\\nRel(gcea, bucket, \\"get static content\\")\\r\\nRel(gceb, bucket, \\"get static content\\")\\r\\nRel(gcea, csqla, \\"get dynamic content\\")\\r\\nRel(gceb, csqla, \\"get dynamic content\\")\\r\\nRel(csqla, csqlb, \\"replication\\")\\r\\nRel(publisher,bucket,\\"publish static content\\")\\r\\n\\r\\n@enduml\\r\\n```\\r\\n\\r\\nThe preceding code generates the diagram below:\\r\\n\\r\\n[![](images/Sample-C4-Diagram-with-GCP-Icons.png)](images/Sample-C4-Diagram-with-GCP-Icons.png)\\r\\n\\r\\nAdditional services can be added and used in your diagrams by adding them to your includes, such as:\\r\\n\\r\\n```\\r\\n!includeurl GCPPuml/DataAnalytics/BigQuery.puml\\r\\n!includeurl GCPPuml/DataAnalytics/CloudDataflow.puml\\r\\n!includeurl GCPPuml/AIandMachineLearning/AIHub.puml\\r\\n!includeurl GCPPuml/AIandMachineLearning/CloudAutoML.puml\\r\\n!includeurl GCPPuml/DeveloperTools/CloudBuild.puml\\r\\n!includeurl GCPPuml/HybridandMultiCloud/Stackdriver.puml\\r\\n!includeurl GCPPuml/InternetofThings/CloudIoTCore.puml\\r\\n!includeurl GCPPuml/Migration/TransferAppliance.puml\\r\\n!includeurl GCPPuml/Security/CloudIAM.puml\\r\\n\' and more\u2026\\r\\n```\\r\\n\\r\\n> The complete template library is available at:\\r\\n> \\r\\n> [https://github.com/gamma-data/GCP-C4-PlantUML](https://github.com/gamma-data/GCP-C4-PlantUML)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"cloud-bigtable-primer-part-i","metadata":{"permalink":"/cloud-bigtable-primer-part-i","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-08-04-cloud-bigtable-primer-part-i/index.md","source":"@site/blog/2020-08-04-cloud-bigtable-primer-part-i/index.md","title":"Cloud Bigtable Primer - Part I","description":"Cloud BigTable","date":"2020-08-04T00:00:00.000Z","formattedDate":"August 4, 2020","tags":[{"label":"bigtable","permalink":"/tags/bigtable"},{"label":"cloud-bigtable","permalink":"/tags/cloud-bigtable"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"nosql","permalink":"/tags/nosql"}],"readingTime":6.69,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"cloud-bigtable-primer-part-i","title":"Cloud Bigtable Primer - Part I","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/cbt-featured-image.png","tags":["bigtable","cloud-bigtable","gcp","google-cloud-platform","googlecloudplatform","nosql"],"keywords":["bigtable","cloud-bigtable","gcp","google-cloud-platform","googlecloudplatform","nosql"]},"prevItem":{"title":"GCP Templates for C4 Diagrams using PlantUML","permalink":"/gcp-templates-for-c4-diagrams-using-plantuml"},"nextItem":{"title":"Automated GCS Object Scanning Using DLP with Notifications Using Slack","permalink":"/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack"}},"content":"![Cloud BigTable](images/cbt-featured-image.png)\\r\\n\\r\\nBigtable is one of the foundational services in the Google Cloud Platform and to this day one of the greatest contributions to the big data ecosystem at large. It is also one of the least known services available, with all the headlines and attention going to more widely used services such as BigQuery.\\r\\n\\r\\n## Background\\r\\n\\r\\nIn 2006 (pre Google Cloud Platform), Google released a white paper called _**\u201cBigtable: A Distributed Storage System for Structured Data\u201d**_, this paper set out the reference architecture for what was to become Cloud Bigtable. This followed several other whitepapers including the GoogleFS and MapReduce whitepapers released in 2003 and 2004 which provided abstract reference architectures for the Google File System (now known as **_Colossus_**) and the MapReduce algorithm. These whitepapers inspired a generation of open source distributed processing systems including Hadoop. Google has long had a pattern of publicising a generalized overview of their approach to solving different storage and processing challenges at scale through white papers.\\r\\n\\r\\n[![Bigtable Whitepaper 2006](images/bigtable-whitepaper.png)](assets/bigtable-osdi06.pdf)\\r\\n\\r\\nThe Bigtable white paper inspired a wave of open source distributed key/value oriented NoSQL data stores including Apache HBase and Apache Cassandra.\\r\\n\\r\\n## What is Bigtable?\\r\\n\\r\\nBigtable is a distributed, petabyte scale NoSQL database. More specifically, Bigtable is\u2026\\r\\n\\r\\n### a map\\r\\n\\r\\nAt its core Bigtable is a distributed map or an associative array indexed by a row key, with values in columns which are created only when they are referenced. Each value is an uninterpreted byte array.\\r\\n\\r\\n### sorted\\r\\n\\r\\nRow keys are stored in lexographic order akin to a clustered index in a relational database.\\r\\n\\r\\n### sparse\\r\\n\\r\\nA given row can have any number of columns, not all columns must have values and NULLs are not stored. There may also be gaps between keys.\\r\\n\\r\\n### multi-dimensional\\r\\n\\r\\nAll values are versioned with a timestamp (or configurable integer). Data is not updated in place, it is instead superseded with another version.\\r\\n\\r\\n## When (and when not) to use Bigtable\\r\\n\\r\\n- You need to do many thousands of operations per second on TB+ scale data\\r\\n- Your access patterns are well known and simple\\r\\n- You need to support random write or random read operations (or sequential reads) - each using a row key as the primary identifier\\r\\n\\r\\n### Don\u2019t use Bigtable if\u2026\\r\\n\\r\\n- You need explicit JOIN capability, that is joining one or more tables\\r\\n- You need to do ad-hoc analytics\\r\\n- Your access patterns are unknown or not well defined\\r\\n\\r\\n### Bigtable vs Relational Database Systems\\r\\n\\r\\nThe following table compares and contrasts Bigtable against relational databases (both transaction oriented and analytic oriented databases):\\r\\n\\r\\n&nbsp;| Bigtable | RDBMS (OLTP) | RDBMS (DSS/MPP)\\r\\n--|--|--|-- \\r\\nData Layout | Column Family Oriented | Row Oriented | Column Oriented\\r\\nTransaction Support | Single Row Only | Yes | Depends (but usually no)\\r\\nQuery DSL | get/put/scan/delete | SQL | SQL\\r\\nIndexes | Row Key Only | Yes | Yes (typically PI based)\\r\\nMax Data Size | PB+ | \'00s GB  to TB | TB+\\r\\nRead/Write Throughput | \\"\'000 | 000s queries/s\\" | \'000s queries/s | \'000s queries/s\\r\\n\\r\\n## Bigtable Data Model\\r\\n\\r\\n**_Tables_** in Bigtable are comprised of rows and columns (sounds familiar so far..). Every row is uniquely identified by a **_rowkey_** (like a primary key..again sounds familiar so far).\\r\\n\\r\\n**_Columns_** belong to **_Column Families_** and only exist when inserted, NULLs are not stored - this is where it starts to differ from a traditional RDBMS. The following image demonstrates the data model for a fictitious table in Bigtable.\\r\\n\\r\\n[![Bigtable Data Model](images/bigtable-data-model.png)](images/bigtable-data-model.png)\\r\\n\\r\\nIn the previous example, we created two Column Families (**_cf1_** and **_cf2_**). These are created during table definition or update operations (akin to DDL operations in the relational world). In this case, we have chosen to store primary attributes, like name, etc in cf1 and features (or derived attributes) in cf2 like indicators.\\r\\n\\r\\n### Cell versioning\\r\\n\\r\\nEach cell has a timestamp/version associated with it, multiple versions of a row can exist. Versions are naturally stored in descending order.\\r\\n\\r\\nProperties such as the max age for a cell or the maximum number of versions to be stored for any given cell are set on the Column Family. Versions are compacted through a process called **_Garbage Collection_** - not to be confused with Java Garbage Collection (albeit same idea).\\r\\n\\r\\nRow Key | Column | Value | Timestamp\\r\\n--|--|--|--\\r\\n123 | cf1:status | ACTIVE | 2020-06-30T08.58.27.560\\r\\n123 | cf1:status | PENDING | 2020-06-28T06.20.18.330\\r\\n123 | cf1:status | INACTIVE | 2020-06-27T07.59.20.460\\r\\n\\r\\n## Bigtable Instances, Clusters, Nodes and Tables\\r\\n\\r\\nBigtable is a \\"no-ops\\" service, meaning you do not need to configure machine types or details about the underlying infrastructure, save a few sizing or performance options - such as the number of nodes in a cluster or whether to use solid state hard drives (SSD) or the magnetic alternative (HDD). The following diagram shows the relationships and cardinality for Cloud Bigtable.\\r\\n\\r\\n[![Bigtable Instances, Clusters and Nodes](images/bigtable-instances-and-nodes.png)](images/bigtable-instances-and-nodes.png)\\r\\n\\r\\n**_Clusters_** and **_nodes_** are the physical compute layer for Bigtable, these are zonal assets, zonal and regional availability can be achieved through replication which we will discuss later in this article.\\r\\n\\r\\n**_Instances_** are a virtual abstraction for clusters, Tables belong to instances (not clusters). This is due to Bigtables underlying architecture which is based upon a separation of storage and compute as shown below.\\r\\n\\r\\n[![Bigtable Separation of Storage and Compute](images/bigtable-storage-and-compute.png)](images/bigtable-storage-and-compute.png)\\r\\n\\r\\nBigtables separation of storage and compute allow it to scale horizontally, as nodes are stateless they can be increased to increase query performance. The underlying storage system in inherently scalable.\\r\\n\\r\\n### Physical Storage & Column Families\\r\\n\\r\\nData (Columns) for Bigtable is stored in **_Tablets_** (as shown in the previous diagram), which store \\"regions\\" of row keys for a particular Column Family. Columns consist of a column family prefix and qualifier, for instance:\\r\\n\\r\\n```\\r\\ncf1:col1\\r\\n```\\r\\n\\r\\nA table can have one or more Column Families. Column families must be declared at schema definition time (could be a create or alter operation). A cell is an intersection of a row key and a version of a column within a column family.\\r\\n\\r\\nStorage settings (such as the compaction/garbage collection properties mentioned before) can be specified for each Column Family - which can differ from other column families in the same table.\\r\\n\\r\\n### Bigtable Availability and Replication\\r\\n\\r\\n**_Replication_** is used to increase availability and durability for Cloud Bigtable \u2013 this can also be used to segregate read and write operations for the same table.\\r\\n\\r\\nData and changes to tables are replicated across multiple regions or multiple zones within the same region, this replication can be blocking (single row transactions) or non blocking (eventually consistent). However all clusters within a Bigtable instance are considered primary (writable).\\r\\n\\r\\nRequests are routed using **_Application Profiles_**, a **_single-cluster routing_** policy can be used for manual failover, whereas a **_multi-cluster routing_** is used for automatic failover.\\r\\n\\r\\n### Backup and Export Options for Bigtable\\r\\n\\r\\nManaged backups can be taken at a table level, new tables can be created from backups. The backups cannot be exported, however table level export and import operations are available via pre-baked Dataflow templates for data stored in GCS in the following formats:\\r\\n\\r\\n- SequenceFiles\\r\\n- Avro Files\\r\\n- Parquet Files\\r\\n- CSV Files\\r\\n\\r\\n## Accessing Bigtable\\r\\n\\r\\nBigtable data and admin functions are available via:\\r\\n\\r\\n- `cbt` (optional component of the Google SDK)\\r\\n- `hbase shell` (REPL shell)\\r\\n- Happybase API (Python API for Hbase)\\r\\n- SDK libraries for:\\r\\n    - Golang\\r\\n    - Python\\r\\n    - Java\\r\\n    - Node.js\\r\\n    - Ruby\\r\\n    - C#, C++, PHP, and more\\r\\n\\r\\nAs Bigtable is not a cheap service, there is a local emulator available which is great for application development. This is part of the Cloud SDK, and can be started using the following command:\\r\\n\\r\\n```\\r\\ngcloud beta emulators bigtable start\\r\\n```\\r\\n\\r\\nIn the next article in this series we will demonstrate admin and data functions as well as the local emulator.\\r\\n\\r\\n> Next Up : Part II - Row Key Selection and Schema Design in Bigtable\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"automated-gcs-object-scanning-using-dlp-with-notifications-using-slack","metadata":{"permalink":"/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-06-01-automated-gcs-object-scanning-using-dlp-with-notifications-using-slack/index.md","source":"@site/blog/2020-06-01-automated-gcs-object-scanning-using-dlp-with-notifications-using-slack/index.md","title":"Automated GCS Object Scanning Using DLP with Notifications Using Slack","description":"Slack GCS DLP","date":"2020-06-01T00:00:00.000Z","formattedDate":"June 1, 2020","tags":[{"label":"cloud-dlp","permalink":"/tags/cloud-dlp"},{"label":"cloud-functions","permalink":"/tags/cloud-functions"},{"label":"cloud-storage","permalink":"/tags/cloud-storage"},{"label":"dlp","permalink":"/tags/dlp"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"gcs","permalink":"/tags/gcs"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"python","permalink":"/tags/python"},{"label":"slack","permalink":"/tags/slack"},{"label":"terraform","permalink":"/tags/terraform"}],"readingTime":2.27,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"automated-gcs-object-scanning-using-dlp-with-notifications-using-slack","title":"Automated GCS Object Scanning Using DLP with Notifications Using Slack","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/Slack-GCS-DLP-Image-e1591007165488.png","tags":["cloud-dlp","cloud-functions","cloud-storage","dlp","gcp","gcs","google-cloud-platform","googlecloudplatform","python","slack","terraform"],"keywords":["cloud-dlp","cloud-functions","cloud-storage","dlp","gcp","gcs","google-cloud-platform","googlecloudplatform","python","slack","terraform"]},"prevItem":{"title":"Cloud Bigtable Primer - Part I","permalink":"/cloud-bigtable-primer-part-i"},"nextItem":{"title":"JSON Wrangling with Go","permalink":"/json-wrangling-with-go"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Slack GCS DLP](images/Slack-GCS-DLP-Image-e1591007165488.png)\\r\\n\\r\\nThis is a follow up to a previous blog, [__Google Cloud Storage Object Notifications using Slack__](https://cloudywithachanceofbigdata.com/google-cloud-storage-object-notifications-using-slack/) in which we used Slack to notify us of new objects being uploaded to GCS.\\r\\n\\r\\nIn this article we will take things a step further, where uploading an object to a GCS bucket will trigger a DLP inspection of the object and if any preconfigured info types (such as credit card numbers or API credentials) are present in the object, a Slack notification will be generated.\\r\\n\\r\\nAs DLP scans are \u201cjobs\u201d, meaning they run asynchronously, we will need to trigger scans and inspect results using two separate Cloud Functions (one for triggering a scan [`gcs-dlp-scan-trigger`] and one for inspecting the results of the scan [`gcs-dlp-evaluate-results`]) and a Cloud PubSub topic [`dlp-scan-topic`] which is used to hold the reference to the DLP job.\\r\\n\\r\\nThe process is described using the sequence diagram below:\\r\\n\\r\\n[![](images/dlp-notifications-using-slack.png)](images/dlp-notifications-using-slack.png)\\r\\n\\r\\n## The Code\\r\\n\\r\\nThe `gcs-dlp-scan-trigger` Cloud Function fires when a new object is created in a specified GCS bucket. This function configures the DLP scan to be executed, including the DLP info types (for instance `CREDIT_CARD_NUMBER`, `EMAIL_ADDRESS`, `ETHNIC_GROUP`, `PHONE_NUMBER`, etc) a and likelihood of that info type existing (for instance `LIKELY`). DLP scans determine the probability of an info type occurring in the data, they do not scan every object in its entirety as this would be too expensive.\\r\\n\\r\\nThe primary function executed in the `gcs-dlp-scan-trigger` Cloud Function is named `inspect_gcs_file`. This function configures and submits the DLP job, supplying a PubSub topic to which the DLP Job Name will be written, the code for the `inspect_gcs_file` is shown here:\\r\\n\\r\\n<Gist id=\\"913a4457f43bc7b80e4405dd01f7b64d\\" \\r\\n/>\\r\\n\\r\\nAt this stage the DLP job is created an running asynchronously, the next Cloud Function, `gcs-dlp-evaluate-results`, fires when a message is sent to the PubSub topic defined in the DLP job. The `gcs-dlp-evaluate-results` reads the DLP Job Name from the PubSub topic, connects to the DLP service and queries the job status, when the job is complete, this function checks the results of the scan, if the `min_likliehood` threshold is met for any of the specified info types, a Slack message is generated. The code for the main method in the `gcs-dlp-evaluate-results` function is shown here:\\r\\n\\r\\n<Gist id=\\"ab377f6c3e448ae7c623d057239e05ed\\" \\r\\n/>\\r\\n\\r\\nFinally, a Slack webhook is used to send the message to a specified Slack channel in a workspace, this is done using the `send_slack_notification` function shown here:\\r\\n\\r\\n<Gist id=\\"15d9e7c0922c26b680bed81abfcbadff\\" \\r\\n/>\\r\\n\\r\\nAn example Slack message is shown here:\\r\\n\\r\\n[![Slack Notification for Sensitive Data Detected in a Newly Created GCS Object](images/gcs-dlp-results-slack-notification.png)](images/gcs-dlp-results-slack-notification.png)\\r\\n\\r\\n> Full source code can be found at: [https://github.com/gamma-data/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack](https://github.com/gamma-data/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"json-wrangling-with-go","metadata":{"permalink":"/json-wrangling-with-go","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-04-22-json-wrangling-with-go/index.md","source":"@site/blog/2020-04-22-json-wrangling-with-go/index.md","title":"JSON Wrangling with Go","description":"JSON Golang","date":"2020-04-22T00:00:00.000Z","formattedDate":"April 22, 2020","tags":[{"label":"golang","permalink":"/tags/golang"},{"label":"json","permalink":"/tags/json"}],"readingTime":4.21,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"json-wrangling-with-go","title":"JSON Wrangling with Go","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/json-go.png","tags":["golang","json"],"keywords":["golang","json"]},"prevItem":{"title":"Automated GCS Object Scanning Using DLP with Notifications Using Slack","permalink":"/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack"},"nextItem":{"title":"Forseti Terraform Validator: Enforcing resource policy compliance in your CI pipeline","permalink":"/forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![JSON Golang](images/json-go.png)\\r\\n\\r\\nGolang is a fantastic language but at first glance it is a bit clumsy when it comes to JSON in contrast to other languages such as Python or Javascript. Having said that once you master the concepts involved with JSON wrangling using Go it is equally as functional \u2013 with added type safety and performance.\\r\\n\\r\\nIn this article we will build a program in Golang to parse a JSON file containing a collection held in a named key \u2013 without knowing the structure of this object, we will expose the schema for the object including data types and recurse the object for its values.\\r\\n\\r\\nThis example uses a great Go package called `tablewriter` to render the output of these operations using a table style result set.\\r\\n\\r\\nThe program has `describe` and `select` verbs as operation types; describe shows the column names in the collection and their respective data types, select prints the keys and values as a tabular result set with column headers for the keys and rows containing their corresponding values.\\r\\n\\r\\nStarting with this:\\r\\n\\r\\n<Gist id=\\"cceeb5b667ccfe8a9e20437d3f1dde42\\" \\r\\n/>\\r\\n\\r\\nWe will end up with this when performing a `describe` operation:\\r\\n\\r\\n<Gist id=\\"fbd04c220a70d439df3a14d4a4f48a3e\\" \\r\\n/>\\r\\n\\r\\nAnd this when performing a `select` operation:\\r\\n\\r\\n<Gist id=\\"0b795b13b160cfbcd6796243c0fbb238\\" \\r\\n/>\\r\\n\\r\\nNow let\u2019s talk about how we got there\u2026\\r\\n\\r\\n## The JSON package\\r\\n\\r\\nSupport for JSON in Go is provided using the `encoding/json` package, this needs to be imported in your program of course\u2026 You will also need to import the `reflect` package \u2013 more on this later. `io/ioutil` is required to read the data from a file input, there are other packages included in the program that are removed for brevity:\\r\\n\\r\\n<Gist id=\\"def7e02eac07ded8b80ff807cf023989\\" \\r\\n/>\\r\\n\\r\\n## Reading the data\u2026\\r\\n\\r\\nWe will read the data from the JSON file into a variable called `body`, note that we are not attempting to deserialize the data at this point. This is also a good opportunity to handle any runtime or IO errors that occur here as well.\\r\\n\\r\\n<Gist id=\\"74a2c2c839a30ed8cc66d83d3ddde3b4\\" \\r\\n/>\\r\\n\\r\\n## The interface\u2026\\r\\n\\r\\nWe will declare an empty interface called `data` which will be used to decode the json object (of which the structure is not known), we will also create an abstract interface called `colldata` to hold the contents of the collection contained inside the JSON object that we are specifically looking for:\\r\\n\\r\\n<Gist id=\\"32555f65af4be1fc2504f2d11e15aa19\\" \\r\\n/>\\r\\n\\r\\n## Validating\u2026\\r\\n\\r\\nNext we need to validate that the input is a valid JSON object, we can use the `json.Valid(body)` method to do this:\\r\\n\\r\\n<Gist id=\\"c7afe41fcca4ba1e3ed009044cea76de\\" \\r\\n/>\\r\\n\\r\\n## Unmarshalling\u2026\\r\\n\\r\\nNow the interesting bits, we will deserialize the JSON object to the empty data interface we created earlier using the `json.Unmarshal()` method:\\r\\n\\r\\n<Gist id=\\"2579ec79be915fb89e91ea0977bfbff6\\" \\r\\n/>\\r\\n\\r\\nNote that this operation is another opportunity to catch unexpected errors and handle them accordingly.\\r\\n\\r\\n## Checking the type of the object using reflection\u2026\\r\\n\\r\\nNow that we have serialized the JSON object into the data interface, there are several ways we can inspect the type of the object (which could be a map or an array). One such way is to use reflection. Reflection is the ability of a program to inspect itself at runtime. An example is shown here:\\r\\n\\r\\n<Gist id=\\"1ccd077de0fdee8973e25ac79719cbf5\\" \\r\\n/>\\r\\n\\r\\nThis instruction would produce the following output for our `zones.json` file:\\r\\n\\r\\n<Gist id=\\"04c1b3ae79e969e4be32ef7fa1c07736\\" \\r\\n/>\\r\\n\\r\\n## The type switch\u2026\\r\\n\\r\\nAnother method to decode the type of the data object (and any objects nested as elements or keys within the data object), is to use the type switch, an example of a type switch function is shown here:\\r\\n\\r\\n<Gist id=\\"2e7a3d62ec6f7c71a9c01bfa8d360e4e\\" \\r\\n/>\\r\\n\\r\\n## Finding the nested collection and recursing it\u2026\\r\\n\\r\\nThe aim of the program is to find a collection (an array of maps) nested in a JSON object. The maps with each element of the array are unknown at runtime and are discovered through recursion.\\r\\n\\r\\nIf we are performing a describe operation, we only need to parse the first element of the collection to get the key names and the data type of the values (for which we will use the same `getObjectType` function to perform a type switch.\\r\\n\\r\\nIf we are performing a select operation, we need to parse the first element to get the column names (the keys in the map) and then we need to recurse each element to get the values for each key.\\r\\n\\r\\nIf the element contains a key named id or name, we will place this at the beginning of the resultant record, as maps are unordered by definition.\\r\\n\\r\\n## The output\u2026\\r\\n\\r\\nAs mentioned, we are using the `tablewriter` package to render the output of the collection as a pretty printed table in our terminal. As wrap around can get pretty ugly an additional `maxfieldlen` argument is provided to truncate the values if needed.\\r\\n\\r\\n## In summary\u2026\\r\\n\\r\\nAlthough it is a bit more involved than some other languages, once you get your head around processing JSON in Go, the possibilities are endless!\\r\\n\\r\\n> Full source code can be found at: [https://github.com/gamma-data/json-wrangling-with-golang](https://github.com/gamma-data/json-wrangling-with-golang)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline","metadata":{"permalink":"/forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-04-18-forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline/index.md","source":"@site/blog/2020-04-18-forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline/index.md","title":"Forseti Terraform Validator: Enforcing resource policy compliance in your CI pipeline","description":"Forseti Terraform","date":"2020-04-18T00:00:00.000Z","formattedDate":"April 18, 2020","tags":[{"label":"devops","permalink":"/tags/devops"},{"label":"forseti","permalink":"/tags/forseti"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"policyascode","permalink":"/tags/policyascode"},{"label":"terraform","permalink":"/tags/terraform"}],"readingTime":8.51,"hasTruncateMarker":false,"authors":[{"name":"Daniel Hussey","title":"Cloud Security Engineer","url":"https://www.linkedin.com/in/daniel-hussey/","imageURL":"http://2.gravatar.com/avatar/b0daeaf079c3665ee65a250adba487ee?s=80","key":"danielhussey"}],"frontMatter":{"slug":"forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline","title":"Forseti Terraform Validator: Enforcing resource policy compliance in your CI pipeline","authors":["danielhussey"],"draft":false,"hide_table_of_contents":true,"image":"images/Forseti-Terraform-e1587291818418.png","tags":["devops","forseti","gcp","google-cloud-platform","googlecloudplatform","policyascode","terraform"],"keywords":["devops","forseti","gcp","google-cloud-platform","googlecloudplatform","policyascode","terraform"]},"prevItem":{"title":"JSON Wrangling with Go","permalink":"/json-wrangling-with-go"},"nextItem":{"title":"Creating a Site to Site VPN Connection Between GCP and Azure with Google Private Access","permalink":"/creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access"}},"content":"![Forseti Terraform](images/Forseti-Terraform-e1587291818418.png)\\r\\n\\r\\nTerraform is a powerful tool for managing your Infrastructure as Code. Declare your resources once, define their variables per environment and sleep easy knowing your CI pipeline will take care of the rest.\\r\\n\\r\\nBut\u2026 one night you wake up in a sweat. The details are fuzzy but you were browsing your favourite cloud provider\u2019s console - probably GCP ;) - and thought you saw a bucket had been created outside of your allowed locations! Maybe it even had risky access controls.\\r\\n\\r\\nYou go brush it off and try to fall back to sleep, but you can\u2019t quite push the thought from your mind that somewhere in all that Terraform code, someone _could_ be declaring resources in unapproved locations, and your CICD pipeline would do nothing to stop it. Oh the regulatory implications.\\r\\n\\r\\n## Enter Terraform Validator by Forseti\\r\\n\\r\\nTerraform Validator by Forseti allows you to declare your Policy as Code, check compliance of your Terraform plans against said Policy, and automatically fail violating plans in a CI step. All without setting up servers or agents.\\r\\n\\r\\nYou\u2019re going to learn how to enforce policy on GCP resources like BigQuery, IAM, networks, MySQL, Google Kubernetes Engine (GKE) and more. If you\u2019re particularly crafty, you may be able to go beyond GCP.\\r\\n\\r\\nForseti\u2019s suite of solutions are GCP focused and allow a wide range of live config validation, monitoring and more using the Policy Library we\u2019re going to set up. These additional capabilities require additional infrastructure. But we\u2019re going one step at a time, starting with enforcing policy during deployment.\\r\\n\\r\\n## Getting Started\\r\\n\\r\\nLet\u2019s assume you already have an established CICD pipeline that uses Terraform, or that you are content to validate your Terraform plans locally for now. In that case, we need just two things:\\r\\n\\r\\n1. A Policy Library\\r\\n2. Terraform Validator\\r\\n\\r\\nIt\u2019s that simple! No new servers, agents, firewall rules, extra service accounts or other nonsense. Just add Policy Library, the Validator tool and you can enforce policy on your Terraform deployments.\\r\\n\\r\\nWe\u2019re going to tinker with some existing GCP-focused sample policies (aka Constraints) that Forseti makes available. These samples cover a wide range of resources and use cases, so it is easy to adjust what\u2019s provided to define your own Constraints.\\r\\n\\r\\n## Policy Library\\r\\n\\r\\nFirst let\'s open up some of Forseti\'s pre-defined constraints. We\u2019ll copy them into our own Git repository and adjust to create policies that match our needs. Repeatable and configurable - that\u2019s Policy as Code at work.\\r\\n\\r\\n### Concepts\\r\\n\\r\\nIn the world of Forseti and in particular Terraform Validator, Policies are defined and understood via easy to read YAML files known as Constraints\\r\\n\\r\\nThere is just enough information in a Constraint file for to make clear its purpose and effect, and by tinkering lightly with a pre-written Constraint you can achieve a lot without looking too deeply into the inner workings . But there\u2019s more happening than meets the eye.\\r\\n\\r\\nConstraints are built on Templates - which are like forms with some extra bits waiting to be completed to make a Constraint. Except there\u2019s a lot more hidden away that\u2019s pretty cool if you want to understand it.\\r\\n\\r\\nThink of a Template as a \u2018Class\u2019 in the OOP sense, and of a Constraint as an instantiated Template with all the key attributes defined.\\r\\n\\r\\nE.g. A generic Template for policy on bucket locations and a Constraint to specify which locations are relevant in a given instance. Again, buckets and locations are just the basic example - the potential applications are far greater.\\r\\n\\r\\nNow the real magic is that just like a \u2018Class\u2019, a Template contains logic that makes everything abstracted away in the Constraint possible. Templates contain inline Rego (ray-go), borrowed lovingly by Forseti from the Open Policy Agent (OPA) team.\\r\\n\\r\\nLearn more about Rego and OPA [here](https://www.openpolicyagent.org/docs/latest/policy-language/) to understand the relationship to our Terraform Validator.\\r\\n\\r\\nBut let\u2019s begin.\\r\\n\\r\\n### Set up your Policies\\r\\n\\r\\n#### Create your Policy Library repository\\r\\n\\r\\nCreate your Policy Library repository by cloning [https://github.com/forseti-security/policy-library](https://github.com/forseti-security/policy-library) into your own VCS.\\r\\n\\r\\nThis repo contains templates and sample constraints which will form the basis of your policies. So get it into your Git environment and clone it to local for the next step.\\r\\n\\r\\n#### Customise sample constraints to fit your needs\\r\\n\\r\\nAs discussed in Concepts, Constraints are defined Templates, which make use of Rego policy language. Nice. So let\u2019s take a sample Constraint, put it in our Policy Library and set the values to what we need. It\u2019s that easy - no need to write new templates or learn Rego if your use case is covered.\\r\\n\\r\\nIn a new branch\u2026\\r\\n\\r\\n1. Copy the sample Constraint `storage_location.yaml` to your Constraints folder.  \\r\\n\\r\\n```bash    \\r\\n$ cp policy-library/samples/storage_location.yaml policy-library/policies/constraints/storage_location.yaml\\r\\n```\\r\\n\\r\\n2. Replace the sample location (`asia-southeast1`) in `storage_location.yaml` with `australia-southeast1`.  \\r\\n\\r\\n```yaml    \\r\\n  spec:  \\r\\n    severity: high  \\r\\n    match:  \\r\\n      target: [\\"organization/*\\"]  \\r\\n    parameters:  \\r\\n      mode: \\"allowlist\\"  \\r\\n      locations:  \\r\\n      - australia-southeast1  \\r\\n      exemptions: []\\r\\n```\\r\\n\\r\\n3. Push back to your repo - not Forseti\u2019s!  \\r\\n    \\r\\n```bash\\r\\n$ git push https://github.com/<your-repository>/policy-library.git\\r\\n```    \\r\\n\\r\\n#### Policy review\\r\\n\\r\\nThere you go - you\u2019ve customised a sample Constraint. Now you have your own instance of version controlled Policy-as-Code and are ready to apply the power of OPA\u2019s Rego policy language that lies within the parent Template. Impressively easy right?\\r\\n\\r\\nThat\u2019s a pretty simple example. You can browse the rest of Forseti\u2019s Policy Library to view other sample Constraints, Templates and the Rego logic that makes all of this work. These can be adjusted to cover all kinds of use cases across GCP resources.\\r\\n\\r\\nI suggest working with and editing the [sample Constraints](https://github.com/forseti-security/policy-library/tree/master/samples) before making any changes to Templates.\\r\\n\\r\\nIf you were to write Rego and Templates from scratch, you might even be able to enforce Policy as Code against non-GCP Terraform code.\\r\\n\\r\\n## Terraform Validator\\r\\n\\r\\nNow, let\u2019s set up the Terraform Validator tool and have it compare a sample piece of Terraform code against the Constraint we configured above. Keep in mind you\u2019ll want to translate what\u2019s done here into steps in your CICD pipeline.\\r\\n\\r\\nOnce the tool is in place, we really just run `terraform plan` and feed the output into Terraform Validator. The Validator compares it to our Constraints, runs all the abstracted logic we don\u2019t need to worry about and returns 0 or 2 when done for pass / fail respectively. Easy.\\r\\n\\r\\nSo using Terraform if I try to make a bucket in `australia-southeast1` it should pass, if I try to make one in the US it should fail. Let\u2019s set up the tool, write some basic Terraform and see how we go.\\r\\n\\r\\n### Setup Terraform Validator\\r\\n\\r\\nCheck for the latest version of `terraform-validator` from the official terraform-validator GCS bucket.\\r\\n\\r\\nVery important when using tf version 0.12 or greater. This is the easy way - you can pull from the [Terraform Validator Github](https://github.com/GoogleCloudPlatform/terraform-validator) and make it yourself too.\\r\\n\\r\\n```bash\\r\\n$ gsutil ls -r gs://terraform-validator/releases\\r\\n```\\r\\n\\r\\nCopy the latest version to the working dir\\r\\n\\r\\n```bash\\r\\n$ gsutil cp gs://terraform-validator/releases/2020-03-05/terraform-validator-linux-amd64 .\\r\\n```\\r\\n\\r\\nMake it executable\\r\\n\\r\\n```bash\\r\\n$ chmod 755 terraform-validator-linux-amd64\\r\\n```\\r\\n\\r\\nReady to go!\\r\\n\\r\\n### Review your Terraform code\\r\\n\\r\\nWe\u2019re going to make a ridiculously simple piece of Terraform that tries to create one bucket in our project to keep things simple.\\r\\n\\r\\n```\\r\\n# main.tf\\r\\n\\r\\nresource \\"google_storage_bucket\\" \\"tf-validator-demo-bucket\\" {\xa0\xa0\\r\\n  name\xa0 \xa0 \xa0 \xa0 \xa0 = \\"tf-validator-demo-bucket\\"\\r\\n\xa0\xa0location\xa0 \xa0 \xa0 = \\"US\\"\\r\\n\xa0\xa0force_destroy = true\\r\\n\\r\\n\xa0\xa0lifecycle_rule {\\r\\n\xa0\xa0\xa0\xa0condition {\\r\\n\xa0\xa0\xa0\xa0\xa0\xa0age = \\"3\\"\\r\\n\xa0\xa0\xa0\xa0}\\r\\n\xa0\xa0\xa0\xa0action {\\r\\n\xa0\xa0\xa0\xa0\xa0\xa0type = \\"Delete\\"\\r\\n\xa0\xa0\xa0\xa0}\\r\\n\xa0\xa0}\\r\\n}\\r\\n```\\r\\n\\r\\nThis is a pretty standard bit of Terraform for a GCS bucket, but made very simple with all the values defined directly in `main.tf`. Note the location of the bucket - it violates our Constraint that was set to the `australia-southeast1` region.\\r\\n\\r\\n### Make the Terraform plan\\r\\n\\r\\nWarm up Terraform.  \\r\\nDouble check your Terraform code if there are any hiccups.\\r\\n\\r\\n```bash\\r\\n$ terraform init\\r\\n```\\r\\n\\r\\nMake the Terraform plan and store output to file.\\r\\n\\r\\n```bash\\r\\n$ terraform plan --out=terraform.tfplan\\r\\n```\\r\\n\\r\\nConvert the plan to JSON\\r\\n\\r\\n```bash\\r\\n$ terraform show -json ./terraform.tfplan > ./terraform.tfplan.json\\r\\n```\\r\\n\\r\\n### Validate the non-compliant Terraform plan against your Constraints, for example\\r\\n\\r\\n```bash\\r\\n$ ./terraform-validator-linux-amd64 validate ./tfplan.tfplan.json --policy-path=../repos/policy-library/\\r\\n```\\r\\n\\r\\nTA-DA!\\r\\n\\r\\n```\\r\\nFound Violations:\\r\\n\\r\\nConstraint allow_some_storage_location on resource //storage.googleapis.com/tf-validator-demo-bucket: //storage.googleapis.com/tf-validator-demo-bucket is in a disallowed location.\\r\\n```\\r\\n\\r\\n### Validate the compliant Terraform plan against your Constraints\\r\\n\\r\\nLet\u2019s see what happens if we repeat the above, changing the location of our GCS bucket to `australia-southeast1`.\\r\\n\\r\\n```bash\\r\\n$ ./terraform-validator-linux-amd64 validate ./tfplan.tfplan.json --policy-path=../repos/policy-library/\\r\\n```\\r\\n\\r\\nResults in..\\r\\n\\r\\n```\\r\\nNo violations found.\\r\\n```\\r\\n\\r\\nSuccess!!!\\r\\n\\r\\nNow all that\u2019s left to do for your Policy as Code CICD pipeline is to configure the rest of your Constraints and run this check before you go ahead and `terraform apply`. Be sure to make the `apply` step dependent on the outcome of the Validator.\\r\\n\\r\\n## Wrap Up\\r\\n\\r\\nWe\u2019ve looked at how to apply Policy as Code to validate our Infrastructure as Code. Sounds pretty modern and DevOpsy doesn\u2019t it.\\r\\n\\r\\nTo recap, we learned about Constraints, which are fully defined instances of Policy as Code. They\u2019re based on YAML Templates that refer to the OPA policy language Rego, but we didn\u2019t have to learn it :)\\r\\n\\r\\nWe created our own version controlled Policy Library.\\r\\n\\r\\nUsing the above learning and some handy pre-existing samples, we wrote policies (Constraints) for GCP infrastructure, specifying a whitelist for locations in which GCS buckets could be deployed.\\r\\n\\r\\nAs mentioned there are [dozens upon dozens of samples](https://github.com/forseti-security/policy-library/tree/master/samples) across BigQuery, IAM, networks, MySQL, Google Kubernetes Engine (GKE) and more to work with.\\r\\n\\r\\nOf course, we stored these configured Constraints in our version-controlled Policy Library.\\r\\n\\r\\n- We looked at a simple set of Terraform code to define a GCS bucket, and stored the Terraform plan to a file before applying it.\\r\\n- We ran Forseti\u2019s Terraform Validator against the Terraform plan file, and had the Validator compare the plan to our Policy Library.\\r\\n- We saw that the results matched our expectations! Compliance with the location specified in our Constraint passed the Validator\u2019s checks, and non-compliance triggered a violation.\\r\\n\\r\\nAwesome. And the best part is that all this required no special permissions, no infrastructure for servers or agents and no networking.\\r\\n\\r\\nAll of that comes with the full Forseti suite of Inventory taking Config Validation of already deployed resources. We might get to that next time.\\r\\n\\r\\nReferences:\\r\\n\\r\\n[https://github.com/GoogleCloudPlatform/terraform-validator](https://github.com/GoogleCloudPlatform/terraform-validator) [https://github.com/forseti-security/policy-library](https://github.com/forseti-security/policy-library) [https://www.openpolicyagent.org/docs/latest/policy-language/](https://www.openpolicyagent.org/docs/latest/policy-language/) [https://cloud.google.com/blog/products/identity-security/using-forseti-config-validator-with-terraform-validator](https://cloud.google.com/blog/products/identity-security/using-forseti-config-validator-with-terraform-validator) [https://forsetisecurity.org/docs/latest/concepts/](https://forsetisecurity.org/docs/latest/concepts/)"},{"id":"creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access","metadata":{"permalink":"/creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-03-27-creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access/index.md","source":"@site/blog/2020-03-27-creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access/index.md","title":"Creating a Site to Site VPN Connection Between GCP and Azure with Google Private Access","description":"This article demonstrates creating a site to site IPSEC VPN connection between a GCP VPC network and an Azure Virtual Network, enabling private RFC1918 network connectivity between virtual networks in both clouds. This is done using a single PowerShell script leveraging Azure PowerShell and gcloud commands in the Google SDK.","date":"2020-03-27T00:00:00.000Z","formattedDate":"March 27, 2020","tags":[{"label":"azure","permalink":"/tags/azure"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"hybrid-cloud","permalink":"/tags/hybrid-cloud"},{"label":"microsoft","permalink":"/tags/microsoft"},{"label":"microsoft-azure","permalink":"/tags/microsoft-azure"},{"label":"multi-cloud","permalink":"/tags/multi-cloud"},{"label":"private-networking","permalink":"/tags/private-networking"},{"label":"vpn","permalink":"/tags/vpn"}],"readingTime":11.78,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access","title":"Creating a Site to Site VPN Connection Between GCP and Azure with Google Private Access","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":false,"image":"images/azure_to_gcp_feature_image-1.png","tags":["azure","gcp","google-cloud-platform","googlecloudplatform","hybrid-cloud","microsoft","microsoft-azure","multi-cloud","private-networking","vpn"],"keywords":["azure","gcp","google-cloud-platform","googlecloudplatform","hybrid-cloud","microsoft","microsoft-azure","multi-cloud","private-networking","vpn"]},"prevItem":{"title":"Forseti Terraform Validator: Enforcing resource policy compliance in your CI pipeline","permalink":"/forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline"},"nextItem":{"title":"Spark in the Google Cloud Platform Part 2","permalink":"/spark-in-the-google-cloud-platform-part-2"}},"content":"import ImageWithCaption from \'/js/ImageWithCaption/ImageWithCaption.js\';\\r\\nimport GcpVpnToAzure from \'./images/gcp-vpn-to-azure.png\';\\r\\nimport AzureVpnToGCP from \'./images/azure-vpn-to-gcp.png\';\\r\\n\\r\\nThis article demonstrates creating a site to site IPSEC VPN connection between a GCP VPC network and an Azure Virtual Network, enabling private RFC1918 network connectivity between virtual networks in both clouds. This is done using a single PowerShell script leveraging Azure PowerShell and gcloud commands in the Google SDK.\\r\\n\\r\\nAdditionally, we will use Azure Private DNS to enable private access between Azure hosts and GCP APIs (such as Cloud Storage or Big Query).\\r\\n\\r\\nAn overview of the solution is provided here:\\r\\n\\r\\n[![Azure to GCP VPN Design](images/gcp-to-azure-vpn-design.png)](images/gcp-to-azure-vpn-design.png)\\r\\n\\r\\nOne note before starting - site to site VPN connections between GCP and Azure currently do not support dynamic routing using BGP, however creating some simple routes on either end of the connection will be enough to get going.\\r\\n\\r\\nLet\u2019s go through this step by step:\\r\\n\\r\\n## Step 1 : Authenticate to Azure\\r\\n\\r\\nAzure\u2019s account equivalent is a subscription, the following command from Azure Powershell is used to authenticate a user to one or more subscriptions.\\r\\n\\r\\n```powershell\\r\\nConnect-AzAccount\\r\\n```\\r\\n\\r\\nThis command will open a browser window prompting you for Microsoft credentials, once authenticated you will be returned to the command line.\\r\\n\\r\\n## Step 2 : Create a Resource Group (Azure)\\r\\n\\r\\nA resource group is roughly equivalent to a project in GCP. You will need to supply a Location (equivalent to a GCP region):\\r\\n\\r\\n```powershell\\r\\nNew-AzResourceGroup `\\r\\n  -Name \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\"\\r\\n```\\r\\n\\r\\n## Step 3 : Create a Virtual Network with Subnets and Routes (Azure)\\r\\n\\r\\nAn Azure Virtual Network is the equivalent of a VPC network in GCP (or AWS), you must define subnets before creating a Virtual Network. In this example we will create two subnets, one Gateway subnet (which needs to be named accordingly) where the VPN gateway will reside, and one subnet named \u2018default\u2019 where we will host VMs which will connect to GCP services over the private VPN connection.\\r\\n\\r\\nBefore defining the default subnet we must create and attach a Route Table (equivalent of a Route in GCP), this particular route will be used to route \u2018private\u2019 requests to services in GCP (such as Big Query).\\r\\n\\r\\n```powershell\\r\\n# define route table and route to GCP private access\\r\\n$azroutecfg = New-AzRouteConfig `\\r\\n  -Name \\"google-private\\" `\\r\\n  -AddressPrefix \\"199.36.153.4/30\\" `\\r\\n  -NextHopType \\"VirtualNetworkGateway\\" \\r\\n\\r\\n$azrttbl = New-AzRouteTable `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Name \\"google-private\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -Route $azroutecfg\\r\\n\\r\\n# define gateway subnet\\r\\n$gatewaySubnet = New-AzVirtualNetworkSubnetConfig  `\\r\\n  -Name \\"GatewaySubnet\\" `\\r\\n  -AddressPrefix \\"10.1.2.0/24\\"\\r\\n\\r\\n# define default subnet\\r\\n$defaultSubnet  = New-AzVirtualNetworkSubnetConfig `\\r\\n  -Name \\"default\\" `\\r\\n  -AddressPrefix \\"10.1.1.0/24\\" `\\r\\n  -RouteTable $azrttbl\\r\\n\\r\\n# create virtual network and subnets\\r\\n$vnet = New-AzVirtualNetwork  `\\r\\n  -Name \\"azure-to-gcp-vnet\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -AddressPrefix \\"10.1.0.0/16\\" `\\r\\n  -Subnet $gatewaySubnet,$defaultSubnet\\r\\n```\\r\\n\\r\\n## Step 4 : Create Network Security Groups (Azure)\\r\\n\\r\\nNetwork Security Groups in Azure are stateful firewalls much like Firewall Rules in VPC networks in GCP. Like GCP, the lower priority overrides higher priority rules.\\r\\n\\r\\nIn the example we will create several rules to allow inbound ICMP, TCP and UDP traffic from our Google VPC and RDP traffic from the Internet (which we will use to logon to a VM in Azure to test private connectivity between the two clouds):\\r\\n\\r\\n```powershell\\r\\n# create network security group\\r\\n$rule1 = New-AzNetworkSecurityRuleConfig `\\r\\n  -Name rdp-rule `\\r\\n  -Description \\"Allow RDP\\" `\\r\\n  -Access Allow `\\r\\n  -Protocol Tcp `\\r\\n  -Direction Inbound `\\r\\n  -Priority 100 `\\r\\n  -SourceAddressPrefix Internet `\\r\\n  -SourcePortRange * `\\r\\n  -DestinationAddressPrefix * `\\r\\n  -DestinationPortRange 3389\\r\\n\\r\\n$rule2 = New-AzNetworkSecurityRuleConfig `\\r\\n  -Name icmp-rule `\\r\\n  -Description \\"Allow ICMP\\" `\\r\\n  -Access Allow `\\r\\n  -Protocol Icmp `\\r\\n  -Direction Inbound `\\r\\n  -Priority 101 `\\r\\n  -SourceAddressPrefix * `\\r\\n  -SourcePortRange * `\\r\\n  -DestinationAddressPrefix * `\\r\\n  -DestinationPortRange *\\r\\n\\r\\n$rule3 = New-AzNetworkSecurityRuleConfig `\\r\\n  -Name gcp-rule `\\r\\n  -Description \\"Allow GCP\\" `\\r\\n  -Access Allow `\\r\\n  -Protocol Tcp `\\r\\n  -Direction Inbound `\\r\\n  -Priority 102 `\\r\\n  -SourceAddressPrefix \\"10.2.0.0/16\\" `\\r\\n  -SourcePortRange * `\\r\\n  -DestinationAddressPrefix * `\\r\\n  -DestinationPortRange *\\r\\n\\r\\n$nsg = New-AzNetworkSecurityGroup `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -Name \\"nsg-vm\\" `\\r\\n  -SecurityRules $rule1,$rule2,$rule3\\r\\n```\\r\\n\\r\\n## Step 5 : Create Public IP Addresses (Azure)\\r\\n\\r\\nWe need to create two Public IP Address (equivalent of an External IP in GCP) which will be used for our VPN gateway and for the VM we will create:\\r\\n\\r\\n```powershell\\r\\n# create public IP address for VM\\r\\n$vmpip = New-AzPublicIpAddress `\\r\\n  -Name \\"vm-ip\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -AllocationMethod Dynamic\\r\\n\\r\\n# create public IP address for NW gateway \\r\\n$ngwpip = New-AzPublicIpAddress `\\r\\n  -Name \\"ngw-ip\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -AllocationMethod Dynamic\\r\\n```\\r\\n\\r\\n## Step 6 : Create Virtual Network Gateway (Azure)\\r\\n\\r\\nThe Virtual Network Gateway in Azure is the VPN Gateway equivalent in Azure which will be used to create a VPN tunnel between Azure and a GCP VPN Gateway. This gateway will be placed in the Gateway subnet created previously and one of the Public IP addresses created in the previous step will be assigned to this gateway.\\r\\n\\r\\n```powershell\\r\\n# create virtual network gateway\\r\\n$ngwipconfig = New-AzVirtualNetworkGatewayIpConfig `\\r\\n  -Name \\"ngw-ipconfig\\" `\\r\\n  -SubnetId $gatewaySubnet.Id `\\r\\n  -PublicIpAddressId $ngwpip.Id\\r\\n\\r\\n# use the AsJob switch as this is a long running process\\r\\n$job = New-AzVirtualNetworkGateway -Name \\"vnet-gateway\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -IpConfigurations $ngwipconfig `\\r\\n  -GatewayType \\"Vpn\\" `\\r\\n  -VpnType \\"RouteBased\\" `\\r\\n  -GatewaySku \\"VpnGw1\\" `\\r\\n  -VpnGatewayGeneration \\"Generation1\\" `\\r\\n  -AsJob\\r\\n\\r\\n$vnetgw = Get-AzVirtualNetworkGateway `\\r\\n  -Name \\"vnet-gateway\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\"\\r\\n```\\r\\n\\r\\n## Step 7 : Create a VPC Network and Subnetwork(s) (GCP)\\r\\n\\r\\nA VPC network and subnet need to be created in GCP, the subnet defines the VPC address space. This address space must not overlap with the Azure Virtual Network CIDR. For all GCP steps it is assumed that the project is set for client config (e.g. gcloud config set project __your_project__) so it does not need to be specified for each operation. Private Google access should be enabled on all subnets created.\\r\\n\\r\\n```powershell\\r\\n# creating VPC network and subnets\\r\\ngcloud compute networks create \\"azure-to-gcp-vpc\\" `\\r\\n  --subnet-mode=custom `\\r\\n  --bgp-routing-mode=regional\\r\\n\\r\\ngcloud compute networks subnets create \\"aus-subnet\\" `\\r\\n  --network  \\"azure-to-gcp-vpc\\" `\\r\\n  --range \\"10.2.1.0/24\\" `\\r\\n  --region \\"australia-southeast1\\" `\\r\\n  --enable-private-ip-google-access\\r\\n```\\r\\n\\r\\n## Step 8 : Create an External IP (GCP)\\r\\n\\r\\nAn external IP address will need to be created in GCP which will be used for the external facing interface of the VPN Gateway.\\r\\n\\r\\n```bash\\r\\n# create external IP\\r\\ngcloud compute addresses create \\"ext-gw-ip\\" `\\r\\n  --region \\"australia-southeast1\\"\\r\\n\\r\\n$gcp_ipaddr_obj = gcloud compute addresses describe \\"ext-gw-ip\\" `\\r\\n  --region \\"australia-southeast1\\" `\\r\\n  --format json | ConvertFrom-Json\\r\\n\\r\\n$gcp_ipaddr = $gcp_ipaddr_obj.address\\r\\n```\\r\\n\\r\\n## Step 9 : Create Firewall Rules (GCP)\\r\\n\\r\\nVPC firewall rules will need to be created in GCP to allow VPN traffic as well as SSH traffic from the internet (which allows you to SSH into VM instances using Cloud Shell).\\r\\n\\r\\n```powershell\\r\\n# create VPN firewall rules\\r\\ngcloud compute firewall-rules create \\"vpn-rule1\\" `\\r\\n  --network \\"azure-to-gcp-vpc\\" `\\r\\n  --allow tcp,udp,icmp `\\r\\n  --source-ranges \\"10.1.0.0/16\\"\\r\\n\\r\\ngcloud compute firewall-rules create \\"ssh-rule1\\" `\\r\\n  --network \\"azure-to-gcp-vpc\\" `\\r\\n  --allow tcp:22\\r\\n```\\r\\n\\r\\n## Step 10 : Create VPN Gateway and Forwarding Rules (GCP)\\r\\n\\r\\nCreate a VPN Gateway and Forwarding Rules in GCP which will be used to create a tunnel between GCP and Azure.\\r\\n\\r\\n```powershell\\r\\n# create cloud VPN \\r\\ngcloud compute target-vpn-gateways create \\"vpn-gw\\" `\\r\\n  --network \\"azure-to-gcp-vpc\\" `\\r\\n  --region \\"australia-southeast1\\" `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n\\r\\n# create forwarding rule ESP\\r\\ngcloud compute forwarding-rules create \\"fr-gw-name-esp\\" `\\r\\n  --ip-protocol ESP `\\r\\n  --address \\"ext-gw-ip\\" `\\r\\n  --target-vpn-gateway \\"vpn-gw\\" `\\r\\n  --region \\"australia-southeast1\\" `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n\\r\\n# creating forwarding rule UDP500\\r\\ngcloud compute forwarding-rules create \\"fr-gw-name-udp500\\" `\\r\\n  --ip-protocol UDP `\\r\\n  --ports 500 `\\r\\n  --address \\"ext-gw-ip\\" `\\r\\n  --target-vpn-gateway \\"vpn-gw\\" `\\r\\n  --region \\"australia-southeast1\\" `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n\\r\\n# creating forwarding rule UDP4500\\r\\ngcloud compute forwarding-rules create \\"fr-gw-name-udp4500\\" `\\r\\n  --ip-protocol UDP `\\r\\n  --ports 4500 `\\r\\n  --address \\"ext-gw-ip\\" `\\r\\n  --target-vpn-gateway \\"vpn-gw\\" `\\r\\n  --region \\"australia-southeast1\\" `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n```\\r\\n\\r\\n## Step 10 : Create VPN Tunnel (GCP Side)\\r\\n\\r\\nNow we will create the GCP side of our VPN tunnel using the Public IP Address of the Azure Virtual Network Gateway created in a previous step. As this example uses a route based VPN the traffic selector values need to be set at 0.0.0.0/0. A PSK (Pre Shared Key) needs to be supplied which will be the same key used when we configure a VPN Connection on the Azure side of the tunnel.\\r\\n\\r\\n```powershell\\r\\n# get peer public IP address of Azure gateway\\r\\n$azpubip = Get-AzPublicIpAddress `\\r\\n  -Name \\"ngw-ip\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\"\\r\\n\\r\\n# create VPN tunnel \\r\\ngcloud compute vpn-tunnels create \\"vpn-tunnel-to-azure\\" `\\r\\n  --peer-address $azpubip.IpAddress `\\r\\n  --local-traffic-selector \\"0.0.0.0/0\\" `\\r\\n  --remote-traffic-selector \\"0.0.0.0/0\\" `\\r\\n  --ike-version 2 `\\r\\n  --shared-secret << Pre-Shared Key >> `\\r\\n  --target-vpn-gateway \\"vpn-gw\\" `\\r\\n  --region  \\"australia-southeast1\\" `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n```\\r\\n\\r\\n## Step 11 : Create Static Routes (GCP Side)\\r\\n\\r\\nAs we are using static routing (as opposed to dynamic routing) we will need to define all of the specific routes on the GCP side. We will need to setup routes for both outgoing traffic to the Azure network as well as incoming traffic for the restricted Google API range (199.36.153.4/30).\\r\\n\\r\\n```powershell\\r\\n# create static route (VPN)\\r\\ngcloud compute routes create \\"route-to-azure\\" `\\r\\n  --destination-range \\"10.1.0.0/16\\" `\\r\\n  --next-hop-vpn-tunnel \\"vpn-tunnel-to-azure\\" `\\r\\n  --network \\"azure-to-gcp-vpc\\" `\\r\\n  --next-hop-vpn-tunnel-region \\"australia-southeast1\\" `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n\\r\\n# create static route (Restricted APIs)\\r\\ngcloud compute routes create apis `\\r\\n  --network  \\"azure-to-gcp-vpc\\" `\\r\\n  --destination-range \\"199.36.153.4/30\\" `\\r\\n  --next-hop-gateway default-internet-gateway `\\r\\n  --project \\"azure-to-gcp-project\\"\\r\\n```\\r\\n\\r\\n## Step 12 : Create a Local Gateway (Azure)\\r\\n\\r\\nA Local Gateway in Azure is an object that represents the remote gateway (GCP VPN gateway).\\r\\n\\r\\n```powershell\\r\\n# create local gateway\\r\\n$azlocalgw = New-AzLocalNetworkGateway `\\r\\n  -Name \\"local-gateway\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -GatewayIpAddress $gcp_ipaddr `\\r\\n  -AddressPrefix \\"10.2.0.0/16\\"\\r\\n```\\r\\n\\r\\n## Step 13 : Create a VPN Connection (Azure)\\r\\n\\r\\nNow we can setup the Azure side of the VPN Connection which is accomplished by associating the Azure Virtual Network Gateway with the Local Network Gateway. A PSK (Pre Shared Key) needs to be supplied which is the same key used for the GCP VPN Tunnel in step 10.\\r\\n\\r\\n```powershell\\r\\n# create connection\\r\\n$azvpnconn = New-AzVirtualNetworkGatewayConnection `\\r\\n  -Name \\"vpn-connection\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -VirtualNetworkGateway1 $vnetgw `\\r\\n  -LocalNetworkGateway2 $azlocalgw `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -ConnectionType IPsec `\\r\\n  -SharedKey  << Pre-Shared Key >>  `\\r\\n  -ConnectionProtocol \\"IKEv2\\"\\r\\n```\\r\\n\\r\\nVPN Tunnel Established!\\r\\n\\r\\nAt this stage we have created an end to end connection between the virtual networks in both clouds. You should see this reflected in the respective consoles in each provider.\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={GcpVpnToAzure}\\r\\naltText=\\"GCP VPN Tunnel to a Azure Virtual Network\\"\\r\\n/>\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={AzureVpnToGCP}\\r\\naltText=\\"Azure VPN Connection to a GCP VPC Network\\"\\r\\n/>\\r\\n\\r\\nCongratulations! You have just setup a multi cloud environment using private networking. Now let\u2019s setup Google Private Access for Azure hosts and create VMs on each side to test our setup.\\r\\n\\r\\n## Step 14 : Create a Private DNS Zone for googleapis.com (Azure)\\r\\n\\r\\nWe will now need to create a Private DNS zone in Azure for the googleapis.com domain which will host records to redirect Google API requests to the Restricted API range.\\r\\n\\r\\n```powershell\\r\\n# create private DNS zone\\r\\nNew-AzPrivateDnsZone `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Name \\"googleapis.com\\"\\r\\n\\r\\n# Add A Records   \\r\\n$Records = @()\\r\\n$Records += New-AzPrivateDnsRecordConfig `\\r\\n  -IPv4Address 199.36.153.4\\r\\n$Records += New-AzPrivateDnsRecordConfig `\\r\\n  -IPv4Address 199.36.153.5\\r\\n$Records += New-AzPrivateDnsRecordConfig `\\r\\n  -IPv4Address 199.36.153.6\\r\\n$Records += New-AzPrivateDnsRecordConfig `\\r\\n  -IPv4Address 199.36.153.7\\r\\n\\r\\nNew-AzPrivateDnsRecordSet `\\r\\n  -Name \\"restricted\\" `\\r\\n  -RecordType A `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -TTL 300 `\\r\\n  -ZoneName \\"googleapis.com\\" `\\r\\n  -PrivateDnsRecords $Records\\r\\n\\r\\n# Add CNAME Records   \\r\\n$Records = @()\\r\\n$Records += New-AzPrivateDnsRecordConfig `\\r\\n  -Cname \\"restricted.googleapis.com.\\"\\r\\n\\r\\nNew-AzPrivateDnsRecordSet `\\r\\n  -Name \\"*\\" `\\r\\n  -RecordType CNAME `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -TTL 300 `\\r\\n  -ZoneName \\"googleapis.com\\" `\\r\\n  -PrivateDnsRecords $Records\\r\\n\\r\\n# Create VNet Link\\r\\nNew-AzPrivateDnsVirtualNetworkLink `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -ZoneName \\"googleapis.com\\" `\\r\\n  -Name \\"dns-zone-link\\" `\\r\\n  -VirtualNetworkId $vnet.Id\\r\\n```\\r\\n\\r\\n## Step 15 : Create a Virtual Machine (Azure)\\r\\n\\r\\nWe will create a VM in Azure which we can use to test the VPN tunnel as well as to test Private Google Access over our VPN tunnel.\\r\\n\\r\\n```powershell\\r\\n# create VM\\r\\n$az_vmlocaladminpwd = ConvertTo-SecureString << Password Param >> `\\r\\n  -AsPlainText -Force\\r\\n$Credential = New-Object System.Management.Automation.PSCredential  (\\"LocalAdminUser\\", $az_vmlocaladminpwd);\\r\\n\\r\\n$nic = New-AzNetworkInterface `\\r\\n  -Name \\"vm-nic\\" `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -SubnetId $defaultSubnet.Id `\\r\\n  -NetworkSecurityGroupId $nsg.Id `\\r\\n  -PublicIpAddressId $vmpip.Id `\\r\\n  -EnableAcceleratedNetworking `\\r\\n  -Force\\r\\n\\r\\n$VirtualMachine = New-AzVMConfig `\\r\\n  -VMName \\"windows-desktop\\" `\\r\\n  -VMSize \\"Standard_D4_v3\\"\\r\\n\\r\\n$VirtualMachine = Set-AzVMOperatingSystem `\\r\\n  -VM $VirtualMachine `\\r\\n  -Windows `\\r\\n  -ComputerName  \\"windows-desktop\\" `\\r\\n  -Credential $Credential `\\r\\n  -ProvisionVMAgent `\\r\\n  -EnableAutoUpdate\\r\\n\\r\\n$VirtualMachine = Add-AzVMNetworkInterface `\\r\\n  -VM $VirtualMachine `\\r\\n  -Id $nic.Id\\r\\n\\r\\n$VirtualMachine = Set-AzVMSourceImage `\\r\\n  -VM $VirtualMachine `\\r\\n  -PublisherName \'MicrosoftWindowsDesktop\' `\\r\\n  -Offer \'Windows-10\' `\\r\\n  -Skus \'rs5-pro\' `\\r\\n  -Version latest\\r\\n\\r\\nNew-AzVM `\\r\\n  -ResourceGroupName \\"azure-to-gcp\\" `\\r\\n  -Location \\"Australia Southeast\\" `\\r\\n  -VM $VirtualMachine `\\r\\n  -Verbose\\r\\n```\\r\\n\\r\\n## Step 16 : Create a VM Instance (GCP)\\r\\n\\r\\nWe will create a Linux VM in GCP to test connectivity to hosts in Azure using the VPN tunnel we have established.\\r\\n\\r\\n```powershell\\r\\n# create VM instance\\r\\ngcloud compute instances create \\"gcp-instance\\" `\\r\\n  --zone \\"australia-southeast1-b\\" `\\r\\n  --machine-type \\"f1-micro\\" `\\r\\n  --subnet \\"aus-subnet\\" `\\r\\n  --network-tier PREMIUM `\\r\\n  --maintenance-policy MIGRATE `\\r\\n  --image=debian-9-stretch-v20200309 `\\r\\n  --image-project=debian-cloud `\\r\\n  --boot-disk-size 10GB `\\r\\n  --boot-disk-type pd-standard `\\r\\n  --boot-disk-device-name instance-1 `\\r\\n  --reservation-affinity any\\r\\n```\\r\\n\\r\\n## Test Connectivity\\r\\n\\r\\nNow we are ready to test connectivity from both sides of the tunnel.\\r\\n\\r\\n### Azure to GCP\\r\\n\\r\\nEstablish a remote desktop (RDP) connection to the Azure VM created in Step 15. Ping the GCP VM instance using its private IP address.\\r\\n\\r\\n[![Test Private IP Connectivity from Azure to GCP](images/azure-ping-to-gcp.png)](images/azure-ping-to-gcp.png)\\r\\n\\r\\n### GCP to Azure\\r\\n\\r\\nNow SSH into the GCP Linux VM instance and ping the Azure host using its private IP address.\\r\\n\\r\\n[![Test Private IP Connectivity from GCP to Azure](images/gcp-ping-to-azure.png)](images/gcp-ping-to-azure.png)\\r\\n\\r\\n## Test Private Google Access from Azure\\r\\n\\r\\nNow that we have established bi-directional connectivity between the two clouds, we can test private access to Google APIs from our Azure host. Follow the steps below to test private access:\\r\\n\\r\\n1. RDP into the Azure VM\\r\\n2. Install the Google Cloud SDK ( [https://cloud.google.com/sdk/](https://cloud.google.com/sdk/))\\r\\n3. Perform an `nslookup` to ensure that calls to googleapis.com resolve to the restricted API range (e.g. `nslookup storage.googleapis.com`). You should see a response showing the A records from the googleapis.com Private DNS Zone created in step 14.\\r\\n4. Now test connectivity to Google APIs, for example to test access to Google Cloud Storage using `gsutil`, or test access to Big Query using the `bq` command\\r\\n\\r\\nCongratulations! You are now a multi cloud ninja!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"spark-in-the-google-cloud-platform-part-2","metadata":{"permalink":"/spark-in-the-google-cloud-platform-part-2","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-02-29-spark-in-the-google-cloud-platform-part-2/index.md","source":"@site/blog/2020-02-29-spark-in-the-google-cloud-platform-part-2/index.md","title":"Spark in the Google Cloud Platform Part 2","description":"Apache Spark in GCP","date":"2020-02-29T00:00:00.000Z","formattedDate":"February 29, 2020","tags":[{"label":"apache-spark","permalink":"/tags/apache-spark"},{"label":"cloud-dataproc","permalink":"/tags/cloud-dataproc"},{"label":"dataproc","permalink":"/tags/dataproc"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"gke","permalink":"/tags/gke"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"kubernetes","permalink":"/tags/kubernetes"},{"label":"spark","permalink":"/tags/spark"}],"readingTime":5.11,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"spark-in-the-google-cloud-platform-part-2","title":"Spark in the Google Cloud Platform Part 2","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/spark-gcp-featured-image.png","tags":["apache-spark","cloud-dataproc","dataproc","gcp","gke","google-cloud-platform","googlecloudplatform","kubernetes","spark"],"keywords":["apache-spark","cloud-dataproc","dataproc","gcp","gke","google-cloud-platform","googlecloudplatform","kubernetes","spark"]},"prevItem":{"title":"Creating a Site to Site VPN Connection Between GCP and Azure with Google Private Access","permalink":"/creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access"},"nextItem":{"title":"Spark in the Google Cloud Platform Part 1","permalink":"/spark-in-the-google-cloud-platform-part-1"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Apache Spark in GCP](images/spark-gcp-featured-image.png)\\r\\n\\r\\nIn the previous post in this series [__Spark in the Google Cloud Platform Part 1__](https://cloudywithachanceofbigdata.com/spark-in-the-google-cloud-platform-part-1/), we started to explore the various ways in which we could deploy Apache Spark applications in GCP. The first option we looked at was deploying Spark using Cloud DataProc, a managed Hadoop cluster with various ecosystem components included.\\r\\n\\r\\n:::note Spark Training Courses\\r\\n\\r\\n[Data Transformation and Analysis Using Apache Spark](https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/)  \\r\\n[Stream and Event Processing using Apache Spark](https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/)  \\r\\n[Advanced Analytics Using Apache Spark](https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/)\\r\\n\\r\\n:::\\r\\n\\r\\nIn this post, we will look at another option for deploying Spark in GCP \u2013 _a Spark Standalone cluster running on GKE_.\\r\\n\\r\\nSpark Standalone refers to the in-built cluster manager provided with each Spark release. Standalone can be a bit of a misnomer as it sounds like a single instance \u2013 which it is not, standalone simply refers to the fact that it is not dependent upon any other projects or components \u2013 such as Apache YARN, Mesos, etc.\\r\\n\\r\\nA Spark Standalone cluster consists of a Master node or instance and one of more Worker nodes. The Master node serves as both a master and a cluster manager in the Spark runtime architecture.\\r\\n\\r\\nThe Master process is responsible for marshalling resource requests on behalf of applications and monitoring cluster resources.\\r\\n\\r\\nThe Worker nodes host one or many Executor instances which are responsible for carrying out tasks.\\r\\n\\r\\nDeploying a Spark Standalone cluster on GKE is reasonably straightforward. In the example provided in this post we will set up a private network (VPC), create a GKE cluster, and deploy a Spark Master pod and two Spark Worker pods (in a real scenario you would typically have many Worker pods).\\r\\n\\r\\nOnce the network and GKE cluster have been deployed, the first step is to create Docker images for both the Master and Workers.\\r\\n\\r\\nThe `Dockerfile` below can be used to create an image capable or running either the Worker or Master daemons:\\r\\n\\r\\n<Gist id=\\"a2828409021205b3f6587c824c59928d\\" \\r\\n/>\\r\\n\\r\\nNote the shell scripts included in the `Dockerfile`: `spark-master` and `spark-worker`. These will be used later on by K8S deployments to start the relative Master and Worker daemon processes in each of the pods.\\r\\n\\r\\nNext, we will use Cloud Build to build an image using the `Dockerfile` are store this in GCR (Google Container Registry), from the Cloud Build directory in our project we will run:\\r\\n\\r\\n```\\r\\ngcloud builds submit --tag gcr.io/spark-demo-266309/spark-standalone\\r\\n```\\r\\n\\r\\nNext, we will create Kubernetes deployments for our Master and Worker pods.\\r\\n\\r\\nFirstly, we need to get cluster credentials for our GKE cluster named \u2018spark-cluster\u2019:\\r\\n\\r\\n```\\r\\ngcloud container clusters get-credentials spark-cluster --zone australia-southeast1-a --project spark-demo-266309\\r\\n```\\r\\n\\r\\nNow from within the `k8s-deployments\\\\deploy` folder of our project we will use the `kubectl` command to deploy the Master pod, service and the Worker pods\\r\\n\\r\\nStarting with the Master deployment, this will deploy our Spark Standalone image into a container running the Master daemon process:\\r\\n\\r\\n<Gist id=\\"31bca11627167e0cd963103e4c7f11d2\\" \\r\\n/>\\r\\n\\r\\nTo deploy the Master, run the following:\\r\\n\\r\\n```\\r\\nkubectl create -f spark-master-deployment.yaml\\r\\n```\\r\\n\\r\\nThe Master will expose a web UI on port 8080 and an RPC service on port 7077, we will need to deploy a K8S service for this, the YAML required to do this is shown here:\\r\\n\\r\\n<Gist id=\\"a72d3c38d7a3f94e88c7affd28a3034b\\" \\r\\n/>\\r\\n\\r\\nTo deploy the Master service, run the following:\\r\\n\\r\\n```\\r\\nkubectl create -f spark-master-service.yaml\\r\\n```\\r\\n\\r\\nNow that we have a Master pod and service up and running, we need to deploy our Workers which are preconfigured to communicate with the Master service.\\r\\n\\r\\nThe YAML required to deploy the two Worker pods is shown here:\\r\\n\\r\\n<Gist id=\\"97ceb93ed35959c41d80fb8c025a7ba1\\" \\r\\n/>\\r\\n\\r\\nTo deploy the Worker pods, run the following:\\r\\n\\r\\n```\\r\\nkubectl create -f spark-worker-deployment.yaml\\r\\n```\\r\\n\\r\\nYou can now inspect the Spark processes running on your GKE cluster.\\r\\n\\r\\n```\\r\\nkubectl get deployments\\r\\n```\\r\\n\\r\\nShows...\\r\\n\\r\\n```\\r\\nNAME           READY   UP-TO-DATE   AVAILABLE   AGE\\r\\n spark-master   1/1     1            1           7m45s\\r\\n spark-worker   2/2     2            2           9s\\r\\n```\\r\\n```\\r\\nkubectl get pods\\r\\n```\\r\\n\\r\\nShows...\\r\\n\\r\\n```\\r\\nNAME                            READY   STATUS    RESTARTS   AGE\\r\\n spark-master-f69d7d9bc-7jgmj    1/1     Running   0          8m\\r\\n spark-worker-55965f669c-rm59p   1/1     Running   0          24s\\r\\n spark-worker-55965f669c-wsb2f   1/1     Running   0          24s\\r\\n```\\r\\nNext, as we need to expose the Web UI for the Master process we will create a _LoadBalancer_ resource. The YAML used to do this is provided here:\\r\\n\\r\\n<Gist id=\\"56ee86f50f329f99679ff243bb00fb07\\" \\r\\n/>\\r\\n\\r\\nTo deploy the LB, you would run the following:\\r\\n\\r\\n```\\r\\nkubectl create -f spark-ui-lb.yaml\\r\\n```\\r\\n\\r\\n__NOTE__ This is just an example, for simplicity we are creating an external _LoadBalancer_ with a public IP, this configuration is likely not be appropriate in most real scenarios, alternatives would include an internal _LoadBalancer_, retraction of Authorized Networks, a jump host, SSH tunnelling or IAP.\\r\\n\\r\\nNow you\u2019re up and running!\\r\\n\\r\\nYou can access the Master web UI from the Google Console link shown here:\\r\\n\\r\\n[![Accessing the Spark Master UI from the Google Cloud Console](images/master-ui-link.png)](images/master-ui-link.png)\\r\\n\\r\\nThe Spark Master UI should look like this:\\r\\n\\r\\n[![Spark Master UI](images/spark-master-ui.png)](images/spark-master-ui.png)\\r\\n\\r\\nNext we will exec into a Worker pod, get a shell:\\r\\n\\r\\n```\\r\\nkubectl exec -it spark-worker-55965f669c-rm59p -- sh\\r\\n```\\r\\n\\r\\nNow from within the shell environment of a Worker \u2013 which includes all of the Spark client libraries, we will submit a simple Spark application:\\r\\n\\r\\n```\\r\\nspark-submit --class org.apache.spark.examples.SparkPi \\\\\\r\\n --master spark://10.11.250.98:7077 \\\\\\r\\n/opt/spark/examples/jars/spark-examples*.jar 10000\\r\\n```\\r\\n\\r\\nYou can see the results in the shell, as shown here:\\r\\n\\r\\n[![Spark Pi Estimator Example](images/spark-application-example.png)](images/spark-application-example.png)\\r\\n\\r\\nAdditionally, as all of the container logs go to Stackdriver, you can view the application logs there as well:\\r\\n\\r\\n[![Container Logs in StackDriver](images/container-logs-in-stackdriver.png)](images/container-logs-in-stackdriver.png)\\r\\n\\r\\nThis is a simple way to get a Spark cluster running, it is not without its downsides and shortcomings however, which include the limited security mechanisms available (SASL, network security, shared secrets).\\r\\n\\r\\nIn the final post in this series we will look at Spark on Kubernetes, using Kubernetes as the Spark cluster manager and interacting with Spark using the Kubernetes API and control plane, see you then.\\r\\n\\r\\n> Full source code for this article is available at: [https://github.com/gamma-data/spark-on-gcp](https://github.com/gamma-data/spark-on-gcp)\\r\\n\\r\\nThe infrastructure coding for this example uses Powershell and Terraform, and is deployed as follows:\\r\\n\\r\\n```powershell\\r\\nPS > .\\\\run.ps1 private-network apply <gcp-project> <region>\\r\\nPS > .\\\\run.ps1 gke apply <gcp-project> <region>\\r\\n```\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"spark-in-the-google-cloud-platform-part-1","metadata":{"permalink":"/spark-in-the-google-cloud-platform-part-1","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-02-14-spark-in-the-google-cloud-platform-part-1/index.md","source":"@site/blog/2020-02-14-spark-in-the-google-cloud-platform-part-1/index.md","title":"Spark in the Google Cloud Platform Part 1","description":"Apache Spark in GCP","date":"2020-02-14T00:00:00.000Z","formattedDate":"February 14, 2020","tags":[{"label":"apache-spark","permalink":"/tags/apache-spark"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"spark","permalink":"/tags/spark"}],"readingTime":5.155,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"spark-in-the-google-cloud-platform-part-1","title":"Spark in the Google Cloud Platform Part 1","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/spark-gcp-featured-image.png","tags":["apache-spark","gcp","google-cloud-platform","googlecloudplatform","spark"],"keywords":["apache-spark","gcp","google-cloud-platform","googlecloudplatform","spark"]},"prevItem":{"title":"Spark in the Google Cloud Platform Part 2","permalink":"/spark-in-the-google-cloud-platform-part-2"},"nextItem":{"title":"Query Cloud SQL through Big Query","permalink":"/query-cloud-sql-through-big-query"}},"content":"![Apache Spark in GCP](images/spark-gcp-featured-image.png)\\r\\n\\r\\nI have been an avid Spark enthusiast since 2014 (the early days..). Spark has featured heavily in every project I have been involved with from data warehousing, ETL, feature extraction, advanced analytics to event processing and IoT applications. I like to think of it as a Swiss army knife for distributed processing.\\r\\n\\r\\n:::note Spark Training Courses\\r\\n\\r\\n[Data Transformation and Analysis Using Apache Spark](https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/)  \\r\\n[Stream and Event Processing using Apache Spark](https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/)  \\r\\n[Advanced Analytics Using Apache Spark](https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/)\\r\\n\\r\\n:::\\r\\n\\r\\nCuriously enough, the first project I had been involved with for some years that did not feature the Apache Spark project was a green field GCP project which got me thinking\u2026 where does Spark fit into the GCP landscape?\\r\\n\\r\\nUnlike the other major providers who use Spark as the backbone of their managed distributed ETL services with examples such as AWS Glue or the Spark integration runtime option in Azure Data Factory, Google\u2019s managed ETL solution is Cloud DataFlow. Cloud DataFlow which is a managed Apache Beam service does not use a Spark runtime (there is a Spark Runner however this is not an option when using CDF). So where does this leave Spark?\\r\\n\\r\\nMy summation is that although Spark is not a first-class citizen in GCP (as far as managed ETL), it is not a second-class citizen either. This article will discuss the various ways Spark clusters and applications can be deployed within the GCP ecosystem.\\r\\n\\r\\n## Quick Primer on Spark\\r\\n\\r\\nEvery Spark application contains several components regardless of deployment mode, the components in the Spark runtime architecture are:\\r\\n\\r\\n- the Driver\\r\\n- the Master\\r\\n- the Cluster Manager\\r\\n- the Executor(s), which run on worker nodes or Workers\\r\\n\\r\\nEach component has a specific role in executing a Spark program and all of the Spark components run in Java virtual machines (JVMs).\\r\\n\\r\\n[![Spark Runtime Architecture](images/spark-runtime.png)](images/spark-runtime.png)\\r\\n\\r\\nCluster Managers schedule and manage distributed resources (compute and memory) across the nodes of the cluster. Cluster Managers available for Spark include:\\r\\n\\r\\n- Standalone\\r\\n- YARN (Hadoop)\\r\\n- Mesos\\r\\n- Kubernetes\\r\\n\\r\\n## Spark on DataProc\\r\\n\\r\\nThis is perhaps the simplest and most integrated approach to using Spark in the GCP ecosystem.\\r\\n\\r\\nDataProc is GCP\u2019s managed Hadoop Service (akin to AWS EMR or HDInsight on Azure). DataProc uses Hadoop/YARN as the Cluster Manager. DataProc clusters can be deployed on a private network (VPC using RFC1918 address space), supports encryption at Rest using Google Managed or Customer Managed Keys in KMS, supports autoscaling and the use of Preemptible Workers, and can be deployed in a HA config.\\r\\n\\r\\nFurthermore, DataProc clusters can enforce strong authentication using Kerberos which can be integrated into other directory services such as Active Directory through the use of cross realm trusts.\\r\\n\\r\\n### Deployment\\r\\n\\r\\nDataProc clusters can be deployed using the `gcloud dataproc clusters create` command or using IaC solutions such as Terraform. For this article I have included an example in the source code using the `gcloud` command to deploy a DataProc cluster on a private network which was created using Terraform.\\r\\n\\r\\n### Integration\\r\\n\\r\\nThe beauty of DataProc is its native integration into IAM and the GCP service plane. Having been a long-time user of AWS EMR, I have found that the usability and integration are in many ways superior in GCP DataProc. Let\u2019s look at some examples...\\r\\n\\r\\n#### IAM and IAP (TCP Forwarding)\\r\\n\\r\\nDataProc is integrated into Cloud IAM using various coarse grained permissions use as `dataproc.clusters.use` and simplified IAM Roles such as `dataproc.editor` or `dataproc.admin`. Members with bindings to the these roles can perform tasks such as submitting jobs and creating workflow templates (which we will discuss shortly), as well as accessing instances such as the master node instance or instances in the cluster using IAP (TCP Forwarding) without requiring a public IP address or a bastion host.\\r\\n\\r\\n#### DataProc Jobs and Workflows\\r\\n\\r\\nSpark jobs can be submitted using the console or via `gcloud dataproc jobs submit` as shown here:\\r\\n\\r\\n[![Submitting a Spark Job using gcloud dataproc jobs submit](images/dataproc-spark-job.png)](images/dataproc-spark-job.png)\\r\\n\\r\\nCluster logs are natively available in StackDriver and standard out from the Spark Driver is visible from the console as well as via `gcloud` commands.\\r\\n\\r\\nComplex Workflows can be created by adding Jobs as Steps in Workflow Templates using the following command:\\r\\n\\r\\n```\\r\\ngcloud dataproc workflow-templates add-job spark\\r\\n```\\r\\n\\r\\n#### Optional Components and the Component Gateway\\r\\n\\r\\nDataProc provides you with a Hadoop cluster including YARN and HDFS, a Spark runtine \u2013 which includes Spark SQL and SparkR. DataProc also supports several optional components including Anaconda, Jupyter, Zeppelin, Druid, Presto, and more.\\r\\n\\r\\nWeb interfaces to some of these components as well as the management interfaces such as the Resource Manager UI or the Spark History Server UI can be accessed through the Component Gateway.\\r\\n\\r\\nThis is a Cloud IAM integrated gateway (much like IAP) which can allow access through an authenticated and authorized console session to web UIs in the cluster \u2013 without the need for SSH tunnels, additional firewall rules, bastion hosts, or public IPs. Very cool.\\r\\n\\r\\nLinks to the component UIs as well as built in UIs like the YARN Resource Manager UI are available directly from through the console.\\r\\n\\r\\n#### Jupyter\\r\\n\\r\\nJupyter is a popular notebook application in the data science and analytics communities used for reproducible research. DataProc\u2019s Jupyter component provides a ready-made Spark application vector using PySpark. If you have also installed the Anaconda component you will have access to the full complement of scientific and mathematic Python packages such as Pandas and NumPy which can be used in Jupyter notebooks as well. Using the Component Gateway, Jupyer notebooks can be accessed directly from the Google console as shown here:\\r\\n\\r\\n[![Jupyter Notebooks using DataProc](images/dataproc-jupyter-notebook.png)](images/dataproc-jupyter-notebook.png)\\r\\n\\r\\nFrom this example you can see that I accessed source data from a GCS bucket and used HDFS as local scratch space.\\r\\n\\r\\nFurthermore, notebooks are automagically saved in your integrated Cloud Storage DataProc staging bucket and can be shared amongst analysts or accessed at a later time. These notebooks also persist beyond the lifespan of the cluster.\\r\\n\\r\\nNext up we will look at deploying a Spark Standalone cluster on a GKE cluster, see you then!\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"query-cloud-sql-through-big-query","metadata":{"permalink":"/query-cloud-sql-through-big-query","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-02-08-query-cloud-sql-through-big-query/index.md","source":"@site/blog/2020-02-08-query-cloud-sql-through-big-query/index.md","title":"Query Cloud SQL through Big Query","description":"cloudsql federated queries","date":"2020-02-08T00:00:00.000Z","formattedDate":"February 8, 2020","tags":[{"label":"big-query","permalink":"/tags/big-query"},{"label":"bigquery","permalink":"/tags/bigquery"},{"label":"cloudsql","permalink":"/tags/cloudsql"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"}],"readingTime":3.89,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"query-cloud-sql-through-big-query","title":"Query Cloud SQL through Big Query","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/cloud-sql-federated-queries.png","tags":["big-query","bigquery","cloudsql","gcp","google-cloud-platform","googlecloudplatform"],"keywords":["big-query","bigquery","cloudsql","gcp","google-cloud-platform","googlecloudplatform"]},"prevItem":{"title":"Spark in the Google Cloud Platform Part 1","permalink":"/spark-in-the-google-cloud-platform-part-1"},"nextItem":{"title":"Google Cloud SQL \u2013 Availability for PostgreSQL \u2013 Part II (Read Replicas)","permalink":"/google-cloud-sql-availability-for-postgresql-read-replicas"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![cloudsql federated queries](images/cloud-sql-federated-queries.png)\\r\\n\\r\\nThis article demonstrates Cloud SQL federated queries for Big Query, a neat and simple to use feature.\\r\\n\\r\\n## Connecting to Cloud SQL\\r\\n\\r\\nOne of the challenges presented when using Cloud SQL on a private network (VPC) is providing access to users. There are several ways to accomplish this which include:\\r\\n\\r\\n- open the database port on the VPC Firewall (5432 for example for Postgres) and let users access the database using a command line or locally installed GUI tool _(may not be allowed in your environment)_\\r\\n- provide a web based interface deployed on your VPC such as PGAdmin deployed on a GCE instance or GKE pod _(adds security and management overhead)_\\r\\n- use the Cloud SQL proxy _(requires additional software to be installed and configured)_\\r\\n\\r\\nIn additional, all of the above solutions require direct IP connectivity to the instance which may not always be available. Furthermore each of these operations requires the user to present some form of authentication \u2013 in many cases the database user and password which then must be managed at an individual level.\\r\\n\\r\\nEnter Cloud SQL federated queries for Big Query\u2026\\r\\n\\r\\n## Big Query Federated Queries for Cloud SQL\\r\\n\\r\\nBig Query allows you to query tables and views in Cloud SQL (currently MySQL and Postgres) using the Federated Queries feature. The queries could be authorized views in Big Query datasets for example.\\r\\n\\r\\nThis has the following advantages:\\r\\n\\r\\n- Allows users to authenticate and use the GCP console to query Cloud SQL\\r\\n- Does not require direct IP connectivity to the user or additional routes or firewall rules\\r\\n- Leverages Cloud IAM as the authorization mechanism \u2013 rather that unmanaged db user accounts and object level permissions\\r\\n- External queries can be executed against a read replica of the Cloud SQL instance to offload query IO from the master instance\\r\\n\\r\\n## Setting it up\\r\\n\\r\\nSetting up big query federated queries for Cloud SQL is exceptionally straightforward, a summary of the steps are provided below:\\r\\n\\r\\n### Step 1. Enable a Public IP on the Cloud SQL instance\\r\\n\\r\\nThis sounds bad, but it isn\u2019t really that bad. You need to enable a public interface for Big Query to be able to establish a connection to Cloud SQL, however this is not accessed through the actual public internet \u2013 rather it is accessed through the Google network using the back end of the front end if you will.\\r\\n\\r\\nFurthermore, you configure an empty list of authorized networks which effectively shields the instance from the public network, this can be configured in Terraform as shown here:\\r\\n\\r\\n<Gist id=\\"81c57a80a7e588b98ea7d294dbaee242\\" \\r\\n/>\\r\\n\\r\\nThis configuration change can be made to a running instance as well as during the initial provisioning of the instance.\\r\\n\\r\\nAs shown below you will get a warning dialog in the console saying that you have no authorized networks - this is by design.\\r\\n\\r\\n[![Cloud SQL Public IP Enabled with No Authorized Networks](images/cloud-sql-publicip-screenshot.png)](images/cloud-sql-publicip-screenshot.png)\\r\\n\\r\\n### Step 2. Create a Big Query dataset which will be used to execute the queries to Cloud SQL\\r\\n\\r\\nConnections to Cloud SQL are defined in a Big Query dataset, this can also be use to control access to Cloud SQL using authorized views controlled by IAM roles.\\r\\n\\r\\n<Gist id=\\"8a4beaab134a1c72613347b5822d1724\\" \\r\\n/>\\r\\n\\r\\n### Step 3. Create a connection to Cloud SQL\\r\\n\\r\\nTo create a connection to Cloud SQL from Big Query you must first enable the BigQuery Connection API, this is done at a project level.\\r\\n\\r\\nAs this is a fairly recent feature there isn\'t great coverage with either the **`bq`** tool or any of the Big Query client libraries to do this so we will need to use the console for now...\\r\\n\\r\\nUnder the _**Resources**_ -> **_Add Data_** link in the left hand panel of the Big Query console UI, select **_Create Connection_**. You will see a side info panel with a form to enter connection details for your Cloud SQL instance.\\r\\n\\r\\nIn this example I will setup a connection to a Cloud SQL read replica instance I have created:\\r\\n\\r\\n[![](images/big-query-add-connection.png)](images/big-query-add-connection.png)\\r\\n\\r\\nCreating a Big Query Connection to Cloud SQL\\r\\n\\r\\nMore information on the Big Query Connections API can be found at: [https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest](https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest)\\r\\n\\r\\nThe following permissions are associated with connections in Big Query:\\r\\n\\r\\n```\\r\\nbigquery.connections.create  \\r\\nbigquery.connections.get  \\r\\nbigquery.connections.list  \\r\\nbigquery.connections.use  \\r\\nbigquery.connections.update  \\r\\nbigquery.connections.delete\\r\\n```\\r\\n\\r\\nThese permissions are conveniently combined into the following predefined roles:\\r\\n\\r\\n```\\r\\nroles/bigquery.connectionAdmin    (BigQuery Connection Admin)         \\r\\nroles/bigquery.connectionUser     (BigQuery Connection User)          \\r\\n```\\r\\n\\r\\n### Step 4. Query away!\\r\\n\\r\\nNow the connection to Cloud SQL can be accessed using the **`EXTERNAL_QUERY`** function in Big Query, as shown here:\\r\\n\\r\\n[![Querying Cloud SQL from Big Query](images/cloud-sql-federated-queries-screenshot.png)](images/cloud-sql-federated-queries-screenshot.png)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"google-cloud-sql-availability-for-postgresql-read-replicas","metadata":{"permalink":"/google-cloud-sql-availability-for-postgresql-read-replicas","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-01-24-google-cloud-sql-availability-for-postgresql-read-replicas/index.md","source":"@site/blog/2020-01-24-google-cloud-sql-availability-for-postgresql-read-replicas/index.md","title":"Google Cloud SQL \u2013 Availability for PostgreSQL \u2013 Part II (Read Replicas)","description":"CloudSQL HA","date":"2020-01-24T00:00:00.000Z","formattedDate":"January 24, 2020","tags":[{"label":"cloudsql","permalink":"/tags/cloudsql"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"ha","permalink":"/tags/ha"},{"label":"highavailability","permalink":"/tags/highavailability"},{"label":"postgresql","permalink":"/tags/postgresql"}],"readingTime":4.23,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"google-cloud-sql-availability-for-postgresql-read-replicas","title":"Google Cloud SQL \u2013 Availability for PostgreSQL \u2013 Part II (Read Replicas)","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/cloudsql-featured-image.png","tags":["cloudsql","gcp","google-cloud-platform","googlecloudplatform","ha","highavailability","postgresql"],"keywords":["cloudsql","gcp","google-cloud-platform","googlecloudplatform","ha","highavailability","postgresql"]},"prevItem":{"title":"Query Cloud SQL through Big Query","permalink":"/query-cloud-sql-through-big-query"},"nextItem":{"title":"Introducing Service Mesh Part II","permalink":"/introducing-service-mesh-part-ii"}},"content":"import Gist from \'react-gist\';\\r\\nimport ImageWithCaption from \'/js/ImageWithCaption/ImageWithCaption.js\';\\r\\nimport SetupImage1 from \'./images/cloud-sql-postgres-read-replicas-1.png\';\\r\\nimport SetupImage2 from \'./images/cloud-sql-postgres-read-replicas-2.png\';\\r\\nimport SetupImage3 from \'./images/cloud-sql-postgres-read-replicas-3.png\';\\r\\nimport SetupImage4 from \'./images/cloud-sql-postgres-read-replicas-4.png\';\\r\\nimport SetupImage5 from \'./images/cloud-sql-postgres-read-replicas-5.png\';\\r\\nimport SetupImage6 from \'./images/cloud-sql-postgres-read-replicas-6.png\';\\r\\nimport SetupImage7 from \'./images/cloud-sql-postgres-read-replicas-7.png\';\\r\\nimport SetupImage8 from \'./images/cloud-sql-postgres-read-replicas-8.png\';\\r\\nimport SetupImage9 from \'./images/cloud-sql-postgres-read-replicas-9.png\';\\r\\n\\r\\n![CloudSQL HA](images/cloudsql-featured-image.png)\\r\\n\\r\\nIn this post we will look at read replicas as an additional method to achieve multi zone availability for Cloud SQL, which gives us - in turn - the ability to offload (potentially expensive) IO operations such as user created backups or read operations without adding load to the master instance.\\r\\n\\r\\nIn the previous post in this series we looked at Regional availability for PostgreSQL HA using Cloud SQL:\\r\\n\\r\\n[__Google Cloud SQL \u2013 Availability, Replication, Failover for PostgreSQL \u2013 Part I__](https://cloudywithachanceofbigdata.com/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql-part-i/)\\r\\n\\r\\nRecall that this option was simple to implement and worked relatively seamlessly and transparently with respect to zonal failover.\\r\\n\\r\\nNow let\'s look at read replicas in Cloud SQL as an additional measure for availability.\\r\\n\\r\\n## Deploying Read Replica(s)\\r\\n\\r\\nDeploying read replicas is slightly more involved than simple regional (high) availability, as you will need to define each replica or replicas as a separate Cloud SQL instance which is a slave to the primary instance (the master instance).\\r\\n\\r\\nAn example using Terraform is provided here, starting by creating the master instance:\\r\\n\\r\\n<Gist id=\\"34371a3c7edab140e70208cd7710c25a\\" \\r\\n/>\\r\\n\\r\\nNext you would specify one or more read replicas (typically in a zone other than the zone the master is in):\\r\\n\\r\\n<Gist id=\\"980f2d6461db0613b4090413041b5ec5\\" \\r\\n/>\\r\\n\\r\\nNote that several of the options supplied are omitted when creating a read replica database instance, such as the backup and maintenance options - as these operations cannot be performed on a read replica as we will see later.\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage1}\\r\\naltText=\\"Cloud SQL Instances - showing master and replica\\"\\r\\n/>\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage2}\\r\\naltText=\\"Cloud SQL Master Instance\\"\\r\\n/>\\r\\n\\r\\nVoila! You have just set up a master instance (the primary instance your application and/or users will connect to) along with a read replica in a different zone which will be asynchronously updated as changes occur on the master instance.\\r\\n\\r\\n## Read Replicas in Action\\r\\n\\r\\nNow that we have created a read replica, lets see it in action. After connecting to the read replica (like you would any other instance), attempt to access a table that has **_not_** been created on the master as shown here:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage3}\\r\\naltText=\\"SELECT operation from the replica instance\\"\\r\\n/>\\r\\n\\r\\nNow create the table and insert some data on the **_master_** instance:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage4}\\r\\naltText=\\"Create a table and insert a record on the master instance\\"\\r\\n/>\\r\\n\\r\\nNow try the select operation on the **_replica_** instance:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage5}\\r\\naltText=\\"SELECT operation from the replica instance (after changes have been made on the master)\\"\\r\\n/>\\r\\n\\r\\nIt works!\\r\\n\\r\\n## Some Points to Note about Cloud SQL Read Replicas\\r\\n\\r\\n- Users connect to a read replica as a normal database connection (as shown above)\\r\\n- Google managed backups (using the console or `gcloud sql backups create ..` ) can **_NOT_** be performed against replica instances\\r\\n- Read replicas can be used to offload IO intensive operations from the the master instance - such as user managed backup operations (e.g. `pg_dump`)\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage6}\\r\\naltText=\\"pg_dump operation against a replica instance\\"\\r\\n/>\\r\\n\\r\\n- **BE CAREFUL** Despite their name, read replicas are **NOT** read only, updates can be made which will NOT propagate back to the master instance - you could get yourself in an awful mess if you allow users to perform `INSERT`, `UPDATE`, `DELETE`, `CREATE` or `DROP` operations against replica instances.\\r\\n\\r\\n## Promoting a Read Replica\\r\\n\\r\\nIf required a read replica can be promoted to a standalone Cloud SQL instance, which is another DR option. Keep in mind however as the read replica is updated in an asynchronous manner, promotion of a read replica may result in a loss of data (hopefully not much but a loss nonetheless). Your application RPO will dictate if this is acceptable or not.\\r\\n\\r\\nPromotion of a read replica is reasonably straightforward as demonstrated here using the console:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage7}\\r\\naltText=\\"Promoting a read replica using the console\\"\\r\\n/>\\r\\n\\r\\nYou can also use the following `gcloud` command:\\r\\n\\r\\n gcloud sql instances promote-replica  <replica\\\\_instance\\\\_name>\\r\\n\\r\\nOnce you click on the _Promote Replica_ button you will see the following warning:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage8}\\r\\naltText=\\"\\"\\r\\n/>\\r\\n\\r\\n_Promoting a read replica using the console_\\r\\n\\r\\nThis simply states that once you promote the replica instance your instance will become an independent instance with no further relationship with the master instance. Once accepted and the promotion process is complete, you can see that you now have two independent Cloud SQL instances (as advertised!):\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage9}\\r\\naltText=\\"Promoted Cloud SQL instance\\"\\r\\n/>\\r\\n\\r\\nSome of the options you would normally configure with a master instance would need to be configured on the promoted replica instance - such as high availability, maintenance and scheduled backups - but in the event of a zonal failure you would be back up and running with virtually no data loss!\\r\\n\\r\\n> Full source code for this article is available at: [https://github.com/gamma-data/cloud-sql-postgres-availability-tutorial](https://github.com/gamma-data/cloud-sql-postgres-availability-tutorial)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"introducing-service-mesh-part-ii","metadata":{"permalink":"/introducing-service-mesh-part-ii","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-01-21-introducing-service-mesh-part-ii/index.md","source":"@site/blog/2020-01-21-introducing-service-mesh-part-ii/index.md","title":"Introducing Service Mesh Part II","description":"Service Mesh","date":"2020-01-21T00:00:00.000Z","formattedDate":"January 21, 2020","tags":[{"label":"k8s","permalink":"/tags/k-8-s"},{"label":"kubernetes","permalink":"/tags/kubernetes"},{"label":"service-mesh","permalink":"/tags/service-mesh"},{"label":"servicemesh","permalink":"/tags/servicemesh"}],"readingTime":4.625,"hasTruncateMarker":false,"authors":[{"name":"Tom Klimovski","title":"Senior Cloud Engineer","url":"https://github.com/tomklimovskigamma","imageURL":"http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80","key":"tomklimovski"}],"frontMatter":{"slug":"introducing-service-mesh-part-ii","title":"Introducing Service Mesh Part II","authors":["tomklimovski"],"draft":false,"hide_table_of_contents":true,"image":"images/service-mesh-1.png","tags":["k8s","kubernetes","service-mesh","servicemesh"],"keywords":["k8s","kubernetes","service-mesh","servicemesh"]},"prevItem":{"title":"Google Cloud SQL \u2013 Availability for PostgreSQL \u2013 Part II (Read Replicas)","permalink":"/google-cloud-sql-availability-for-postgresql-read-replicas"},"nextItem":{"title":"Google Cloud SQL \u2013 Availability, Replication, Failover for PostgreSQL \u2013 Part I","permalink":"/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql"}},"content":"![Service Mesh](images/service-mesh-1.png)\\r\\n\\r\\nThis is a follow up to the previous post:\\r\\n\\r\\n[__Sick of hearing about Service Mesh? Here\u2019s what you need to know...__](https://cloudywithachanceofbigdata.com/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know/)\\r\\n\\r\\n## Refresher\\r\\n\\r\\nA refresher on the data plane, and what the userspace proxy can perform:\\r\\n\\r\\n- **Routing:** Given a REST request for `/hello` from the local service instance, where should that request be sent?\\r\\n- **Load Balancing:** Once routing has done its job, to which upstream service instance should the request be sent? With what timeout? If the request fails, should it be retried?\\r\\n- **Authorisation and Authentication:** For requests that are incoming, can cryptographic functions determine the authenticity of that requests? Is the called allowed to invoke the requested endpoint?\\r\\n- **Observability:** Detailed logging, statistics, distributed tracing data so that operators can understand the traffic flow and debug problems as they occur\\r\\n- **Service Discovery:** What backend/upstream service instances are available?\\r\\n- **Health Checking:** Are upstream service instances healthy and ready to accept traffic?\\r\\n\\r\\nThe control plane is slightly less complex. For the data plane to act in a coordinated fashion, the control plane gives you the machinery to make that happen. This is the magical part of the service mesh; the control plane takes a set of isolated sidecar proxies and turns them into a distributed system. The control plane in turn provides an API to allow the user to modify and inspect the behaviour of the data plane.\\r\\n\\r\\nYou can see from the diagram below the proxies are right next to the service in the same node. We usually call those \'sidecar\' containers.\\r\\n\\r\\n[![](images/control-data-plane.png)](images/control-data-plane.png)\\r\\n\\r\\nThe diagram above gives you a high level indication of what the service mesh would look like. What if I don\'t have many services? Then the service mesh probably isn\'t for you. That\'s a whole lot of machinery to run a single proxy! Having said this, if your solution is running hundreds or thousands of services, then you\'re going to require a whole heap of proxies.\\r\\n\\r\\nSo there you have it. The service mesh with its control and data plane. To put it simply, the goal of the control plane is to monitor and set a policy that will eventually be enacted by the data plane.\\r\\n\\r\\n## Why?\\r\\n\\r\\nYou\'ve taken over a project, and the security team have mandated the use of the service mesh. You\'ve never used it yourself before, and the confusion as to why we need another thing is getting you down. An additional thing next to my container that will add latency? And consume resources? And I have to maintain it?! Why would anyone need or want this?\\r\\n\\r\\nWhile there are a few answers to this, the most important answer is something I alluded to in an example in part 1 of this series: this design is a great way to add additional logic into the system. Not only can you add additional logic (to containers possibly outside of your control) but you can do this uniformly across the entire mesh! _The service mesh gives you features that are critical for running software that\'s uniform across your whole stack_\\r\\n\\r\\nThe set of features that the service mesh can provide include reliability features (Retries, timeouts etc), observability features (latencies, volume etc) and security features (mTLS, access control etc).\\r\\n\\r\\n## Let\'s break it down\\r\\n\\r\\n**Server-side software relies on these critical features** If you\'re building any type of modern server-side software that\'s predicated on multiple services, think API\'s and web-apps, and if you\'re continually adding features to this in a short timeframe, then all the features listed above become critical for you. Your applications must be reliable, observable and most importantly secure. This is exactly what the service mesh helps you with.\\r\\n\\r\\n**One view to rule them all** The features mentioned above are language-agnostic, don\'t care about your framework, who wrote it or any part of your development life cycle. They give you, your team and your company a consistent way to deploy changes across your service landscape\\r\\n\\r\\n**Decoupled from application code** It\'s important to have a single place to include application and business logic, and not have the nightmare of managing that in multiple components of your system. The core stewardship of the functionality that the service mesh provides lies at the _platform level_. This includes maintenance, deployments, operation etc. The application can be updated and deployed by developers maintaining the application, and the service mesh can change without the application being involved.\\r\\n\\r\\n## In short\\r\\n\\r\\nYes, while the features of the service mesh could be implemented as application code, this solution would not help in driving uniform features sets across the whole system, which is the value proposition for the service mesh.\\r\\n\\r\\n_If you\'re a business-logic developer_, you probably don\'t need to worry about the service mesh. Keep pumping out that new fangled business logic that makes the software oh-so-usable\\r\\n\\r\\n_If you\'re in a platform role_ and most likely using _Kubernetes_, then you should be right on top of the service mesh! That is unless your architecture dictates a monolith. You\'re going to have a lot of services talking to one another, all tied together with an overarching dependency.\\r\\n\\r\\n_If you\'re in a platform role with no Kubernetes_ but a bunch of microservices, you should maybe care a little bit about the service mesh, but without the power of Kubernetes and the ease of deployment it brings, you\'ll have to weigh up how you intend to manage all those proxies.\\r\\n\\r\\nI hope you enjoyed this article, please feel free to reach out to me at:\\r\\n\\r\\nTom Klimovski  \\r\\nPrincipal Consultant, Gamma Data  \\r\\n[tom.klimovski@gammadata.io](mailto:tom.klimovski@gammadata.io)"},{"id":"google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql","metadata":{"permalink":"/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-01-17-google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql/index.md","source":"@site/blog/2020-01-17-google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql/index.md","title":"Google Cloud SQL \u2013 Availability, Replication, Failover for PostgreSQL \u2013 Part I","description":"CloudSQL HA","date":"2020-01-17T00:00:00.000Z","formattedDate":"January 17, 2020","tags":[{"label":"cloudsql","permalink":"/tags/cloudsql"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"ha","permalink":"/tags/ha"},{"label":"highavailability","permalink":"/tags/highavailability"},{"label":"postgresql","permalink":"/tags/postgresql"}],"readingTime":4.5,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql","title":"Google Cloud SQL \u2013 Availability, Replication, Failover for PostgreSQL \u2013 Part I","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/cloudsql-featured-image.png","tags":["cloudsql","gcp","google-cloud-platform","googlecloudplatform","ha","highavailability","postgresql"],"keywords":["cloudsql","gcp","google-cloud-platform","googlecloudplatform","ha","highavailability","postgresql"]},"prevItem":{"title":"Introducing Service Mesh Part II","permalink":"/introducing-service-mesh-part-ii"},"nextItem":{"title":"Sick of hearing about Service Mesh? Here\u2019s what you need to know...","permalink":"/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know"}},"content":"![CloudSQL HA](images/cloudsql-featured-image.png)\\r\\n\\r\\nIn this multi part blog we will explore the features available in Google Cloud SQL for High Availability, Backup and Recovery, Replication and Failover and Security (at rest and in transit) for the PostgreSQL DBMS engine. Some of these features are relatively hot of the press and in Beta \u2013 which still makes them available for general use.\\r\\n\\r\\nWe will start by looking at the High Availability (HA) options available to you when using the PostgreSQL engine in Google Cloud SQL.\\r\\n\\r\\nMost of you would be familiar with the concepts of High Availability, Redundancy, Fault Tolerance, etc but let\u2019s start with a definition of HA anyway:\\r\\n\\r\\n> High availability (HA) is a characteristic of a system, which aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.\\r\\n> \\r\\n> Wikipedia\\r\\n\\r\\nHigher than a normal period is quite subjective, typically this is quantified by a percentage represented by a number of \u201c9s\u201d, that is 99.99% (which would be quoted as \u201cfour nines\u201d), this would allot you 52.60 minutes of downtime over a one-year period.\\r\\n\\r\\nEssentially the number of 9\u2019s required will drive your bias towards the options available to you for Cloud SQL HA.\\r\\n\\r\\nWe will start with Cloud SQL HA in its simplest form, Regional Availability.\\r\\n\\r\\n## Regional Availability\\r\\n\\r\\nKnowing what we know about the Google Cloud Platform, regional availability means that our application or service (in this case Cloud SQL) should be resilient to a failure of any one zone in our region. In fact, as all GCP regions have at least 3 zones \u2013 two zones could fail, and our application would still be available.\\r\\n\\r\\nRegional availability for Cloud SQL (which Google refers to as High Availability), creates a standby instance in addition to the primary instance and uses a regional Persistent Disk resource to store the database instance data, transaction log and other state files, which is synchronously replicated to a Persistent Disk resource local to the zones that the primary and standby instances are located in.\\r\\n\\r\\nA shared IP address (like a Virtual IP) is used to serve traffic to the healthy (normally primary) Cloud SQL instance.\\r\\n\\r\\nAn overview of Cloud SQL HA is shown here:\\r\\n\\r\\n[![Cloud SQL High Availability](images/cloud-sql-ha.png)](images/cloud-sql-ha.png)\\r\\n\\r\\n## Implementing High Availability for Cloud SQL\\r\\n\\r\\nImplementing Regional Availability for Cloud SQL is dead simple, it is one argument:\\r\\n\\r\\n```\\r\\navailability_type = \\"REGIONAL\\"\\r\\n```\\r\\n\\r\\nUsing the `gcloud` command line utility, this would be:\\r\\n\\r\\n```\\r\\ngcloud sql instances create postgresql-instance-1234 \\\\\\r\\n  --availability-type=REGIONAL \\\\\\r\\n  --database-version= POSTGRES_9_6\\r\\n```\\r\\n\\r\\nUsing Terraform (with a complete set of options) it would look like:\\r\\n\\r\\n```\\r\\nresource \\"google_sql_database_instance\\" \\"postgres_ha\\" {\\r\\n  provider = google-beta\\r\\n  region = var.region\\r\\n  project = var.project\\r\\n  name = \\"postgresql-instance-${random_id.instance_suffix.hex}\\"\\r\\n  database_version = \\"POSTGRES_9_6\\"\\r\\n  settings {\\r\\n   tier = var.tier\\r\\n   disk_size = var.disk_size\\r\\n   activation_policy = \\"ALWAYS\\"\\r\\n   disk_autoresize = true\\r\\n   disk_type = \\"PD_SSD\\"\\r\\n   **availability_type = \\"REGIONAL\\"**\\r\\n   backup_configuration {\\r\\n     enabled = true\\r\\n     start_time = \\"00:00\\"\\r\\n   }\\r\\n   ip_configuration  {\\r\\n     ipv4_enabled = false\\r\\n     private_network = google_compute_network.private_network.self_link\\r\\n   }\\r\\n   maintenance_window  {\\r\\n     day = 7\\r\\n     hour = 0\\r\\n     update_track = \\"stable\\"\\r\\n   }\\r\\n  }\\r\\n } \\r\\n```\\r\\n\\r\\nOnce deployed you will notice a few different items in the console, first from the instance overview page you can see that the High Availability option is `ENABLED` for your instance.\\r\\n\\r\\n[![](images/cloud-sql-ha-1.png)](images/cloud-sql-ha-1.png)\\r\\n\\r\\nSecond, you will see a Failover button enabled on the detailed management view for this instance.\\r\\n\\r\\n[![](images/cloud-sql-ha-2.png)](images/cloud-sql-ha-2.png)\\r\\n\\r\\n## Failover\\r\\n\\r\\nFailovers and failbacks can be initiated manually or automatically (should the primary be unresponsive). A manual failover can be invoked by executing the command:\\r\\n\\r\\n```\\r\\ngcloud sql instances failover postgresql-instance-1234\\r\\n```\\r\\n\\r\\nThere is an `--async` option which will return immediately, invoking the failover operation asynchronously.\\r\\n\\r\\nFailover can also be invoked from the Cloud Console using the Failover button shown previously. As an example I have created a connection to a regionally available Cloud SQL instance and started a command which runs a loop and prints out a counter:\\r\\n\\r\\n[![](images/cloud-sql-ha-3.png)](images/cloud-sql-ha-3.png)\\r\\n\\r\\nNow using the `gcloud` command shown earlier, I have invoked a manual failover of the Cloud SQL instance.\\r\\n\\r\\nOnce the failover is initiated, the client connection is dropped (as the server is momentarily unavailable):\\r\\n\\r\\n[![](images/cloud-sql-ha-4.png)](images/cloud-sql-ha-4.png)\\r\\n\\r\\nThe connection can be immediately re-established afterwards, the state of the running query is lost - **_importantly no data is lost_** however. If your application clients had retry logic in their code and they weren\'t executing a long running query, chances are no one would notice! Once reconnecting normal database activities can be resumed:\\r\\n\\r\\n[![](images/cloud-sql-ha-5.png)](images/cloud-sql-ha-5.png)\\r\\n\\r\\nA quick check of the instance logs will show that the failover event has occured:\\r\\n\\r\\n[![](images/cloud-sql-ha-6.png)](images/cloud-sql-ha-6.png)\\r\\n\\r\\nNow when you return to the instance page in the console you will see a Failback button, which indicates that your instance is being served by the standby:\\r\\n\\r\\n[![](images/cloud-sql-ha-7.png)](images/cloud-sql-ha-7.png)\\r\\n\\r\\nNote that there may be a slight delay in the availability of this option as the replica is still being synched.\\r\\n\\r\\nIt is worth noting that nothing comes for free! When you run in REGIONAL or High Availability mode - you are effectively paying double the costs as compared to running in ZONAL mode. However availability and cost have always been trade-offs against one another - you get what you pay for...\\r\\n\\r\\n> More information can be found at: [https://cloud.google.com/sql/docs/postgres/high-availability](https://cloud.google.com/sql/docs/postgres/high-availability)\\r\\n\\r\\nNext up we will look at read replicas (and their ability to be promoted) as another high availability alternative in Cloud SQL.\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"sick-of-hearing-about-service-mesh-heres-what-you-need-to-know","metadata":{"permalink":"/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2020-01-09-sick-of-hearing-about-service-mesh-heres-what-you-need-to-know/index.md","source":"@site/blog/2020-01-09-sick-of-hearing-about-service-mesh-heres-what-you-need-to-know/index.md","title":"Sick of hearing about Service Mesh? Here\u2019s what you need to know...","description":"Service Mesh","date":"2020-01-09T00:00:00.000Z","formattedDate":"January 9, 2020","tags":[{"label":"k8s","permalink":"/tags/k-8-s"},{"label":"kubernetes","permalink":"/tags/kubernetes"},{"label":"service-mesh","permalink":"/tags/service-mesh"},{"label":"servicemesh","permalink":"/tags/servicemesh"}],"readingTime":3.38,"hasTruncateMarker":false,"authors":[{"name":"Tom Klimovski","title":"Senior Cloud Engineer","url":"https://github.com/tomklimovskigamma","imageURL":"http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80","key":"tomklimovski"}],"frontMatter":{"slug":"sick-of-hearing-about-service-mesh-heres-what-you-need-to-know","title":"Sick of hearing about Service Mesh? Here\u2019s what you need to know...","authors":["tomklimovski"],"draft":false,"hide_table_of_contents":true,"image":"images/service-mesh-1.png","tags":["k8s","kubernetes","service-mesh","servicemesh"],"keywords":["k8s","kubernetes","service-mesh","servicemesh"]},"prevItem":{"title":"Google Cloud SQL \u2013 Availability, Replication, Failover for PostgreSQL \u2013 Part I","permalink":"/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql"},"nextItem":{"title":"The Ultimate AWS to GCP Thesaurus","permalink":"/ultimate-aws-to-gcp-thesaurus"}},"content":"![Service Mesh](images/service-mesh-1.png)\\r\\n\\r\\nSo you\u2019ve started delivering a new project and it\u2019s all about this \u201cCloud Native\u201d or \u201cMicroservices\u201d thing. You\u2019re a Delivery Manager or Software Engineer at some type of company and someone has lightly peppered a meeting with a term, \u2018Mesh\u2019.\\r\\n\\r\\nThey possibly said event mesh. Or better yet, they mentioned a service mesh. As time went on you kept hearing more and more about the service mesh. You\u2019ve attempted to read up about it, digested a whole bunch of new terms and still didn\u2019t completely understand what the Mesh even does, why you would need it or why the hype train around this technology shows no sign of stopping. This article is an attempt to provide a focused guide to the service mesh, and why it is so interesting.\\r\\n\\r\\n## Ok, so what is this thing?\\r\\n\\r\\nTruth be told, the service mesh is actually pretty simple. It\u2019s built around the idea of small, repeatable bits of software, in this case userspace proxies, stuck very close to your services. This is called the **_data plane_**. In addition to the userspace proxies, you also get a bunch of management processes, which is referred to as the **_control plane_**. Simply put, the data plane (userspace proxies) intercepts all calls between services and the control plane (management processes) coordinates the wholesale behaviour of those proxies. This allows you to perform sweeping changes across your service landscape via the control planes API\u2019s, operators and provides the capability to measure your mesh as a whole.\\r\\n\\r\\nBefore we get into the engineering of what the proxies are, let\u2019s go with an example.\\r\\n\\r\\n- The business has bought some software.\\r\\n- The engineers are tasked with deploying this software in their Kubernetes cluster.\\r\\n- The engineers first task is to containerise this application, expose its functionality to downstream applications and deploy it to the cluster in a repeatable, continuous fashion.\\r\\n- There\u2019s a requirement in your organisation that says \u2018I need all communications to this vendors software as TLS1.3\u2019. Or, \u2018I would like to measure all API latency from this application\u2019.\\r\\n\\r\\nThe engineer replies \u2018I can\u2019t make changes to a third party application! What do I do?\u2019. Service mesh to the rescue.\\r\\n\\r\\nUsing a service mesh, you can deploy a proxy right next to your vendor container and in effect, abstract away the complexities of measurement and data transport mechanisms, and allow the vendor software to concentrate on it\u2019s business logic.\\r\\n\\r\\nThis vendor container is now part of the **_service mesh_**.\\r\\n\\r\\n## Proxies\\r\\n\\r\\nWhen we talk about proxies, we usually discuss things in OSI model terminology, that is to say Layers 1 through 7. Most of the time when it comes to proxies, you\u2019re comparing Layer 4 to Layer 7. Here\u2019s a quick run-down:\\r\\n\\r\\nLayer 4 (L4) -> operates with the delivery of messages with no regard to the content of the messages. They would simply forward network packets to and from the server without inspecting any part of the packets.\\r\\n\\r\\nLayer 7 (L7) -> this is a higher level, application layer. This deals with the actual content of the message. If you were routing network traffic, you could do this at L7 in a much more sophisticated way because you can now make decisions based on the packets messages within.\\r\\n\\r\\nWhy pick between L4 and L7? _Speed_.\\r\\n\\r\\nBack to the service mesh, these userspace proxies are L7-aware TCP proxies. Think _**NGINX**_ or _**haproxy**_. There are different proxies; [Linkerd](https://linkerd.io/) is an ultralight service mesh for Kubernetes. The most popular is [Envoy](https://www.envoyproxy.io/), which was created by the ride-share company Lyft. Above, I also mentioned NGINX and haproxy which are also quite popular. So what differentiates NGINX proxies from the service mesh? Their _focus_. You would implement NGINX as an Ingress proxy (traffic entering your network), but when it comes to proxies that focus on traffic between services, that\u2019s when the service mesh proxy comes in to play.\\r\\n\\r\\nOk, probably time for a diagram now that we\u2019ve explained the Data Plane.\\r\\n\\r\\n[![](images/service-mesh.png)](images/service-mesh.png)\\r\\n\\r\\nTune in for part 2 for when we discuss the Control Plane!"},{"id":"ultimate-aws-to-gcp-thesaurus","metadata":{"permalink":"/ultimate-aws-to-gcp-thesaurus","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-12-30-the-ultimate-aws-to-gcp-thesaurus/index.md","source":"@site/blog/2019-12-30-the-ultimate-aws-to-gcp-thesaurus/index.md","title":"The Ultimate AWS to GCP Thesaurus","description":"aws to gcp thesauraus","date":"2019-12-30T00:00:00.000Z","formattedDate":"December 30, 2019","tags":[{"label":"amazonwebservices","permalink":"/tags/amazonwebservices"},{"label":"aws","permalink":"/tags/aws"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"}],"readingTime":4.42,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"ultimate-aws-to-gcp-thesaurus","title":"The Ultimate AWS to GCP Thesaurus","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":false,"image":"images/aws-to-gcp-thesauraus.png","tags":["amazonwebservices","aws","gcp","googlecloudplatform"],"keywords":["amazonwebservices","aws","gcp","googlecloudplatform"]},"prevItem":{"title":"Sick of hearing about Service Mesh? Here\u2019s what you need to know...","permalink":"/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know"},"nextItem":{"title":"Google Cloud Storage Object Notifications using Slack","permalink":"/google-cloud-storage-object-notifications-using-slack"}},"content":"![aws to gcp thesauraus](images/aws-to-gcp-thesauraus.png)\\r\\n\\r\\nThere are many posts available which map analogous services between the different cloud providers, but this post attempts to go a step further and map additional concepts, terms, and configuration options to be the definitive thesaurus for cloud practitioners familiar with AWS looking to fast track their familiarisation with GCP.\\r\\n\\r\\nIt should be noted that AWS and GCP are fundamentally different platforms, nowhere is this more apparent than in the way networking is implemented between the two providers, see: [__GCP Networking for AWS Professionals__](https://cloudywithachanceofbigdata.com/gcp-networking-for-aws-professionals/)\\r\\n\\r\\nThis post is focused on the core infrastructure, networking and security services offered by the two major cloud providers, I will do a future post on higher level services such as the ML/AI offerings from the respective providers.\\r\\n\\r\\nFurthermore this will be a living post which I will continue to update, I encourage comments from readers on additional mappings which I will incorporate into the post as well.\\r\\n\\r\\nI have broken this down into sections based upon the layout of the AWS Console.\\r\\n\\r\\n- [![](images/compute.png) __Compute__](#compute)\\r\\n- [![](images/storage.png) __Storage__](#storage)\\r\\n- [![](images/database.png) __Database__](#database)\\r\\n- [![](images/networking.png) __Networking & Content Delivery__](#networking)\\r\\n- [![](images/security.png) __Security, Identity, & Compliance__](#security)\\r\\n\\r\\n<a name=\\"compute\\"></a>\\r\\n\\r\\n## ![](images/compute.png) Compute\\r\\n\\r\\n![](images/aws.png) | ![](images/gcp.png)\\r\\n--- | ---\\r\\nEC2 (Elastic Compute Cloud) | GCE (Google Compute Engine)\\r\\nAvailability Zone | Zone\\r\\nInstance | VM Instance\\r\\nInstance Family | Machine Family\\r\\nInstance Type | Machine Type\\r\\nAmazon Machine Image (AMI) | Image\\r\\nIAM Role (for an EC2 Instance) | Service Account\\r\\nSecurity Groups | VPC Firewall Rules (ALLOW)\\r\\nTag | Label\\r\\nTermination Protection | Deletion Protection\\r\\nReserved Instances | Committed Use Discounts\\r\\nCapacity Reservation | Reservation\\r\\nUser Data | Startup Script\\r\\nSpot Instances | Preemptible VMs\\r\\nDedicated Instances | Sole Tenancy\\r\\nEBS Volume | Persistent Disk\\r\\nAuto Scaling Group | Managed Instance Group\\r\\nLaunch Configuration | Instance Template\\r\\nELB Listener | URL Map (Load Balancer)\\r\\nELB Target Group | Backend/ Instance Group\\r\\nInstance Storage (ephemeral) | Local SSDs\\r\\nEBS Snapshots | Snapshots\\r\\nKeypair | SSH Keys\\r\\nElastic IP | External IP\\r\\nLambda | Google Cloud Functions\\r\\nElastic Beanstalk | Google App Engine\\r\\nElastic Container Registry (ECR) | Google Container Registry (GCR)\\r\\nElastic Container Service (ECS) | Google Kubernetes Engine (GKE)\\r\\nElastic Kubernetes Service (EKS) | Google Kubernetes Engine (GKE)\\r\\nAWS Fargate | Cloud Run\\r\\nAWS Service Quotas | Allocation Quotas\\r\\nAccount (within an Organisation)&dagger; | Project\\r\\nRegion | Region\\r\\nAWS Cloud\u200bFormation | Cloud Deployment Manager\\r\\n\\r\\n<a name=\\"storage\\"></a>\\r\\n\\r\\n## ![](images/storage.png) Storage\\r\\n\\r\\n![](images/aws.png) | ![](images/gcp.png)\\r\\n--- | ---\\r\\nSimple Storage Service (S3) | Google Cloud Storage (GCS)\\r\\nStandard Storage Class | Standard Storage Class\\r\\nInfrequent Access Storage Class | Nearline Storage Class\\r\\nAmazon Glacier | Coldline Storage Class\\r\\nLifecycle Policy | Retention Policy\\r\\nTags | Labels\\r\\nSnowball | Transfer Appliance\\r\\nRequester Pays | Requester Pays\\r\\nRegion | Location Type/Location\\r\\nObject Lock | Hold\\r\\nVault Lock (Glacier) | Bucket Lock\\r\\nMulti Part Upload | Parallel Composite Transfer\\r\\nCross-Origin Resource Sharing (CORS) | Cross-Origin Resource Sharing (CORS)\\r\\nStatic Website Hosting | Bucket Website Configuration\\r\\nS3 Access Points | VPC Service Controls\\r\\nObject Notifications | Pub/Sub Notifications for Cloud Storage\\r\\nPresigned URL | Signed URL\\r\\nTransfer Acceleration | Storage Transfer Service\\r\\nElastic File System (EFS) | Cloud Filestore\\r\\nAWS DataSync | Transfer Service for on-premises data\\r\\nETag | ETag\\r\\nBucket | Bucket\\r\\n`aws s3` | `gsutil`\\r\\n\\r\\n<a name=\\"database\\"></a>\\r\\n\\r\\n## ![](images/database.png) Database\\r\\n\\r\\n![](images/aws.png) | ![](images/gcp.png)\\r\\n--- | ---\\r\\nRelational Database Service (RDS) | Cloud SQL\\r\\nDynamoDB | Cloud Datastore\\r\\nElastiCache | Cloud Memorystore\\r\\nTable (DynamoDB) | Kind (Cloud Datastore)\\r\\nItem (DynamoDB) | Entity (Cloud Datastore)\\r\\nPartition Key (DynamoDB) | Key (Cloud Datastore)\\r\\nAttributes (DynamoDB) | Properties (Cloud Datastore)\\r\\nLocal Secondary Index (DynamoDB) | Composite Index (Cloud Datastore)\\r\\nElastic Map Reduce (EMR) | Cloud DataProc\\r\\nAthena | Big Query\\r\\nAWS Glue | Cloud DataFlow\\r\\nGlue Catalog | Data Catalog\\r\\nAmazon Simple Notification Service (SNS) | Cloud PubSub (push subscription)\\r\\nAmazon Kinesis | Cloud PubSub\\r\\nAmazon Simple Queue Service (SQS) | Cloud PubSub (poll and pull mode)\\r\\n\\r\\n<a name=\\"networking\\"></a>\\r\\n\\r\\n## ![](images/networking.png) Networking & Content Delivery\\r\\n\\r\\n![](images/aws.png) | ![](images/gcp.png)\\r\\n--- | ---\\r\\nVirtual Private Cloud (VPC) (Regional) | VPC Network (Global or Regional)\\r\\nSubnet (Zonal) | Subnet (Regional)\\r\\nRoute Tables | Routes\\r\\nNetwork ACLs (NACLS) | VPC Firewall Rules (ALLOW or DENY)\\r\\nCloudFront | Cloud CDN\\r\\nRoute 53 | Cloud DNS/Google Domains\\r\\nDirect Connect | Dedicated (or Partner) Interconnect\\r\\nVirtual Private Network (VPN) | Cloud VPN\\r\\nAWS PrivateLink | Google Private Access\\r\\nNAT Gateway | Cloud NAT\\r\\nElastic Load Balancer | Load Balancer\\r\\nAWS WAF | Cloud Armour\\r\\nVPC Peering Connection | VPC Network Peering\\r\\nAmazon API Gateway | Apigee API Gateway\\r\\nAmazon API Gateway | Cloud Endpoints\\r\\n\\r\\n<a name=\\"security\\"></a>\\r\\n\\r\\n## ![](images/security.png) Security, Identity, & Compliance\\r\\n\\r\\n![](images/aws.png) | ![](images/gcp.png)\\r\\n--- | ---\\r\\nRoot Account | Super Admin\\r\\nIAM User | Member\\r\\nIAM Policy | Role (Collection of Permissions)\\r\\nIAM Policy Attachment | IAM Role Binding (or IAM Binding)\\r\\nKey Management Service (KMS) | Cloud KMS\\r\\nCloudHSM | Cloud HSM\\r\\nAmazon Inspector (agent based) | Cloud Security Scanner (scan based)\\r\\nAWS Security Hub | Cloud Security Command Center (SCC)\\r\\nSecrets Manager | Secret Manager\\r\\nAmazon Macie | Cloud Data Loss Prevention (DLP)\\r\\nAWS WAF | Cloud Armour\\r\\nAWS Shield | Cloud Armour\\r\\n\\r\\n\u2020 No direct equivalent, this is the closest equivalent\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"google-cloud-storage-object-notifications-using-slack","metadata":{"permalink":"/google-cloud-storage-object-notifications-using-slack","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-11-09-google-cloud-storage-object-notifications-using-slack/index.md","source":"@site/blog/2019-11-09-google-cloud-storage-object-notifications-using-slack/index.md","title":"Google Cloud Storage Object Notifications using Slack","description":"This article describes the steps to integrate Slack with Google Cloud Functions to get notified about object events within a specified Google Cloud Storage bucket.","date":"2019-11-09T00:00:00.000Z","formattedDate":"November 9, 2019","tags":[{"label":"gcp","permalink":"/tags/gcp"},{"label":"googlecloudplatform","permalink":"/tags/googlecloudplatform"},{"label":"slack","permalink":"/tags/slack"},{"label":"terraform","permalink":"/tags/terraform"}],"readingTime":2.85,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"google-cloud-storage-object-notifications-using-slack","title":"Google Cloud Storage Object Notifications using Slack","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/Slack-GCS-Image.png","tags":["gcp","googlecloudplatform","slack","terraform"],"keywords":["gcp","googlecloudplatform","slack","terraform"]},"prevItem":{"title":"The Ultimate AWS to GCP Thesaurus","permalink":"/ultimate-aws-to-gcp-thesaurus"},"nextItem":{"title":"Map Reduce is Dead, Long Live Map Reduce","permalink":"/map-reduce-is-dead-long-live-map-reduce"}},"content":"import Gist from \'react-gist\';\\r\\nimport ImageWithCaption from \'/js/ImageWithCaption/ImageWithCaption.js\';\\r\\nimport SetupImage1 from \'./images/slack-notifications-setup-1.png\';\\r\\nimport SetupImage2 from \'./images/slack-notifications-setup-2.png\';\\r\\nimport SetupImage3 from \'./images/slack-notifications-setup-3.png\';\\r\\nimport SetupImage4 from \'./images/slack-notifications-setup-4.png\';\\r\\nimport SetupImage5 from \'./images/slack-notifications-setup-5.png\';\\r\\nimport SetupImage6 from \'./images/slack-notifications-setup-6.png\';\\r\\nimport SetupImage7 from \'./images/output-onlinepngtools.png\';\\r\\nimport SetupImage8 from \'./images/slack-notification.png\';\\r\\n\\r\\nThis article describes the steps to integrate Slack with Google Cloud Functions to get notified about object events within a specified Google Cloud Storage bucket.\\r\\n\\r\\n[![Google Cloud Storage Object Notifications using Slack](images/Slack-GCS.png)](images/Slack-GCS.png)\\r\\n\\r\\nEvents could include the creation of new objects, as well as delete, archive or metadata operations performed on a given bucket.\\r\\n\\r\\nThis pattern could be easily extended to other event sources supported by Cloud Functions including:\\r\\n\\r\\n- Cloud Pub/Sub messages\\r\\n- Cloud Firestore and Firebase events\\r\\n- Stackdriver log entries\\r\\n\\r\\nMore information can be found at [https://cloud.google.com/functions/docs/concepts/events-triggers](https://cloud.google.com/functions/docs/concepts/events-triggers).\\r\\n\\r\\nThe prerequisite steps to configure Slack are provided here:\\r\\n\\r\\n1. First you will need to create a Slack app (assuming you have already set up an account and a workspace). The following screenshots demonstrate this process:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage1}\\r\\naltText=\\"Create a Slack app\\"\\r\\n/>\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage2}\\r\\naltText=\\"Give the app a name and associate it with an existing Slack workspace\\"\\r\\n/>\\r\\n\\r\\n2. Next you need to Enable and Activate Incoming Webhooks to your app and add this to your workspace. The following screenshots demonstrate this process:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage3}\\r\\naltText=\\"Enable Incoming Web Hooks for the app\\"\\r\\n/>\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage4}\\r\\naltText=\\"Activate incoming webhooks\\"\\r\\n/>\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage5}\\r\\naltText=\\"Add the webhook to your workspace\\"\\r\\n/>\\r\\n\\r\\n3. Next you need to specify a channel for notifications generated from object events.\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage6}\\r\\naltText=\\"Select a channel for the webhook\\"\\r\\n/>\\r\\n\\r\\n4. Now you need to copy the Webhook url provided, you will use this later in your Cloud Function.\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage7}\\r\\naltText=\\"Copy the webhook URL to the clipboard\\"\\r\\n/>\\r\\n\\r\\n> Treat your webhook url as a secret, do not upload this to a public source code repository\\r\\n\\r\\nNext you need to create your Cloud Function, this example uses Python but you can use an alternative runtime including Node.js or Go.\\r\\n\\r\\nThis example templates the source code using the Terraform `template_file` data source. The function source code is shown here:\\r\\n\\r\\n<Gist id=\\"e248abd1af393e58de84e8776161c8cb\\" \\r\\n/>\\r\\n\\r\\nWithin your Terraform code you need to render your Cloud Function code substituting the `slack_webhook_url` for it\'s value which you will supply as a Terraform variable. The rendered template file is then placed in a local directory along with a `requirements.txt` file and zipped up. The resulting Zip archive is uploaded to a specified bucket where it will be sourced to create the Cloud Function.\\r\\n\\r\\n<Gist id=\\"e247d09d33a4aca9154de081f3063978\\" \\r\\n/>\\r\\n\\r\\nNow you need to create the Cloud Function, the following HCL snippet demonstrates this:\\r\\n\\r\\n<Gist id=\\"87e2e83e5b2b800d685a8d239280ca13\\" \\r\\n/>\\r\\n\\r\\nThe `event_trigger` block in particular specifies which GCS bucket to watch and what events will trigger invocation of the function. Bucket events include:\\r\\n\\r\\n- `google.storage.object.finalize` _(the creation of a new object)_\\r\\n- `google.storage.object.delete`\\r\\n- `google.storage.object.archive`\\r\\n- `google.storage.object.metadataUpdate`\\r\\n\\r\\nYou could add additional logic to the Cloud Function code to look for specific object names or naming patterns, but keep in mind the function will fire upon every event matching the `event_type` and `resource` criteria.\\r\\n\\r\\nTo deploy the function, you would simply run:\\r\\n\\r\\n```\\r\\nterraform apply -var=\\"slack_webhook_url=https://hooks.slack.com/services/XXXXXXXXX/XXXXXXXXX/XXXXXXXXXXXXXXXXXXXXXXXX\\"\\r\\n```\\r\\n\\r\\nNow once you upload a file named `test-object.txt`, voil\xe0!:\\r\\n\\r\\n<ImageWithCaption \\r\\nimageSrc={SetupImage8}\\r\\naltText=\\"Slack notification for a new object created\\"\\r\\n/>\\r\\n\\r\\n> Full source code is available at: [https://github.com/gamma-data/gcs-object-notifications-using-slack](https://github.com/gamma-data/gcs-object-notifications-using-slack)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"map-reduce-is-dead-long-live-map-reduce","metadata":{"permalink":"/map-reduce-is-dead-long-live-map-reduce","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-09-01-map-reduce-is-dead-long-live-map-reduce/index.md","source":"@site/blog/2019-09-01-map-reduce-is-dead-long-live-map-reduce/index.md","title":"Map Reduce is Dead, Long Live Map Reduce","description":"Map Reduce is Dead","date":"2019-09-01T00:00:00.000Z","formattedDate":"September 1, 2019","tags":[{"label":"big-data","permalink":"/tags/big-data"},{"label":"hadoop","permalink":"/tags/hadoop"},{"label":"map-reduce","permalink":"/tags/map-reduce"}],"readingTime":3.025,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"map-reduce-is-dead-long-live-map-reduce","title":"Map Reduce is Dead, Long Live Map Reduce","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/image.png","tags":["big-data","hadoop","map-reduce"],"keywords":["big-data","hadoop","map-reduce"]},"prevItem":{"title":"Google Cloud Storage Object Notifications using Slack","permalink":"/google-cloud-storage-object-notifications-using-slack"},"nextItem":{"title":"Ansible Tower for Continuous Infrastructure","permalink":"/ansible-tower-for-continuous-infrastructure"}},"content":"![Map Reduce is Dead](images/image.png)\\r\\n\\r\\nFirstly, this is not another Hadoop obituary, there are enough of those out there already.\\r\\n\\r\\nThe generalized title of this article has been used as an expression to convey the idea that something old has been replaced by something new. In the case of the expression \u201cthe King is dead, long live the King\u201d the inference is that although one monarch has passed, another monarch instantly succeeds him.\\r\\n\\r\\nIn the age of instant gratification and hype cycle driven \u2018pump and dump\u2019 investment we are very quick to discard technologies that don\u2019t realise overzealous targets for sales or market share. In our continuous attempts to find the next big thing, we are quick to throw out the last big thing and everything associated with it.\\r\\n\\r\\n## The Reports of My Death Have Been Greatly Exaggerated\\r\\n\\r\\nA classic example of this is the notion that Map Reduce is dead. Largely proliferated by the Hadoop obituaries which seem to be growing exponentially with each day.\\r\\n\\r\\nA common e-myth is that Google invented the Map Reduce pattern, which is completely incorrect. In 2004, Google described a framework distributed systems implementation of the Map Reduce pattern in a white paper named _\u201cMapReduce: Simplified Data Processing on Large Clusters.\u201d_ \u2013 this would inspire the first-generation processing framework (MapReduce) in the Hadoop project. But neither Google nor Yahoo! nor contributors to the Hadoop project (which include the pure play vendors) created the Map Reduce algorithm or processing pattern and neither shall any one of these have the rights to kill it.\\r\\n\\r\\nThe origins of the Map Reduce pattern can be traced all the way back to the early foundations of functional programming beginning with Lambda Calculus in the 1930s to LISP in the 1960s. Map Reduce is an integral pattern in all of today\u2019s functional and distributed systems programming. You only need to look at the support for `map()` and `reduce()` operators in some of the most popular languages today including Python, JavaScript, Scala, and many more languages that support functional programming.\\r\\n\\r\\nAs far as distributed processing frameworks go, the Map Reduce pattern and its `map()` and `reduce()` methods are very prominent as higher order functions in APIs such as Spark, Kafka Streams, Apache Samza and Apache Flink to name a few.\\r\\n\\r\\nWhile the initial Hadoop adaptation of Map Reduce has been supplanted by superior approaches, the Map Reduce processing pattern is far from dead.\\r\\n\\r\\n## On the fall of Hadoop...\\r\\n\\r\\nThere is so much hysteria around the fall of Hadoop, we need to be careful not to toss the baby out with the bath water. Hadoop served a significant role in bringing open source, distributed systems from search engine providers to academia all the way to the mainstream, and still serves an important purpose in many organizations data ecosystems today and will continue to do so for some time.\\r\\n\\r\\nOK, it wasn\u2019t the panacea to everything, but who said it was supposed to be? The Hadoop movement was hijacked by hysteria, hype, venture capital, over ambitious sales targets and financial engineering \u2013 this does not mean the technology was bad.\\r\\n\\r\\nHadoop spawned many significant related projects such as Spark, Kafka and Presto to name a few. These projects paved the way for cloud integration, which is now the dominant vector for data storage, processing, and analysis.\\r\\n\\r\\nWhile the quest for world domination by the Hadoop pure play vendors may be over, the Hadoop movement (and the impact it has had on the enterprise data landscape) will live on.\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"ansible-tower-for-continuous-infrastructure","metadata":{"permalink":"/ansible-tower-for-continuous-infrastructure","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-08-29-ansible-tower-for-continuous-infrastructure/index.md","source":"@site/blog/2019-08-29-ansible-tower-for-continuous-infrastructure/index.md","title":"Ansible Tower for Continuous Infrastructure","description":"As infrastructure and teams scale, effective and robust configuration management requires growing beyond manual processes and local conventions. Fortunately, Ansible Tower (or the upstream Open Source project Ansible AWX) provides a perfect platform for configuration management at scale.","date":"2019-08-29T00:00:00.000Z","formattedDate":"August 29, 2019","tags":[{"label":"ansible","permalink":"/tags/ansible"},{"label":"ci-cd","permalink":"/tags/ci-cd"},{"label":"continuous-infrastructure","permalink":"/tags/continuous-infrastructure"}],"readingTime":5.595,"hasTruncateMarker":false,"authors":[{"name":"Chris Ottinger","title":"Senior Technologist","url":"https://github.com/datwiz","imageURL":"http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80","key":"chrisottinger"}],"frontMatter":{"slug":"ansible-tower-for-continuous-infrastructure","title":"Ansible Tower for Continuous Infrastructure","authors":["chrisottinger"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["ansible","ci-cd","continuous-infrastructure"],"keywords":["ansible","ci-cd","continuous-infrastructure"]},"prevItem":{"title":"Map Reduce is Dead, Long Live Map Reduce","permalink":"/map-reduce-is-dead-long-live-map-reduce"},"nextItem":{"title":"Managing Secrets in CICD Pipelines","permalink":"/managing-secrets-in-cicd-pipelines"}},"content":"As infrastructure and teams scale, effective and robust configuration management requires growing beyond manual processes and local conventions. Fortunately, [Ansible Tower](https://www.ansible.com/products/tower) (or the upstream Open Source project [Ansible AWX](https://github.com/ansible/awx)) provides a perfect platform for configuration management at scale.\\r\\n\\r\\nThe\xa0[Ansible Tower/AWX documentation](https://docs.ansible.com/ansible-tower/index.html)\xa0and tutorials provide comprehensive information about the individual components. \xa0However, assembling all the moving pieces into a whole working solution can involve some trial and error and reverse engineering in order to understand how the components relate to one another. \xa0Ansible Tower, like the core Ansible solution, offers flexibility in how features assembled to support different typed of workflows. The types of workflows can include once-off initial configurations, ad-hoc system maintenance, or continuous convergence.\\r\\n\\r\\nContinuous convergence, also referred to as desired state, regularly re-applies the defined configuration to infrastructure. This tends to \'correct the drift\' often encountered when only applying the configuration on infrastructure setup. For example, a continuous convergence approach to configuration management could apply the desired configuration on a recurring schedule of every 30 minutes. \xa0\\r\\n\\r\\nSome continuous convergence workflow characteristics can include:\\r\\n\\r\\n- Idempotent Ansible roles. If there are no required configuration deviations, run will report 0 changes.\\r\\n- A source code repository per Ansible role, similar to the Ansible Galaxy approach,\\r\\n- A source code repository for Ansible playbooks that include the individual Ansible roles,\\r\\n- A host configured to provide one unique service function only,\\r\\n- An Ansible playbook defined for each unique service function that gets applied to the host,\\r\\n- Playbooks applied to each host on a repeating schedule.\\r\\n\\r\\nOne way to achieve a continuous convergence workflow combines the Ansible Tower components according to the following conceptual model.\\r\\n\\r\\n[![](images/Ansible-AWX-Continuous-Convergence.png)](images/Ansible-AWX-Continuous-Convergence.png)\\r\\n\\r\\n## The Workflow Components\\r\\n\\r\\n### Playbook and Role Source Code\\r\\n\\r\\n**Ansible roles** contain the individual tasks, handlers, and content\xa0with a role responsible for the installation and configuration of a particular software service.\\r\\n\\r\\n**Ansible playbooks**\xa0configure a host for a particular service function in the environment acting as a wrapper for the individual role based configurations. \xa0All the roles expected to be applied to a host must be defined in the playbook.\\r\\n\\r\\n### Source Code Repositories\\r\\n\\r\\n**Role git repositor**ies contain the versioned definition of a role, e.g. one git repository per individual role. \xa0The roles are pulled into the playbooks using the git reference and tags, which pegs the role version used within a playbook.\\r\\n\\r\\n**Project git repositories**\xa0group the individual playbooks into single collection, e.g. one git repository per set of playbooks. \xa0As with roles, specific versions of project repositories are also identified by version tags.\xa0\\r\\n\\r\\n### Ansible Tower Server\\r\\n\\r\\nTwo foundational concepts in Ansible Tower are projects and inventories. Projects provide access to playbooks and roles. Inventories provide the connection to \\"real\\" infrastructure. \xa0Inventories and projects also provide authorisation scope for activities in Ansible Tower. For example, a given group can use the playbooks in Project X and apply jobs to hosts in Inventory Y.\\r\\n\\r\\nEach\xa0**Ansible Tower Project**\xa0is backed by a project git repository. \xa0Each repository contains the playbooks and included roles that can be applied by a given job. \xa0The Project is the glue between the Ansible configuration tasks and the plays that apply the configuration.\\r\\n\\r\\n**Ansible Tower Inventories**\xa0are sets of hosts grouped for administration, similar to inventory sets used when applying playbooks manually. \xa0One option is to group hosts into Inventories by environment. \xa0For example, the hosts for development may be in one Inventory while the hosts for production may be in another Inventory. \xa0User authorisation controls are applied at the Inventory level.\\r\\n\\r\\n**Ansible Tower\xa0Inventory Groups**\xa0define sub-sets of hosts within the larger Inventory. \xa0These subsets can then be used to limit the scope of a playbook job. \xa0One option is to group hosts within an Inventory by function. \xa0For example, the hosts for web servers may be in one Inventory Group and the hosts for databases may be in another Inventory Group. \xa0This enables one playbook to target one inventory group. \xa0Inventory groups effectively provide metadata labels for hosts in the Inventory.\\r\\n\\r\\nAn\xa0**Ansible Job Template**\xa0determines the configuration to be applied to hosts. \xa0The Job Template\xa0links a playbook from a project to an inventory. \xa0 The inventory scope can be optionally further limited by specifying inventory group limits. \xa0A Job Template can be invoked either on an ad-hoc basis or via a recurring schedule.\\r\\n\\r\\n**Ansible Job Schedules**\xa0define the time and frequency at which the configuration specified in the Job Template is applied. \xa0Each Job Template can be associated with one or more Job Schedules. \xa0A schedule supports either once-off execution, for example during a defined change window, or regularly recurring execution. \xa0A job schedule that applies the desired state configuration with a frequency of 30 minutes provides an example of a job schedule used for a continuous convergence workflow.\\r\\n\\r\\n### \\"Real\\" Infrastructure\\r\\n\\r\\nAn\xa0**Ansible Job Instance**\xa0defines a single invocation of an Ansible Job Template, both for scheduled and ad-hoc invocations of the job template. \xa0Outside of Ansible Tower, the Job Instance is the equivalent of executing the\xa0`ansible-playbook`\xa0command using an inventory file.\\r\\n\\r\\nA\xa0**Host**\xa0is the actual target infrastructure resources configured by the job instance, applying an ansible playbook of included roles.\\r\\n\\r\\n## A note on Ansible Variables\\r\\n\\r\\nAs with other features of Ansible and Ansible Tower, variables also offer flexibility in defining parameters and context when applying a configuration. \xa0In addition to declaring and defining variables in roles and playbooks, variable definitions can also be defined in Ansible Tower job templates, inventory and inventory groups, and individual hosts. \xa0Given the plethora of options for variable definition locations, without a set of conventions for managing variable values, debugging runtime issues with roles and playbooks can become difficult. \xa0E.g. which value defined at which location was used when applying the role?\\r\\n\\r\\nOne example of variable definitions conventions could include:\\r\\n\\r\\n- Variables shall be given default values in the role, .e.g. in the\xa0`../defaults/main.yml`\xa0file.\\r\\n- If the variable must have a \'real\' value supplied when applying the playbook, the variable shall be defined with an obvious placeholder value which will fail if not overridden.\\r\\n- Variables shall be described in the role\xa0`README.md`\xa0documentation\\r\\n- Do not apply variables at the host inventory level as host inventory can be transient.\\r\\n- Variables that select specific capabilities within a role shall be defined at the Ansible Tower Inventory Group. \xa0For example, a role contains the configuration definition for both master and work nodes. \xa0The Inventory Group variables are used to indicate which hosts must have the master configuration and applied and which must have the worker configuration applied.\\r\\n- Variables that define the environment context for configuration shall be defined in the Ansible Tower Job Template.\\r\\n\\r\\nFollowing these conventions, each of the possible variable definition options serves a particular purpose. \xa0When an issue with variable definition does arise, the source is easily identified."},{"id":"managing-secrets-in-cicd-pipelines","metadata":{"permalink":"/managing-secrets-in-cicd-pipelines","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-07-16-managing-secrets-in-cicd-pipelines/index.md","source":"@site/blog/2019-07-16-managing-secrets-in-cicd-pipelines/index.md","title":"Managing Secrets in CICD Pipelines","description":"Gitlab Vault","date":"2019-07-16T00:00:00.000Z","formattedDate":"July 16, 2019","tags":[{"label":"ci-cd","permalink":"/tags/ci-cd"},{"label":"gitlab-ci","permalink":"/tags/gitlab-ci"},{"label":"hashicorp-vault","permalink":"/tags/hashicorp-vault"},{"label":"jenkins","permalink":"/tags/jenkins"},{"label":"secrets-management","permalink":"/tags/secrets-management"}],"readingTime":7.54,"hasTruncateMarker":false,"authors":[{"name":"Chris Ottinger","title":"Senior Technologist","url":"https://github.com/datwiz","imageURL":"http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80","key":"chrisottinger"}],"frontMatter":{"slug":"managing-secrets-in-cicd-pipelines","title":"Managing Secrets in CICD Pipelines","authors":["chrisottinger"],"draft":false,"hide_table_of_contents":true,"image":"images/Gitlab-Vault.png","tags":["ci-cd","gitlab-ci","hashicorp-vault","jenkins","secrets-management"],"keywords":["ci-cd","gitlab-ci","hashicorp-vault","jenkins","secrets-management"]},"prevItem":{"title":"Ansible Tower for Continuous Infrastructure","permalink":"/ansible-tower-for-continuous-infrastructure"},"nextItem":{"title":"Change Data Capture at Scale using Spark","permalink":"/change-data-capture-at-scale-using-spark"}},"content":"![Gitlab Vault](images/Gitlab-Vault.png)\\r\\n\\r\\n## Overview\\r\\n\\r\\nWith the adoption automation for deploying and managing application environments, protecting privileged accounts and credential secrets in a consistent, secure, and scalable manner becomes critical. \xa0Secrets can include account usernames, account passwords and API tokens. \xa0Good credentials management and secrets automation practices reduce the risk of secrets escaping into the wild and being used either intentionally (hacked) or unintentionally (accident).\\r\\n\\r\\n- Reduce the likelihood of passwords slipping into source code commits and getting pushed to code repositories, especially public repositories such as github.\\r\\n- Minimise the secrets exposure surface area by reducing the number of people who require knowledge of credentials. \xa0With an automated credentials management process that number can reach zero.\\r\\n- Limit the useful life of a secret by employing short expiry times and small time-to-live (TTL) values. \xa0Automation enables reliable low-effort secret re-issue and rotation.\\r\\n\\r\\n## Objectives\\r\\n\\r\\nThe following objectives have been considered in designing a secrets automation solution that can be integrated into an existing CICD environment.\\r\\n\\r\\n- Integrate into an existing CICD environment without requiring an \\"all or nothing\\" implementation. \xa0Allow existing jobs to operate alongside jobs that have been converted to the new secrets automation solution.\\r\\n- A single design that can be applied across different toolchains and deployment models. \xa0For example, deployment to a Kubernetes environment can use the same secrets management process as an application installation on a virtual machine. \xa0Similarly, the design can be used with different CICD tools, such as [GitLab-CI](https://about.gitlab.com), [Travis-CI](https://travis-ci.org), or other build and deploy automation tool.\\r\\n- Multi-cloud capable by limiting coupling to a specific hosting environment or cloud services provider.\\r\\n- The use of secrets (or not) can be decided at any point in time, without requiring changes to the CICD job definition, similar to the use of feature flags in applications.\\r\\n- Enable changes to secrets, either due to rotation or revocation, to be maintained from a central service point. \xa0Avoid storing the same secret multiple times in different locations.\\r\\n- Secrets organised in predictable locations in a \\"rest-ish\\" fashion by treating secrets and credentials as attributes of entities.\\r\\n- Use environment variables as the standard interface between deployment orchestration and deployed application, following the 12 Factor App approach.\\r\\n\\r\\n## Solution\\r\\n\\r\\n- Secrets stored centrally in Hashicorp Vault.\\r\\n- CICD jobs retrieve secrets from Vault and configure the application deployment environment.\\r\\n- Deployed applications use the secrets supplied by CICD job to access backend services.\\r\\n\\r\\n[![CICD Secrets with Vault](images/Screen-Shot-2019-07-16-at-17.03.47.png)](images/Screen-Shot-2019-07-16-at-17.03.47.png)\\r\\n\\r\\n## Storing Secrets\\r\\n\\r\\nUse\xa0[Vault by Hashicorp](https://www.vaultproject.io/)\xa0as a centralised secrets storage service. \xa0The CICD service retrieves secrets information for integration and deployment jobs. \xa0Vault provides a flexible set of features to support numerous different workflows and available as either Vault Open Source or Vault Enterprise. \xa0The secrets management pattern described uses the Vault Open Source version. \xa0The workflow described here can be explored using Vault in the unsecured development mode, however, a properly configured and managed Vault service is required for production use.\\r\\n\\r\\nVault supports a number of secrets backends and access workflow models. \xa0This solution makes use of the\xa0[Vault AppRole method](https://www.vaultproject.io/docs/auth/approle.html), which is designed to support machine-to-machine automated workflows. \xa0With the AppRole workflow model human access to secrets is minimised through the use of access controls and temporary credentials with short TTL\'s. \xa0Within Vault, secrets are organised using an entity centric \\"rest-ish\\" style approach ensuring a given secret for a given service is stored in a single predictable location.\\r\\n\\r\\nThe use of Vault satisfies several of the design objectives:\\r\\n\\r\\n- enables single point management of secrets. The secrets content is stored in a single location referenced at CICD job runtime. \xa0On the next invocation, the CICD job retrieves the latest version of the secrets content.\\r\\n- enables storing secrets in predictable locations with file system directory style path location. \xa0The \\"rest-ish\\" approach to organising secret locations enables storing a given secret only once. \xa0Access policies provide the mechanism to limit CICD \xa0visibility to only those secrets required for the CICD job.\\r\\n\\r\\n## Passing Secrets to Applications\\r\\n\\r\\nUse environment variables to pass secrets from the CICD service to the application environment. \xa0\\r\\n\\r\\nThere are existing utilities available for populating a child process environment with Vault sourced secrets, such as\xa0[vaultenv](https://github.com/channable/vaultenv)\xa0or\xa0[envconsul](https://github.com/hashicorp/envconsul). \xa0This approach works well for running an application service. \xa0However, with CICD, often there are often sets of tasks that require access to secrets information as opposed to a single command. \xa0Using the child environment approach would require wrapping each command in a CICD job step with the env utility. \xa0This works against the objective of introducing a secrets automation solution into existing CICD jobs without requiring substantial refactoring. \xa0Similarly, some CICD solutions such as\xa0[Jenkins](https://jenkins.io/)\xa0provide Vault integration plugins which pre-populate the environment with secrets content. \xa0This meets the objective of minimal CICD job refactoring, but closely couples the solution to a particular CICD service stack, reducing portability. \xa0\\r\\n\\r\\nWith a job script oriented CICD automation stack like GitLab-CI or Travis-CI, an alternative is to insert a job step at the beginning of a series of CICD tasks that will populated the required secret values into expected environment variables. \xa0Subsequent tasks in the job can then execute without requiring refactoring. \xa0The decision on whether to source a particular environment variable\'s content directly from the CICD job setup or from the Vault secrets store can be made by adding an optional prefix to environment variables to be sourced from the Vault secrets store. \xa0The prefixed instance of the environment variable contains the location or path to the required secret. \xa0Secret locations are identified using the convention\xa0`/<vault-secret-path>/<secret-key>`\\r\\n\\r\\n- enables progressive implementation due to transparency of secret sourcing. Subsequent steps continue to rely on expected environment vars\\r\\n- enables use in any toolchain that supports use of environment variables to pass information to application environment.\xa0\\r\\n- CICD job steps not tied to a specific secrets store. An alternative secrets storage service could be supported by only requiring modification of the secret getter utility.\\r\\n- control of whether to source application environment variables from the CICD job directly or from the secrets engine is managed at the CICD job setup level as opposed to requiring CICD job refactoring to switch the content source.\\r\\n- continues the 12 Factor App approach of using environment variables to pass context to application environments.\\r\\n\\r\\n## Example Workflow\\r\\n\\r\\nAn example workflow for a CICD job designed to use environment variables for configuring an application.\\r\\n\\r\\n### Assumptions\\r\\n\\r\\nThe following are available in the CICD environment.\\r\\n\\r\\n- A job script oriented CICD automation stack that executes job tasks as a series of shell commands, such as [GitLab-CI](https://about.gitlab.com) or [Jenkins Pipelines](https://jenkins.io/doc/book/pipeline/).\\r\\n- A secrets storage engine with a python API, such as Hashicorp Vault.\\r\\n- CICD execution environment includes the\xa0`[get-vault-secrets-by-approle](https://github.com/datwiz/cicd-secrets-in-vault/blob/master/scripts/get-vault-secrets-by-approle)`\xa0utility script.\\r\\n\\r\\n### Workflow Steps\\r\\n\\r\\n### Add a Vault secret\\r\\n\\r\\nAdd a secret to Vault at the location\xa0`secret/fake-app/users/fake-users`\xa0with a key/value entry of\xa0`password=fake-password`\\r\\n\\r\\n### Add a Vault access policy\\r\\n\\r\\nAdd a Vault policy for the CICD job (or set of CICD jobs) that includes \'read\' access to the secret.\\r\\n\\r\\n```\\r\\n# cicd-fake-app-policy \\r\\npath \\"secret/data/fake-app/users/fake-user\\" {\\r\\n    capabilities = [\\"read\\"]\\r\\n}\\r\\n\\r\\npath \\"secret/metadata/fake-app/users/fake-user\\" {\\r\\n    capabilities = [\\"list\\"]\\r\\n}\\r\\n```\\r\\n\\r\\n### Add a Vault appRole\\r\\n\\r\\nAdd a Vault appRole linked to the new policy. \xa0This example specifies a new appRole with an secret-id TTL of 60 days and non-renewable access tokens with a TTL of 5 minutes. \xa0The CICD job uses the access token to read secrets.\\r\\n\\r\\n```\\r\\nvault write auth/approle/role/fake-role \\\\\\r\\n    secret_id_ttl=1440h \\\\\\r\\n    token_ttl=5m \\\\\\r\\n    token_max_ttl=5m \\\\\\r\\n    policies=cicd-fake-app-policy\\r\\n```\\r\\n\\r\\n### Read the Vault approle-id\\r\\n\\r\\nRetrieve the approle-id of the new appRole taking note of the returned approle-id.\\r\\n\\r\\n```\\r\\nvault\xa0read auth/approle/role/fake-role\\r\\n```\\r\\n\\r\\n### Add a Vault appRole secret-id\\r\\n\\r\\nAdd a secret-id for the appRole, taking note of the returned secret-id\\r\\n\\r\\n```\\r\\nvault write -f auth/approle/role/fake-role/secret-id\\r\\n```\\r\\n\\r\\n### Add CICD Job Steps\\r\\n\\r\\nIn the CICD job definition insert job steps to retrieve secrets values a set variables in the job execution environment. These are the steps to add in a gitlab-ci.yml CICD job.\\r\\n\\r\\n```\\r\\n...\\r\\nscript:\\r\\n- get-vault-secrets-by-approle > ${VAULT_VAR_FILE}\\r\\n- source ${VAULT_VAR_FILE} && rm ${VAULT_VAR_FILE}\\r\\n...\\r\\n```\\r\\n\\r\\nThe helper script `get-vault-secrets-by-approle` could be executed and sourced in a single step, e.g.\xa0`source $(get-vault-secrets-by-approle)`. \xa0However, when executed in\xa0a single statement all script output is processed by the\xa0`source`\xa0command and script\xa0error messages\xa0don\'t get printed and captured in the job logs. \xa0Splitting the read and environment var sourcing into 2 steps aids in troubleshooting.\\r\\n\\r\\n### Add CICD job vars for Vault access\\r\\n\\r\\nIn the CICD job configuration add Vault access environment variables.\\r\\n\\r\\n```\\r\\nVAULT_ADDR=https://vault.example.com:8200\\r\\nVAULT_ROLE_ID=db02de05-fa39-4855-059b-67221c5c2f63\\r\\nVAULT_SECRET_ID=6a174c20-f6de-a53c-74d2-6018fcceff64\\r\\nVAULT_VAR_FILE=/var/tmp/vault-vars.sh\\r\\n```\\r\\n\\r\\n### Add CICD job vars for Vault secrets\\r\\n\\r\\nIn the CICD job configuration add environment variables for the items to be sourced from vault secrets. \xa0The secret path follows the convention of\xa0`<secret-mount-path>/<secret-path>/<secret-key>`\\r\\n\\r\\n```\\r\\nV_FAKE_PASSWORD=secret/fake-app/users/fake-user/password\\r\\n```\\r\\n### Remove CICD job vars\\r\\n\\r\\nIn the CICD job configuration remove the previously used\xa0`FAKE_APP_PASSWORD`\xa0variable.\\r\\n\\r\\n### Execute the CICD job\\r\\n\\r\\nKick off the CICD job. \xa0Any CICD job configuration variables prefixed with \\"`V_`\\" results in the addition of a corresponding environment variable in the job execution environment with content sourced from Vault.\\r\\n\\r\\n> Full source code can be found at:\\r\\n> \\r\\n> [https://github.com/datwiz/cicd-secrets-in-vault](https://github.com/datwiz/cicd-secrets-in-vault)"},{"id":"change-data-capture-at-scale-using-spark","metadata":{"permalink":"/change-data-capture-at-scale-using-spark","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-06-28-change-data-capture-at-scale-using-spark/index.md","source":"@site/blog/2019-06-28-change-data-capture-at-scale-using-spark/index.md","title":"Change Data Capture at Scale using Spark","description":"CDC using Spark","date":"2019-06-28T00:00:00.000Z","formattedDate":"June 28, 2019","tags":[{"label":"big-data","permalink":"/tags/big-data"},{"label":"cdc","permalink":"/tags/cdc"},{"label":"pyspark","permalink":"/tags/pyspark"},{"label":"python","permalink":"/tags/python"},{"label":"spark","permalink":"/tags/spark"}],"readingTime":8.335,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"change-data-capture-at-scale-using-spark","title":"Change Data Capture at Scale using Spark","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/CDC-using-Spark.png","tags":["big-data","cdc","pyspark","python","spark"],"keywords":["big-data","cdc","pyspark","python","spark"]},"prevItem":{"title":"Managing Secrets in CICD Pipelines","permalink":"/managing-secrets-in-cicd-pipelines"},"nextItem":{"title":"Synthetic CDC Data Generator","permalink":"/synthetic-cdc-data-generator"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![CDC using Spark](images/CDC-using-Spark.png)\\r\\n\\r\\nChange Data Capture (CDC) is one of the most challenging processing patterns to implement at scale. I personally have had several cracks at this using various different frameworks and approaches, the most recent of which was implemented using Spark \u2013 and I think I have finally found the best approach. Even though the code examples referenced use Spark, the pattern is language agnostic \u2013 the focus is on the approach not the specific implementation (as this could be applied to any framework or runtime).\\r\\n\\r\\n:::note Spark Training Courses\\r\\n\\r\\n[Data Transformation and Analysis Using Apache Spark](https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/)  \\r\\n[Stream and Event Processing using Apache Spark](https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/)  \\r\\n[Advanced Analytics Using Apache Spark](https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/)\\r\\n\\r\\n:::\\r\\n\\r\\nThe first challenge you are faced with, is to compare a very large dataset (representing the current state of an object) with another potentially very large dataset (representing new or incoming data). Ideally, you would like the process to be configuration driven and accommodate such things as composite primary keys, or operational columns which you would like to restrict from change detection. You may also want to implement a pattern to segregate sensitive attributes from non-sensitive attributes.\\r\\n\\r\\n## Overview\\r\\n\\r\\nThis pattern (and all my other recent attempts) is fundamentally based upon calculating a deterministic hash of the key and non-key attribute(s), and then using this hash as the basis for comparison. The difference between this pattern and my other attempts is in the distillation and reconstitution of data during the process, as well as breaking the pattern into discrete stages (designed to minimize the impact to other applications). This pattern can be used to process delta or full datasets.\\r\\n\\r\\nA high-level flowchart representing the basic pattern is shown here:\\r\\n\\r\\n[![CDC Flowchart](images/CDC.png)](images/CDC.png)\\r\\n\\r\\n## The Example\\r\\n\\r\\nThe example provided uses the [Synthetic CDC Data Generator application](https://github.com/avensolutions/synthetic-cdc-data-generator), configuring an incoming set with 5 uuid columns acting as a composite key, and 10 random number columns acting as non key values. The initial days payload consists of 10,000 records, the subsequent days payload consists of another 10,000 records. From the initial dataset, a `DELETE` operation was performed at the source system for 20% of records, an `UPDATE` was performed on 40% of the records and the remaining 40% of records were unchanged. In this case the 20% of records that were deleted at the source, were replaced by new `INSERT` operations creating new keys.\\r\\n\\r\\nAfter creating the synthesized day 1 and day 2 datasets, the files are processed as follows:\\r\\n\\r\\n$ spark-submit cdc.py config.yaml data/day1 2019-06-18  \\r\\n$ spark-submit cdc.py config.yaml data/day2 2019-06-19\\r\\n\\r\\nWhere `config.yaml` is the configuration for the dataset, data/day1 and data/day2 represent the different data files, and 2019-06-18 and 2019-06-19 represent a business effective date.\\r\\n\\r\\n## The Results\\r\\n\\r\\nYou should see the following output from running the preceding commands for day 1 and day 2 respectively:\\r\\n\\r\\n### Day 1:\\r\\n\\r\\n<Gist id=\\"b75edc7825b46c12b328d78d47b4b902\\" \\r\\n/>\\r\\n\\r\\n### Day 2:\\r\\n\\r\\n<Gist id=\\"ca92e132105fb5bb381bf9dfca562bf4\\" \\r\\n/>\\r\\n\\r\\nA summary analysis of the resultant dataset should show:\\r\\n\\r\\n<Gist id=\\"ded1f98dc4fce13c9bb3d12a51a46b94\\" \\r\\n/>\\r\\n\\r\\n## Pattern Details\\r\\n\\r\\nDetails about the pattern and its implementation follow.\\r\\n\\r\\n### Current and Historical Datasets\\r\\n\\r\\nThe output of each operation will yield a current dataset (that is the current stateful representation of a give object) and a historical dataset partition (capturing the net changes from the previous state in an appended partition).\\r\\n\\r\\nThis is useful, because often consumers will primarily query the latest state of an object. The change sets (or historical dataset partitions) can be used for more advanced analysis by sophisticated users.\\r\\n\\r\\n### Type 2 SCDs (sort of)\\r\\n\\r\\nTwo operational columns are added to each current and historical object:\\r\\n\\r\\n- `OPERATION` : Represents the last known operation to the record, valid values include :\\r\\n    - `I` (`INSERT`)\\r\\n    - `U` (`UPDATE`)\\r\\n    - `D` (`DELETE` \u2013 hard `DELETE`s, applies to full datasets only)\\r\\n    - `X` (Not supplied, applies to delta processing only)\\r\\n    - `N` (No change)\\r\\n- `EFF_START_DATE`\\r\\n\\r\\nSince data structures on most big data or cloud storage platforms are immutable, we only store the effective start date for each record, this is changed as needed with each coarse-grained operation on the current object. The effective end date is inferred by the presence of a new effective start date (or change in the `EFF_START_DATE` value for a given record).\\r\\n\\r\\n### The Configuration\\r\\n\\r\\nI am using a YAML document to store the configuration for the pattern. Important attributes to include in your configuration are a list of keys and non keys and their datatype (this implementation does type casting as well). Other important attributes include the table names and file paths for the current and historical data structures.\\r\\n\\r\\nThe configuration is read at the beginning of a routine as an input along with the path of an incoming data file (a CSV file in this case) and a business effective date (which will be used as the `EFF_START_DATE` for new or updated records).\\r\\n\\r\\nProcessing is performed using the specified key and non key attributes and the output datasets (current and historical) are written to columnar storage files (parquet in this case). This is designed to make subsequent access and processing more efficient.\\r\\n\\r\\n### The Algorithm\\r\\n\\r\\nI have broken the process into stages as follows:\\r\\n\\r\\n#### Stage 1 \u2013 Type Cast and Hash Incoming Data\\r\\n\\r\\nThe first step is to create deterministic hashes of the configured key and non key values for incoming data. The hashes are calculated based upon a list of elements representing the key and non key values using the MD5 algorithm. The hashes for each record are then stored with the respective record. Furthermore, the fields are casted their target datatype as specified in the configuration. Both of these operations can be performed in a single pass of each row using a `map()` operation.\\r\\n\\r\\nImportantly we only calculate hashes once upon arrival of new data, as the hashes are persisted for the life of the data \u2013 and the data structures are immutable \u2013 the hashes should never change or be invalidated.\\r\\n\\r\\n#### Stage 2 \u2013 Determine INSERTs\\r\\n\\r\\nWe now compare Incoming Hashes with previously calculated hash values for the (previous day\u2019s) current object. If no current object exists for the dataset, then it can be assumed this is a first run. In this case every record is considered as an `INSERT` with an `EFF_START_DATE` of the business effective date supplied.\\r\\n\\r\\nIf there is a current object, then the key and non key hash values (only the hash values) are read from the current object. These are then compared to the respective hashes of the incoming data (which should still be in memory).\\r\\n\\r\\nGiven the full outer join:\\r\\n\\r\\nincoming\\\\_data(keyhash, nonkeyhash) \\r\\nFULL OUTER JOIN  \\r\\ncurrent\\\\_data(keyhash, nonkeyhash) \\r\\nON keyhash\\r\\n\\r\\nKeys which exist in the left entity which do not exist in the right entity must be the results of an INSERT operation.\\r\\n\\r\\nTag these records with an operation of `I` with an `EFF_START_DATE` of the business effective date, then rejoin only these records with their full attribute payload from the incoming dataset. Finally, write out these records to the current and historical partition in `overwrite` mode.\\r\\n\\r\\n#### Stage 3 - Determine DELETEs or Missing Records\\r\\n\\r\\nReferring the previous full outer join operation, keys which exist in the right entity (current object) which do not appear in the left entity (incoming data) will be the result of a (hard) `DELETE` operation if you are processing full snapshots, otherwise if you are processing deltas these would be missing records (possibly because there were no changes at the source).\\r\\n\\r\\nTag these records as `D` or `X` respectively with an `EFF_START_DATE` of the business effective date, rejoin these records with their full attribute payload from the current dataset, then write out these records to the current and historical partition in `append` mode.\\r\\n\\r\\n#### Stage 4 - Determine UPDATEs or Unchanged Records\\r\\n\\r\\nAgain, referring to the previous full outer join, keys which exist in both the incoming and current datasets must be either the result of an `UPDATE` or they could be unchanged. To determine which case they fall under, compare the non key hashes. If the non key hashes differ, it must have been a result of an `UPDATE` operation at the source, otherwise the record would be unchanged.\\r\\n\\r\\nTag these records as `U` or `N` respectively with an `EFF_START_DATE` of the business effective date (in the case of an update - otherwise maintain the current `EFF_START_DATE`), rejoin these records with their full attribute payload from the incoming dataset, then write out these records to the current and historical partition in `append` mode.\\r\\n\\r\\n### Key Callouts\\r\\n\\r\\nA summary of the key callouts from this pattern are:\\r\\n\\r\\n- Use the RDD API for iterative record operations (such as type casting and hashing)\\r\\n- Persist hashes with the records\\r\\n- Use Dataframes for `JOIN` operations\\r\\n- Only perform `JOIN`s with the `keyhash` and `nonkeyhash` columns \u2013 this minimizes the amount of data shuffled across the network\\r\\n- Write output data in columnar (Parquet) format\\r\\n- Break the routine into stages, covering each operation, culminating with a `saveAsParquet()` action \u2013 this may seem expensive but for large datsets it is more efficient to break down DAGs for each operation\\r\\n- Use caching for objects which will be reused between actions\\r\\n\\r\\n#### Metastore Integration\\r\\n\\r\\nAlthough I did not include this in my example, you could easily integrate this pattern with a metastore (such as a Hive metastore or AWS Glue Catalog), by using table objects and `ALTER TABLE` statements to add historical partitions.\\r\\n\\r\\n#### Further optimisations\\r\\n\\r\\nIf the incoming data is known to be relatively small (in the case of delta processing for instance), you could consider a broadcast join where the smaller incoming data is distributed to all of the different Executors hosting partitions from the current dataset.\\r\\n\\r\\nAlso you could add a key to the column config to configure a column to be nullable or not.\\r\\n\\r\\nHappy CDCing!\\r\\n\\r\\n> Full source code for this article can be found at: [https://github.com/avensolutions/cdc-at-scale-using-spark](https://github.com/avensolutions/cdc-at-scale-using-spark)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"synthetic-cdc-data-generator","metadata":{"permalink":"/synthetic-cdc-data-generator","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-06-28-synthetic-cdc-data-generator/index.md","source":"@site/blog/2019-06-28-synthetic-cdc-data-generator/index.md","title":"Synthetic CDC Data Generator","description":"This is a simple routine to generate random data with a configurable number or records, key fields and non key fields to be used to create synthetic data for source change data capture (CDC) processing. The output includes an initial directory containing CSV files representing an initial data load, and an incremental directory containing CSV files representing incremental data.","date":"2019-06-28T00:00:00.000Z","formattedDate":"June 28, 2019","tags":[{"label":"cdc","permalink":"/tags/cdc"},{"label":"python","permalink":"/tags/python"},{"label":"spark","permalink":"/tags/spark"}],"readingTime":1.73,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"synthetic-cdc-data-generator","title":"Synthetic CDC Data Generator","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"/img/fullstackchronicles-cover-image.png","tags":["cdc","python","spark"],"keywords":["cdc","python","spark"]},"prevItem":{"title":"Change Data Capture at Scale using Spark","permalink":"/change-data-capture-at-scale-using-spark"},"nextItem":{"title":"Scalable, Secure Application Load Balancing with VPC Native GKE and Istio","permalink":"/scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\nThis is a simple routine to generate random data with a configurable number or records, key fields and non key fields to be used to create synthetic data for source change data capture (CDC) processing. The output includes an initial directory containing CSV files representing an initial data load, and an incremental directory containing CSV files representing incremental data.\\r\\n\\r\\nSpark Training Courses from the AlphaZetta Academy\\r\\n\\r\\n[Data Transformation and Analysis Using Apache Spark](https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/)  \\r\\n[Stream and Event Processing using Apache Spark](https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/)  \\r\\n[Advanced Analytics Using Apache Spark](https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/)\\r\\n\\r\\nArguments (by position) include:\\r\\n\\r\\n- `no_init_recs` : the number of initial records to generate\\r\\n- `no_incr_recs` : the number of incremental records on the second run - should be `>= no_init_recs`\\r\\n- `no_keys` : number of key columns in the dataset \u2013 keys are generated as UUIDs\\r\\n- `no_nonkeys` : number of non-key columns in the dataset \u2013 nonkey values are generated as random numbers\\r\\n- `pct_del` : percentage of initial records deleted on the second run - between 0.0 and 1.0\\r\\n- `pct_upd` : percentage of initial records updated on the second run - between 0.0 and 1.0\\r\\n- `pct_unchanged` : percentage of records unchanged on the second run - between 0.0 and 1.0\\r\\n- `initial_output` : folder for initial output in CSV format\\r\\n- `incremental_output` : folder for incremental output in CSV format\\r\\n\\r\\nNOTE : `pct_del` + `pct_upd` + `pct_unchanged` must equal 1.0\\r\\n\\r\\nExample usage:\\r\\n\\r\\n```\\r\\n$ spark-submit synthetic-cdc-data-generator.py 100000 100000 2 3 0.2 0.4 0.4 data/day1 data/day2\\r\\n```\\r\\n\\r\\nExample output from the **_day1_** run for the above configuration would look like this:\\r\\n\\r\\n<Gist id=\\"befb034da2b4f25a1dbbc0e9b4b8eef6\\" \\r\\n/>\\r\\n\\r\\nNote that this routine can be run subsequent times producing different key and non key values each time, as the keys are UUIDs and the values are random numbers.\\r\\n\\r\\nWe will use this application to generate random input data to demonstrate CDC using Spark in a subsequent post, see you soon!\\r\\n\\r\\n> Full source code can be found at: [https://github.com/avensolutions/synthetic-cdc-data-generator](https://github.com/avensolutions/synthetic-cdc-data-generator)\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio","metadata":{"permalink":"/scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-05-18-scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio/index.md","source":"@site/blog/2019-05-18-scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio/index.md","title":"Scalable, Secure Application Load Balancing with VPC Native GKE and Istio","description":"istio","date":"2019-05-18T00:00:00.000Z","formattedDate":"May 18, 2019","tags":[{"label":"cloud","permalink":"/tags/cloud"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"istio","permalink":"/tags/istio"},{"label":"load-balancing","permalink":"/tags/load-balancing"},{"label":"vpc-native","permalink":"/tags/vpc-native"}],"readingTime":2.955,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio","title":"Scalable, Secure Application Load Balancing with VPC Native GKE and Istio","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/istio-blog-feature-image.png","tags":["cloud","gcp","google-cloud-platform","istio","load-balancing","vpc-native"],"keywords":["cloud","gcp","google-cloud-platform","istio","load-balancing","vpc-native"]},"prevItem":{"title":"Synthetic CDC Data Generator","permalink":"/synthetic-cdc-data-generator"},"nextItem":{"title":"AWS Professional and Speciality Exam Tips","permalink":"/aws-professional-and-speciality-exam-tips"}},"content":"![istio](images/istio-blog-feature-image.png)\\r\\n\\r\\nAt the time of this writing, GCP does not have a generally available non-public facing Layer 7 load balancer. While this is sure to change in the future, this article outlines a design pattern which has been proven to provide scalable and extensible application load balancing services for multiple applications running in Kubernetes pods on GKE.\\r\\n\\r\\nWhen you create a service of type LoadBalancer in GKE, Kubernetes hooks into the provider (GCP in this case) on your behalf to create a Google Load Balancer, while this may be specified as INTERNAL, there are two issues:\\r\\n\\r\\n### Issue #1:\\r\\n\\r\\nThe GCP load balancer created for you is a Layer 4 TCP load balancer.\\r\\n\\r\\n### Issue #2:\\r\\n\\r\\nThe normal behaviour is for Google to enumerate all of the node pools in your GKE cluster and \u201cautomagically\u201d create mapping GCE instance groups for each node pool for each zone the instances are deployed in. This means the entire surface area of your cluster is exposed to the external network \u2013 which may not be optimal for internal applications on a multi tenanted cluster.\\r\\n\\r\\n### The Solution:\\r\\n\\r\\nUsing [Istio](https://istio.io/) deployed on GKE along with the [Istio Ingress Gateway](https://istio.io/docs/concepts/traffic-management/#ingress-and-egress) along with an externally created load balancer, it is possible to get scalable HTTP load balancing along with all the normal ALB goodness (stickiness, path-based routing, host-based routing, health checks, TLS offload, etc.).\\r\\n\\r\\nAn abstract depiction of this architecture is shown here:\\r\\n\\r\\n[![Istio Ingress Design Pattern for VPC Native GKE Clusters](images/istio-ingress-blog.png)](images/istio-ingress-blog.png)\\r\\n\\r\\nThis can be deployed with a combination of [Terraform](https://www.terraform.io/) and kubectl. The steps to deploy at a high level are:\\r\\n\\r\\n1. Create a GKE cluster with at least two node pools: ingress-nodepool and service-nodepool. Ideally create these node pools as multi-zonal for availability. You could create additional node pools for your Egress Gateway or an operations-nodepool to host Istio, etc as well.\\r\\n2. Deploy Istio.\\r\\n3. Deploy the Istio Ingress Gateway service on the ingress-nodepool using Service type NodePort.\\r\\n4. Create an associated Certificate Gateway using server certificates and private keys for TLS offload.\\r\\n5. Create a service in the service-nodepool.\\r\\n6. Reserve an unallocated static IP address from the node network range.\\r\\n7. [Create an internal TCP load balancer](https://cloud.google.com/load-balancing/docs/internal/setting-up-internal):\\r\\n    1. Specify the frontend as the IP address reserved in step 6.\\r\\n    2. Specify the backend as the managed instance groups created during the node pool creation for the ingress-nodepool (ingress-nodepool-ig-a, ingress-nodepool-ig-b, ingress-nodepool-ig-c).\\r\\n    3. Specify ports 80 and 443.\\r\\n8. Create a GCP Firewall Rule to allow traffic from authorized sources (network tags or CIDR ranges) to a target of the ingress-nodepool network tag.\\r\\n9. Create a Cloud DNS A Record for your managed zone as \\\\*.namespace.zone pointing to the IP Address assigned to the load balancer frontend in step 7.1.\\r\\n10. [Enable Health Checks through the GCP firewall](https://cloud.google.com/load-balancing/docs/health-checks#firewall_rules) to reach the ingress-nodepool network tag at a minimum \u2013 however there is no harm in allowing these to all node pools.\\r\\n\\r\\nThe service should then be resolvable and routable from authorized internal networks (peered private VPCs or internal networks connected via VPN or Dedicated Interconnect) as:\\r\\n\\r\\n> https://_service__.__namespace__.__zone__/__endpoint__\\r\\n\\r\\n### The advantages of this design pattern are...\\r\\n\\r\\n1. The Ingress Gateway provides fully functional application load balancing services.\\r\\n2. Istio provides service discovery and routing using names and namespaces.\\r\\n3. The Ingress Gateway service and ingress gateway node pool can be scaled as required to meet demand.\\r\\n4. The Ingress Gateway is multi zonal for greater availability\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"aws-professional-and-speciality-exam-tips","metadata":{"permalink":"/aws-professional-and-speciality-exam-tips","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-04-04-aws-professional-and-speciality-exam-tips/index.md","source":"@site/blog/2019-04-04-aws-professional-and-speciality-exam-tips/index.md","title":"AWS Professional and Speciality Exam Tips","description":"AWS pro and specialty certs","date":"2019-04-04T00:00:00.000Z","formattedDate":"April 4, 2019","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"certification","permalink":"/tags/certification"}],"readingTime":4.77,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"aws-professional-and-speciality-exam-tips","title":"AWS Professional and Speciality Exam Tips","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/aws-pro-and-specialty-certs.png","tags":["aws","certification"],"keywords":["aws","certification"]},"prevItem":{"title":"Scalable, Secure Application Load Balancing with VPC Native GKE and Istio","permalink":"/scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio"},"nextItem":{"title":"GCP Networking for AWS Professionals","permalink":"/gcp-networking-for-aws-professionals"}},"content":"![AWS pro and specialty certs](images/aws-pro-and-specialty-certs.png)\\r\\n\\r\\nOne you get beyond the Associate level AWS certification exams into the Professional or Speciality track exams the degree of difficulty rises significantly. As a veteran of the Certified Solutions Architect Professional and Big Data Specialty exams, I thought I would share my experiences which I believe are applicable to all the certification streams and tracks in the AWS certification program.\\r\\n\\r\\nFirst off let me say that I am a self-professed certification addict, having sat more than thirty technical certification exams over my thirty plus year career in technology including certification and re-certification exams. I would put the AWS professional and specialty exams right up there in terms of their level of difficulty.\\r\\n\\r\\nThe AWS Professional and Specialty exams are specifically designed to be challenging. Although they have removed the pre-requisites for these exams (much to my dismay\u2026), you really need to be prepared for these exams otherwise you are throwing your hard-earned money away.  \\r\\n\\r\\nThere are very few - if any - \u201ceasy\u201d questions. All of the questions are scenario based and require you to design a solution to meet multiple requirements. The question and/or the correct answer will invariably involve the use of multiple AWS services (not just one). You will be tested on your reading comprehension, time management and ability to cope under pressure as well as being tested on your knowledge of the AWS platform.  \\r\\n\\r\\nThe following sections provide some general tips which will help you approach the exam and give you the best chance of success on the day. This is not a brain dump or a substitute for the hard work and dedication required to ensure success on your exam day.\\r\\n\\r\\n## Time Management\\r\\n\\r\\nNeedless to say, your ability to manage time is critical, on average you will have approximately 2-3 minutes to answer each question. Reading the questions and answers carefully may take up 1-2 minutes on its own. If the answer is not apparent to you, you are best to mark the question and come back to it at the end of the exam.  \\r\\n\\r\\nIn many cases there may be subsequent questions and answer sets which jog your memory or help you deduce the correct answers to the questions you initial passed on. For instance, you may see references in future questions which put context around services you may not be completely familiar with, this may enable you to answer flagged questions with more confidence.  \\r\\n\\r\\nOf course, you must answer all questions before completing the exam, there are no points for incomplete or unattempted answers.\\r\\n\\r\\n## Recommended Approach to each Question\\r\\n\\r\\nMost of the questions on the Professional or Specialty certification exams fall into one of three categories:\\r\\n\\r\\n- Short-ish question, multiple long detailed answer options\\r\\n- Long-ish scenario question, multiple relatively short answer options\\r\\n- Long-ish question with multiple relatively long, detailed answers\\r\\n\\r\\nThe latter scenario is thankfully less common. However, in all cases it is important to read the last sentence in the question first, this will provide indicators to help you read through the question in its entirety and all of the possible answers with a clear understanding of what is _\u201creally\u201d_ being asked. For instance, the operative phrase may be _\u201chighly available\u201d_ or _\u201cmost cost effective\u201d_.\\r\\n\\r\\nTry to eliminate answers based on what you know, for instance answers with erroneous instance families can be eliminated immediately. This will give you a much better statistical chance of success, even if you have to venture an educated guess in the end.\\r\\n\\r\\n## The Most Complicated Solution is Probably Not the Correct One\\r\\n\\r\\nIn many answer sets to questions on the Professional or Specialty exams you will see some ridiculously complicated solution approaches, these are most often incorrect answers. Although there may be enough loosely relevant terminology or services to appear reasonable.\\r\\n\\r\\nNote the following statement direct from the AWS Certified Solutions Architect Professional Exam Blueprint:\\r\\n\\r\\n>   \\r\\n> \u201cDistractors, or incorrect answers, are response options that an examinee with incomplete knowledge or skill would likely choose. However, they are generally plausible responses that fit in the content area defined by the test objective.\u201d\\r\\n\\r\\nAWS wants professionals who design and implement solutions which are simple, sustainable, highly available, scalable and cost effective. One of the key Amazon Leadership Principles is _\u201cInvent and Simplify\u201d_, simplify is often the operative word.\\r\\n\\r\\n## Don\u2019t spend time on dumps or practice exams (other than those from AWS)\\r\\n\\r\\nThe question pools for AWS exams are enormous, the chances of you getting the same questions and answer sets as someone else are slim. Furthermore, non-AWS sources may not be trustworthy. There is no substitute to AWS white papers, how to\u2019s, and real-life application of your learnings.\\r\\n\\r\\n## Don\u2019t focus on Service Limits or Calculations\\r\\n\\r\\nIn my experiences with AWS exams, they are not overly concerned with service limits, default values, formulas (e.g. the formula to calculate required partitions for a DynamoDB table) or syntax - so don\u2019t waste time remembering them. You should however understand the 7 layer OSI model and be able to read and interpret CIDR notation.\\r\\n\\r\\nMainly, however, they want you to understand how services work together in an AWS solution to achieve an outcome for a customer.\\r\\n\\r\\n## Some Final Words of Advice\\r\\n\\r\\n**Always do what you think AWS would want you to do!**\xa0\\r\\n\\r\\nIt is worthwhile having a quick look at the [AWS Leadership Principles](https://blog.aboutamazon.com.au/amazon-in-australia/our-leadership-principles) (I have already referenced one of these in this article) as these are applied religiously in every aspect of the AWS business.\xa0 In particular, you should pay specific attention to the principals around simplicity and frugality.\\r\\n\\r\\n**Good luck!**\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"gcp-networking-for-aws-professionals","metadata":{"permalink":"/gcp-networking-for-aws-professionals","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-02-21-gcp-networking-for-aws-professionals/index.md","source":"@site/blog/2019-02-21-gcp-networking-for-aws-professionals/index.md","title":"GCP Networking for AWS Professionals","description":"A primer on GCP networking for AWS engineers and architects","date":"2019-02-21T00:00:00.000Z","formattedDate":"February 21, 2019","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"cloud","permalink":"/tags/cloud"},{"label":"gcp","permalink":"/tags/gcp"},{"label":"google-cloud-platform","permalink":"/tags/google-cloud-platform"},{"label":"networking","permalink":"/tags/networking"}],"readingTime":4.05,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"gcp-networking-for-aws-professionals","title":"GCP Networking for AWS Professionals","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"description":"A primer on GCP networking for AWS engineers and architects","image":"images/gcp-aws-networking.png","tags":["aws","cloud","gcp","google-cloud-platform","networking"],"keywords":["aws","cloud","gcp","google-cloud-platform","networking"]},"prevItem":{"title":"AWS Professional and Speciality Exam Tips","permalink":"/aws-professional-and-speciality-exam-tips"},"nextItem":{"title":"The Streaming Data Warehouse","permalink":"/the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined"}},"content":"![GCP AWS Networking](images/gcp-aws-networking.png)\\r\\n\\r\\nGCP and AWS share many similarities, they both provide similar services and both leverage containerization, virtualization and software defined networking.\\r\\n\\r\\nThere are some significant differences when it comes to their respective implementations, networking is a key example of this.\\r\\n\\r\\nBefore we compare and contrast the two different approaches to networking, it is worthwhile noting the genesis of the two major cloud providers.\\r\\n\\r\\n#### _Google was born to be global, Amazon became global_\\r\\n\\r\\nBy no means am I suggesting that Amazon didn\'t have designs on going global from it\'s beginnings, but AWS was driven (entirely at the beginning) by the needs of the Amazon eCommerce business. Amazon started in the US before expanding into other regions (such as Europe and Australia). In some cases the expansion took decades (Amazon only entered Australia as a retailer in 2018).\\r\\n\\r\\nGoogle, by contrast, was providing application, search and marketing services worldwide from its very beginning. GCP which was used as the vector to deliver these services and applications was architected around this global model, even though their actual data centre expansion may not have been as rapid as AWS\u2019s (for example GCP opened its Australia region 5 years after AWS).\\r\\n\\r\\nTheir respective networking implementations reflect how their respective companies evolved.\\r\\n\\r\\n#### _AWS is a leader in IaaS, GCP is a leader in PaaS_\\r\\n\\r\\nThis is only an opinion and may be argued, however if you look at the chronology of the two platforms, consider this:\\r\\n\\r\\n- The first services released by AWS (simultaneously for all intents and purposes) were S3, SQS and EC2\\r\\n- The first service released by Google was AppEngine (a pure PaaS offering)\\r\\n\\r\\nGoogle has launched and matured their IaaS offerings since as AWS has done similarly with their PaaS offerings, but they started from much different places.\\r\\n\\r\\nWith all of that said, here are the key differences when it comes to networking between the two major cloud providers:\\r\\n\\r\\n### GCP VPCs are Global by default, AWS VPCs are Regional only\\r\\n\\r\\nThis is the first fundamental difference between the two providers. Each GCP project is allocated one VPC network with Subnets in each of the 18 GCP Regions. Whereas each AWS Account is allocated one Default VPC in each AWS Region with a Subnet in each AWS Availability Zone for that Region, that is each account has 17 VPCs in each of the 17 Regions (excluding GovCloud regions).\\r\\n\\r\\n[![Default Global VPC Network in GCP](images/gcp-default-network.png)](images/gcp-default-network.png)\\r\\n\\r\\nIt is entirely possible to create VPCs in GCP which are Regional, but they are Global by default.\\r\\n\\r\\nThis global tenancy can be advantageous in many cases, but can be limiting in others, for instance there is a limit of 25 peering connections to any one VPC, the limit in AWS is 125.\\r\\n\\r\\n### GCP Subnets are Regional, AWS Subnets are Zonal\\r\\n\\r\\nSubnets in GCP automatically span all Zones in a Region, whereas AWS VPC Subnets are assigned to Availability Zones in a Region. This means you are abstracted from some of the networking and zonal complexity, but you have less control over specific network placement of instances and endpoints. You can infer from this design that Zones are replicated or synchronised within a Region, making them less of a direct consideration for High Availability (or at least as much or your concern as they otherwise would be).\\r\\n\\r\\n### All GCP Firewall Rules are Stateful\\r\\n\\r\\nAWS Security Groups are stateful firewall rules \u2013 meaning they maintain connection state for inbound connections, AWS also has Network ACLs (NACLs) which are stateless firewall rules. GCP has no direct equivalent of NACLs, however GCP Firewall Rules are more configurable than their AWS counterparts. For instance, GCP Firewall Rules can include Deny actions which is not an option with AWS Security Group Rules.\\r\\n\\r\\n### Load Balancers in GCP are layer 4 (TCP/UDP) unless they are public facing\\r\\n\\r\\nAWS Application Load Balancers can be deployed in private VPCs with no external IPs attached to them. GCP has Application Load Balancers (Layer 7 load balancers) but only for public facing applications, internal facing load balancers in GCP are Network Load Balancers. This presents some challenges with application level load balancing functionality such as stickiness. There are potential workarounds however such as NGINX in GKE behind\\r\\n\\r\\n### Firewall rules are at the Network Level not at the Instance or Service Level\\r\\n\\r\\nThere are simple firewall settings available at the instance level, these are limited to allowing HTTP and HTTPS traffic to the instance only and don\u2019t allow you to specify sources. Detailed Firewall Rules are set at the GCP VPC Network level and are not attached or associated with instances as they are in AWS.\\r\\n\\r\\n_Hopefully this is helpful for AWS engineers and architects being exposed to GCP for the first time!_\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined","metadata":{"permalink":"/the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-02-14-the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined/index.md","source":"@site/blog/2019-02-14-the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined/index.md","title":"The Streaming Data Warehouse","description":"Kappa Architecture and Data Warehousing re-imagined","date":"2019-02-14T00:00:00.000Z","formattedDate":"February 14, 2019","tags":[{"label":"data-warehousing","permalink":"/tags/data-warehousing"},{"label":"kafka","permalink":"/tags/kafka"},{"label":"kappa-architecture","permalink":"/tags/kappa-architecture"},{"label":"stream-processing","permalink":"/tags/stream-processing"},{"label":"streaming-analytics","permalink":"/tags/streaming-analytics"}],"readingTime":3.525,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined","title":"The Streaming Data Warehouse","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"image":"images/sdw.png","tags":["data-warehousing","kafka","kappa-architecture","stream-processing","streaming-analytics"],"keywords":["data-warehousing","kafka","kappa-architecture","stream-processing","streaming-analytics"]},"prevItem":{"title":"GCP Networking for AWS Professionals","permalink":"/gcp-networking-for-aws-professionals"},"nextItem":{"title":"Test Driven Infrastructure and Test Automation with Ansible, Molecule and Azure","permalink":"/test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure"}},"content":"### Kappa Architecture and Data Warehousing re-imagined\\r\\n\\r\\n![Streaming Data Warehouse](images/sdw.png)\\r\\n\\r\\nThe aspiration to extend data analysis (predictive, descriptive or otherwise) to streaming event data has been common across every enterprise scale program I have been involved with. Often, however, this aspiration goes unrealised as it tends to slide down the priority scale as we still grapple with legacy batch oriented integration patterns and processes.\\r\\n\\r\\nEvent processing is not a new concept, real time event and transaction processing has been a standard feature for security, digital and operations functions for some time, however in the Data Warehousing, BI and Advanced Analytics worlds it is often spoken about but rarely implemented, except for tech companies of course. In many cases personalization is still a batch oriented process, e.g. train a model from a feature set built from historical data, generate recommendations in batch, serve these recommendations upon the next visit - wash, rinse, and repeat.\\r\\n\\r\\nLambda has existed for several years now as a data-processing architecture pattern designed to incorporate both batch and stream-processing capabilities. Moreover, messaging platforms have existed for decades, from point-to-point messaging systems, to message-oriented-middleware systems, to distributed pub-sub messaging systems such as Apache Kafka.\\r\\n\\r\\nAdditionally, open source streaming data processing frameworks and tools have proliferated in recent years with projects such as Storm, Samza, Flink and Spark Streaming becoming established solutions.\\r\\n\\r\\nKafka in particular, with its focus on durability, resiliency, availability and consistency, has graduated into fully fledged data platform **not simply a transient messaging system**. In many cases Kafka is serving as a back end for operational processes, such as applications implementing the CQRS (Command Query Responsibility Segregation) design pattern.  \\r\\n\\r\\nIn other words, it is not the technology that holds us back, it\'s our lack of imagination.\\r\\n\\r\\nEnter [Kappa Architecture](http://milinda.pathirage.org/kappa-architecture.com/) where we no longer have to attempt to integrate streaming data with batch processes\u2026**everything is a stream**. The ultimate embodiment of Kappa Architecture is the **_Streaming Data Warehouse_**.\\r\\n\\r\\nIn the Streaming Data Warehouse, tables are represented by topics. Topics represent either:\\r\\n\\r\\n- unbounded event or change streams; or\\r\\n- stateful representations of data (such as master, reference or summary data sets).\\r\\n\\r\\nThis approach makes possible the enrichment and/or summarisation of transaction or event data with master or reference data. Furthermore many of the patterns used in data warehousing and master data management are inherent in Kafka as you can represent the current state of an object as well as the complete change history of that object (in other words change data capture and associated slowly changing dimensions from one inbound stream).\\r\\n\\r\\nData is acquired from source systems either in real time or as a scheduled extract process, **in either case the data is presented to Kafka as a stream**.\\r\\n\\r\\nThe Kafka Avro Schema Registry provides a systematic contract with source systems which also serves as a data dictionary for consumers supporting schema evolution with backward and forward compatibility. Data is retained on the Kafka platform for a designated period of time (days or weeks) where it is available for applications and processes to consume - these processes can include data summarisation or sliding window operations for reporting or notification, or data integration or datamart building processes which sink data to other systems - these could include relational or non-relational data stores.\\r\\n\\r\\nReal time applications can be built using the KStreams API and emerging tools such as KSQL can be used to provide a well-known interface for sampling streaming data or performing windowed processing operations on streams. Structured Streaming in Spark or Spark Streaming in its original RDD/DStream implementation can be used to prepare and enrich data for machine learning operations using Spark ML or Spark MLlib.  \\r\\n\\r\\nIn addition, data sinks can operate concurrently to sink datasets to S3 or Google Cloud Storage or both (multi cloud - like real time analytics - is something which is talked about more than it\u2019s implemented\u2026).\\r\\n\\r\\nIn the Streaming Data Warehouse architecture Kafka is much more than a messaging platform it is a distributed data platform, which could easily replace major components of a legacy (or even a modern) data architecture.  \\r\\n\\r\\nIt just takes a little imagination\u2026\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure","metadata":{"permalink":"/test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-01-31-test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure/index.md","source":"@site/blog/2019-01-31-test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure/index.md","title":"Test Driven Infrastructure and Test Automation with Ansible, Molecule and Azure","description":"Molecule Ansible Azure","date":"2019-01-31T00:00:00.000Z","formattedDate":"January 31, 2019","tags":[{"label":"ansible","permalink":"/tags/ansible"},{"label":"azure","permalink":"/tags/azure"},{"label":"cloud","permalink":"/tags/cloud"},{"label":"infrastructure-code","permalink":"/tags/infrastructure-code"},{"label":"molecule","permalink":"/tags/molecule"},{"label":"python","permalink":"/tags/python"},{"label":"test-automation","permalink":"/tags/test-automation"}],"readingTime":4.465,"hasTruncateMarker":false,"authors":[{"name":"Chris Ottinger","title":"Senior Technologist","url":"https://github.com/datwiz","imageURL":"http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80","key":"chrisottinger"}],"frontMatter":{"slug":"test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure","title":"Test Driven Infrastructure and Test Automation with Ansible, Molecule and Azure","authors":["chrisottinger"],"draft":false,"hide_table_of_contents":true,"image":"images/molecule-ansible-azure.png","tags":["ansible","azure","cloud","infrastructure-code","molecule","python","test-automation"],"keywords":["ansible","azure","cloud","infrastructure-code","molecule","python","test-automation"]},"prevItem":{"title":"The Streaming Data Warehouse","permalink":"/the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined"},"nextItem":{"title":"S3 Object Notifications using Lambda and SES","permalink":"/s3-object-notifications-using-lambda-and-ses"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Molecule Ansible Azure](images/molecule-ansible-azure.png)\\r\\n\\r\\nA few years back, before the rise of the hyper-scalers, I had my first infracode \'aha moment\' with OpenStack. The second came with [Kitchen](https://kitchen.ci/).\\r\\n\\r\\nI had already been using test driven development for application code and configuration automation for infrastructure but Kitchen brought the two together. Kitchen made it possible to write tests, spin up infrastructure, and then tear everything down again - the Red/Green/Refactor cycle for infrastructure. What made this even better was that it wasn\'t a facsimile of a target environment, it was the same - same VM\'s, same OS, same network.\\r\\n\\r\\nComing from a Chef background for configuration automation, Kitchen is a great fit to the Ruby ecosystem. Kitchen works with Ansible and Azure, but a Ruby environment and at least a smattering of Ruby coding skills are required.\\r\\n\\r\\n[Molecule](https://molecule.readthedocs.io/) provides a similar red-green development cycle to Kitchen, but without the need to step outside of the familiar Python environment.\\r\\n\\r\\nOut of the box, Molecule supports development of Ansible roles using either a Docker or Virtual Box infrastructure provider. Molecule also leverages the Ansible drivers for private and public cloud platforms.\\r\\n\\r\\nMolecule can be configured to test an individual role or collections of roles in Ansible playbooks.\\r\\n\\r\\nThis tutorial demonstrates how to use Molecule with Azure to develop and test an individual Ansible role following the red/green/refactor infracode workflow, which can be generalised as:\\r\\n\\r\\n- **Red**\\\\- write a failing infrastructure test\\r\\n- **Green** - write the Ansible tasks needed to pass the test\\r\\n- Refactor - repeat the process\\r\\n\\r\\nThe steps required for this tutorial are as follows:\\r\\n\\r\\n## Azure setup\\r\\n\\r\\nEnsure there is an existing Azure Resource Group that will be used for infracode development and testing. Within the resource group, ensure there is a single virtual network (vnet) with a single subnet. Ansible will use these for the default network setup.\\r\\n\\r\\n## Setup a working environment\\r\\n\\r\\nThere are a number of options for setting up a Python environment for Ansible and Molecule, including Python virtualenv or a Docker container environment.\\r\\n\\r\\n## Create a Docker image for Ansible+Molecule+Azure\\r\\n\\r\\nThis tutorial uses a Docker container environment. A `Dockerfile` for the image can be found in `./molecule-azure-image/Dockerfile`. The image sets up a sane Python3 environment with Ansible, Ansible\\\\[azure\\\\], and Molecule `pip` modules installed.\\r\\n\\r\\n<Gist id=\\"4bd0c2ccae06dcaedffc2d91e594145f\\" \\r\\n/>\\r\\n\\r\\n## Create a Docker workspace\\r\\n\\r\\nSetup a working environment using the Docker image with Ansible, Molecule, and the `azure-cli` installed.\\r\\n\\r\\n<Gist id=\\"f80ef20a720914cfd4e02cf9783fec06\\" \\r\\n/>\\r\\n\\r\\nThis example assumes the following:\\r\\n\\r\\n- a resource group already exists with access rights to create virtual machines; and\\r\\n- the resource group contains a single vnet with a single subnet\\r\\n\\r\\n## Log into an Azure subcription\\r\\n\\r\\nAnsible supports a number of different methods for authenticating with Azure. This example uses the `azure-cli` to login interactively.\\r\\n\\r\\n<Gist id=\\"fd8987e7f724de5393a411c24c74978b\\" \\r\\n/>\\r\\n\\r\\n## Create an empty Ansible role with Molecule\\r\\n\\r\\nMolecule provides an `init` function with defaults for various providers. The molecule-azure-role-template creates an empty role with scaffolding for Azure.\\r\\n\\r\\n<Gist id=\\"f9b301d950a2254ab9af4806f2110544\\" \\r\\n/>\\r\\n\\r\\nCheck that the environment is working by running the following code:\\r\\n\\r\\n<Gist id=\\"d56c3cd1e25b51acc634e5adb8a0a256\\" \\r\\n/>\\r\\n\\r\\nThe output should look be similar to\u2026\\r\\n\\r\\n<Gist id=\\"a3f8aed99a7c910588a5651d8cabf0e8\\" \\r\\n/>\\r\\n\\r\\n## Spin up an Azure VM\\r\\n\\r\\nSpin up a fresh VM to be used for infra-code development.\\r\\n\\r\\n<Gist id=\\"14a621ee65f9c2db583ed5ef94274c71\\" \\r\\n/>\\r\\n\\r\\nMolecule provides a handy option for logging into the new VM:\\r\\n\\r\\n<Gist id=\\"456aa8a8860bf785b382e18ede204d33\\" \\r\\n/>\\r\\n\\r\\nThere is now a fresh Ubuntu 18.04 virtual machine ready for infra-code development. For this example, a basic Nginx server will be installed and verified.\\r\\n\\r\\n## Write a failing test\\r\\n\\r\\n[Testinfra](https://testinfra.readthedocs.io/en/latest/) provides a `pytest` based framework for verifying server and infrastructure configuration. Molecule then manages the execution of those `testinfra` tests. The Molecule template provides a starting point for crafting tests of your own. For this tutorial, installation of the `nginx` service is verified. Modify the tests file using `vi molecule/default/tests/test_default.py`\\r\\n\\r\\n<Gist id=\\"5b22b20a192aecbecb8cc229cb5f2a69\\" \\r\\n/>\\r\\n\\r\\n## Execute the failing test\\r\\n\\r\\nThe Ansible task needed to install and enable `nginx` has not yet been written, so the test should fail:\\r\\n\\r\\n<Gist id=\\"38eb4bb776a41db7aa68f5962a97af62\\" \\r\\n/>\\r\\n\\r\\nIf the initial sample tests in `test_default.py` are kept, then 3 tests should fail and 2 tests should pass.\\r\\n\\r\\n## Write a task to install `nginx`\\r\\n\\r\\nAdd a task to install the `nginx` service using `vi tasks/main.yml`:\\r\\n\\r\\n<Gist id=\\"40d884f0c3a39fc4b3e921d451d60358\\" \\r\\n/>\\r\\n\\r\\n## Apply the role\\r\\n\\r\\nApply the role to the instance created using Molecule.\\r\\n\\r\\n<Gist id=\\"5787aee41e2e3e9373f656677567ae41\\" \\r\\n/>\\r\\n\\r\\nThe `nginx` package should now be installed, both enabled and started, and listening on port 80. Note that the `nginx` instance will not be accessible from the Internet due to the Azure network security rules. The `nginx` instance can be confirmed manually by logging into the instance and using `curl` to make a request to the `nginx` service.\\r\\n\\r\\n<Gist id=\\"fb02518e7129bf28e27822c42221f706\\" \\r\\n/>\\r\\n\\r\\n## Execute the passing test\\r\\n\\r\\nAfter applying the Ansible task to the instance, the `testinfra` tests should now pass.\\r\\n\\r\\n<Gist id=\\"b6359519ca6068615f8f1473636f90ea\\" \\r\\n/>\\r\\n\\r\\n## Cleanup\\r\\n\\r\\nNow that the Ansible role works as defined in the test specification, the development environment can be cleaned up.\\r\\n\\r\\n<Gist id=\\"150971a02b3f4b2c65d551cb09a203d0\\" \\r\\n/>\\r\\n\\r\\nMolecule removes the Azure resources created to develop and test the configuration role. Note that deletion may take a few minutes.\\r\\n\\r\\nFinally, once you are done, exit the container environment. If the container was started with the `--rm` switch, the container will also be removed, leaving you with a clean workspace and newly minted Ansible role with automated test cases.\\r\\n\\r\\n<Gist id=\\"4fbb00b116b1a389b0343f6424b19a1b\\" \\r\\n/>"},{"id":"s3-object-notifications-using-lambda-and-ses","metadata":{"permalink":"/s3-object-notifications-using-lambda-and-ses","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-01-18-s3-object-notifications-using-lambda-and-ses/index.md","source":"@site/blog/2019-01-18-s3-object-notifications-using-lambda-and-ses/index.md","title":"S3 Object Notifications using Lambda and SES","description":"Simple pattern for formatted emails from S3 object notifications using AWS Lambda and SES, built with Terraform and Python","date":"2019-01-18T00:00:00.000Z","formattedDate":"January 18, 2019","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"boto3","permalink":"/tags/boto-3"},{"label":"lambda","permalink":"/tags/lambda"},{"label":"python","permalink":"/tags/python"},{"label":"s3","permalink":"/tags/s-3"},{"label":"ses","permalink":"/tags/ses"},{"label":"terraform","permalink":"/tags/terraform"}],"readingTime":2.15,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"s3-object-notifications-using-lambda-and-ses","title":"S3 Object Notifications using Lambda and SES","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"description":"Simple pattern for formatted emails from S3 object notifications using AWS Lambda and SES, built with Terraform and Python","image":"images/s3-object-notifications-using-Lambda-and-SES-with-Terraform.png","tags":["aws","boto3","lambda","python","s3","ses","terraform"],"keywords":["aws","boto3","lambda","python","s3","ses","terraform"]},"prevItem":{"title":"Test Driven Infrastructure and Test Automation with Ansible, Molecule and Azure","permalink":"/test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure"},"nextItem":{"title":"Infrastructure Automation using AWS Lambda","permalink":"/infrastructure-automation-using-aws-lambda"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![S3 object notifications using Lambda and SES with Terraform](images/s3-object-notifications-using-Lambda-and-SES-with-Terraform.png)\\r\\n\\r\\nFollowing on from the previous post in the Really Simple Terraform series [simple-lambda-ec2-scheduler](https://cloudywithachanceofbigdata.com/really-simple-terraform-infrastructure-automation-using-aws-lambda/), where we used Terraform to deploy a Lambda function including the packaging of the Python function into a ZIP archive and creation of all supporting objects (roles, policies, permissions, etc) \u2013 in this post we will take things a step further by using templating to update parameters in the Lambda function code before the packaging and creation of the Lambda function.\\r\\n\\r\\nS3 event notifications can be published directly to an SNS topic which you could create an email subscription, this is quite straightforward. However the email notifications you get look something like this:\\r\\n\\r\\n![Email Notification sent via an SNS Topic Subscription](images/sns-object-notification-email.png)\\r\\n\\r\\nThere is very little you can do about this.\\r\\n\\r\\nHowever if you take a slightly different approach by triggering a Lambda function to send an email via SES you have much more control over content and formatting. Using this approach you could get an email notification that looks like this:\\r\\n\\r\\n![Email Notification sent using Lambda and SES](images/ses-object-notification-email.png)\\r\\n\\r\\nMuch easier on the eye!\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\nYou will need verified AWS SES (Simple Email Service) email addresses for the sender and recipient\u2019s addresses used for your object notification emails. This can be done via the console as shown here:\\r\\n\\r\\n![SES Email Address Verification](images/ses-verify.png)\\r\\n\\r\\n_Note that SES is not available in every AWS region, pick one that is generally closest to your particular reason (but it really doesn\'t matter for this purpose)._\\r\\n\\r\\n## Deployment\\r\\n\\r\\nThe Terraform module creates an IAM Role and associated policy for the Lambda function as shown here:\\r\\n\\r\\n<Gist id=\\"023fab404c0df759d6d1d4bdb02ab4e8\\" \\r\\n/>\\r\\n\\r\\nVariables in the module are substituted into the function code template, the rendered template file is then packaged as a ZIP archive to be uploaded as the Lambda function source as shown here:\\r\\n\\r\\n<Gist id=\\"7d72d8c67114a9df0af1528a3b754d9e\\" \\r\\n/>\\r\\n\\r\\n_As in the previous post, I will reiterate that although Terraform is technically not a build tool, it can be used for simple build operations such as this._\\r\\n\\r\\nThe Lambda function is deployed using the following code:\\r\\n\\r\\n<Gist id=\\"5e7f2a238e8e0270cd55def40a389903\\" \\r\\n/>\\r\\n\\r\\nFinally the S3 object notification events are configured as shown here:\\r\\n\\r\\n<Gist id=\\"e7de65f20c79e0efb115024597864a75\\" \\r\\n/>\\r\\n\\r\\nUse the following commands to run this example (I have created a default credentials profile, but you could supply your API credentials directly, use STS, etc):\\r\\n\\r\\n```\\r\\ncd simple-notifications-with-lambda-and-ses\\r\\nterraform init\\r\\nterraform apply\\r\\n```\\r\\n\\r\\n> *Full source code can be found at: [__https://github.com/avensolutions/simple-notifications-with-lambda-and-ses__](https://github.com/avensolutions/simple-notifications-with-lambda-and-ses)*\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"infrastructure-automation-using-aws-lambda","metadata":{"permalink":"/infrastructure-automation-using-aws-lambda","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-01-15-infrastructure-automation-using-aws-lambda/index.md","source":"@site/blog/2019-01-15-infrastructure-automation-using-aws-lambda/index.md","title":"Infrastructure Automation using AWS Lambda","description":"Simple pattern for automating EC2 tasks using AWS Lambda and Terraform","date":"2019-01-15T00:00:00.000Z","formattedDate":"January 15, 2019","tags":[{"label":"aws","permalink":"/tags/aws"},{"label":"automation","permalink":"/tags/automation"},{"label":"boto3","permalink":"/tags/boto-3"},{"label":"cloudwatch","permalink":"/tags/cloudwatch"},{"label":"ec2","permalink":"/tags/ec-2"},{"label":"lambda","permalink":"/tags/lambda"},{"label":"python","permalink":"/tags/python"},{"label":"scheduled-tasks","permalink":"/tags/scheduled-tasks"},{"label":"terraform","permalink":"/tags/terraform"}],"readingTime":2.035,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"infrastructure-automation-using-aws-lambda","title":"Infrastructure Automation using AWS Lambda","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"description":"Simple pattern for automating EC2 tasks using AWS Lambda and Terraform","image":"images/automate-infrastructure-tasks-using-lambda-with-terraform.png","tags":["aws","automation","boto3","cloudwatch","ec2","lambda","python","scheduled-tasks","terraform"],"keywords":["aws","automation","boto3","cloudwatch","ec2","lambda","python","scheduled-tasks","terraform"]},"prevItem":{"title":"S3 Object Notifications using Lambda and SES","permalink":"/s3-object-notifications-using-lambda-and-ses"},"nextItem":{"title":"Multi Stage ETL Framework using Spark SQL","permalink":"/multi-stage-etl-framework-using-spark-sql"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Automate infrastructure tasks using Lambda with Terraform](images/automate-infrastructure-tasks-using-lambda-with-terraform.png)\\r\\n\\r\\nThere are many other blog posts and examples available for either scheduling infrastructure tasks such as the starting or stopping of EC2 instances; or deploying a Lambda function using Terraform. However, I have found many of the other examples to be unnecessarily complicated, so I have put together a very simple example doing both.\\r\\n\\r\\nThe function itself could be easily adapted to take other actions including interacting with other AWS services using the boto3 library (the Python AWS SDK). The data payload could be modified to pass different data to the function as well.\\r\\n\\r\\nThe script only requires input variables for **_schedule\\\\_expression_** (cron schedule based upon GMT for triggering the function \u2013 could also be expressed as a rate, e.g. **_rate(5 minutes))_** and **_environment_** (value passed to the function on each invocation). In this example the Input data is the value for the \u201cEnvironment\u201d key for an EC2 instance tag \u2013 a user defined tag to associate the instance to a particular environment (e.g. Dev, Test. Prod). The key could be changed as required, for instance if you wanted to stop instances based upon their given name or part thereof you could change the tag key to be \u201cName\u201d.\\r\\n\\r\\nWhen triggered, the function will stop all running EC2 instances with the given Environment tag.\\r\\n\\r\\nThe Terraform script creates:\\r\\n\\r\\n- an IAM Role and associated policy for the Lambda Function\\r\\n- the Lambda function\\r\\n- a Cloudwatch event rule and trigger\\r\\n\\r\\nThe IAM role and policies required for the Lambda function are deployed as shown here:\\r\\n\\r\\n<Gist id=\\"6b8ed7c149a60e823361ee282615b826\\" \\r\\n/>\\r\\n\\r\\nThe function source code is packaged into a ZIP archive and deployed using Terraform as follows:\\r\\n\\r\\n<Gist id=\\"ca6a26a62302ff809eae028bbfb28b41\\" \\r\\n/>\\r\\n\\r\\nAdmittedly Terraform is an infrastructure automation tool and not a build/packaging tool (such as Jenkins, etc), but in this case the packaging only involves zipping up the function source code, so Terraform can be used as a \u2018one stop shop\u2019 to keep things simple.\\r\\n\\r\\nThe Cloudwatch schedule trigger is deployed as follows:\\r\\n\\r\\n<Gist id=\\"7920fda821eb4f03d8ba942da572180c\\" \\r\\n/>\\r\\n\\r\\nUse the following commands to run this example (I have created a default credentials profile, but you could supply your API credentials directly, use STS, etc):\\r\\n\\r\\n```\\r\\ncd simple-lambda-ec2-scheduler\\r\\nterraform init\\r\\nterraform apply\\r\\n```\\r\\n\\r\\n![Terraform output](images/terraform-screenshot.png)\\r\\n\\r\\n> *Full source code can be found at: [__https://github.com/avensolutions/simple-lambda-ec2-scheduler__](https://github.com/avensolutions/simple-lambda-ec2-scheduler)*\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"},{"id":"multi-stage-etl-framework-using-spark-sql","metadata":{"permalink":"/multi-stage-etl-framework-using-spark-sql","editUrl":"https://github.com/stackql/fullstackchronicles.io/edit/main/src/blog/2019-01-09-multi-stage-etl-framework-using-spark-sql/index.md","source":"@site/blog/2019-01-09-multi-stage-etl-framework-using-spark-sql/index.md","title":"Multi Stage ETL Framework using Spark SQL","description":"A simple configuration driven Spark SQL ETL framework","date":"2019-01-09T00:00:00.000Z","formattedDate":"January 9, 2019","tags":[{"label":"etl","permalink":"/tags/etl"},{"label":"spark","permalink":"/tags/spark"},{"label":"sql","permalink":"/tags/sql"}],"readingTime":2.325,"hasTruncateMarker":false,"authors":[{"name":"Jeffrey Aven","title":"Technologist and Cloud Consultant","url":"https://www.linkedin.com/in/jeffreyaven/","imageURL":"https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80","key":"jeffreyaven"}],"frontMatter":{"slug":"multi-stage-etl-framework-using-spark-sql","title":"Multi Stage ETL Framework using Spark SQL","authors":["jeffreyaven"],"draft":false,"hide_table_of_contents":true,"description":"A simple configuration driven Spark SQL ETL framework","image":"images/spark-sql-etl-framework.png","tags":["etl","spark","sql"],"keywords":["etl","spark","sql"]},"prevItem":{"title":"Infrastructure Automation using AWS Lambda","permalink":"/infrastructure-automation-using-aws-lambda"}},"content":"import Gist from \'react-gist\';\\r\\n\\r\\n![Spark SQL ETL Framework](images/spark-sql-etl-framework.png)\\r\\n\\r\\nMost traditional data warehouse or datamart ETL routines consist of multi stage SQL transformations, often a series of CTAS (`CREATE TABLE AS SELECT`) statements usually creating transient or temporary tables \u2013 such as volatile tables in Teradata or Common Table Expressions (CTE\u2019s).\\r\\n\\r\\n:::note Spark Training Courses\\r\\n\\r\\n[Data Transformation and Analysis Using Apache Spark](https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/)  \\r\\n[Stream and Event Processing using Apache Spark](https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/)  \\r\\n[Advanced Analytics Using Apache Spark](https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/)\\r\\n\\r\\n:::\\r\\n\\r\\nThe initial challenge when moving from a SQL/MPP based ETL framework platformed on Oracle, Teradata, SQL Server, etc to a Spark based ETL framework is what to do with this\u2026\\r\\n\\r\\n![Multi Stage SQL Based ETL](images/multi-stage-sql.png)\\r\\n\\r\\nOne approach is to use the lightweight, configuration driven, multi stage Spark SQL based ETL framework described in this post.\\r\\n\\r\\nThis framework is driven from a YAML configuration document. YAML was preferred over JSON as a document format as it allows for multi-line statements (SQL statements), as well as comments - which are very useful as SQL can sometimes be undecipherable even for the person that wrote it.\\r\\n\\r\\nThe YAML config document has three main sections: __`sources`__, __`transforms`__ and __`targets`__.\\r\\n\\r\\n### Sources\\r\\n\\r\\nThe __`sources`__ section is used to configure the input data source(s) including optional column and row filters. In this case the data sources are tables available in the Spark catalog (for instance the AWS Glue Catalog or a Hive Metastore), this could easily be extended to read from other datasources using the Spark DataFrameReader API.\\r\\n\\r\\n<Gist id=\\"eaf03229466718ee125e0a6d23370f1b\\" \\r\\n/>\\r\\n\\r\\n### Transforms\\r\\n\\r\\nThe __`transforms`__ section contains the multiple SQL statements to be run in sequence where each statement creates a temporary view using objects created by preceding statements.\\r\\n\\r\\n<Gist id=\\"89ad7ac6b036e5f22b2d3dec43b1fe44\\" \\r\\n/>\\r\\n\\r\\n### Targets\\r\\n\\r\\nFinally the __`targets`__ section writes out the final object or objects to a specified destination (S3, HDFS, etc).\\r\\n\\r\\n<Gist id=\\"5af780dd6b6e5ddd79a4cac8a59e6a69\\" \\r\\n/>\\r\\n\\r\\n### Process SQL Statements\\r\\n\\r\\nThe __`process_sql_statements.py`__ script that is used to execute the framework is very simple (30 lines of code not including comments, etc). It loads the sources into Spark Dataframes and then creates temporary views to reference these datasets in the __`transforms`__ section, then sequentially executes the SQL statements in the list of transforms. Lastly the script writes out the final view or views to the desired destination \u2013 in this case parquet files stored in S3 were used as the target.\\r\\n\\r\\nYou could implement an object naming convention such as prefixing object names with `sv_`, `iv_`, `fv_` (for source view, intermediate view and final view respectively) if this helps you differentiate between the different objects.\\r\\n\\r\\nTo use this framework you would simply use __`spark-submit`__ as follows:\\r\\n\\r\\n```\\r\\nspark-submit process_sql_statements.py config.yml\\r\\n```\\r\\n\\r\\n> *Full source code can be found at: [__https://github.com/avensolutions/spark-sql-etl-framework__](https://github.com/avensolutions/spark-sql-etl-framework)*\\r\\n\\r\\n> if you have enjoyed this post, please consider [__buying me a coffee \u2615__](https://www.buymeacoffee.com/jeffreyaven) to help me keep writing!"}]}')}}]);