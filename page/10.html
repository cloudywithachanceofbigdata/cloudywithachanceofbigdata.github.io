<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.18">
<link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Full Stack Chronicles Blog Feed RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Full Stack Chronicles Blog Feed Atom Feed">
<link rel="alternate" type="application/json" href="/feed.json" title="Full Stack Chronicles Blog Feed JSON Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0FVDC1E8G6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0FVDC1E8G6",{})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Full Stack Chronicles" href="/opensearch.xml"><title data-rh="true">Full Stack Chronicles | Full Stack Chronicles</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" name="twitter:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" property="og:url" content="https://fullstackchronicles.io/page/10"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="wot-verification" content="e7522390be4370727fac"><meta data-rh="true" property="og:title" content="Full Stack Chronicles | Full Stack Chronicles"><meta data-rh="true" name="description" content="Full stack design patterns and random musings"><meta data-rh="true" property="og:description" content="Full stack design patterns and random musings"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://fullstackchronicles.io/page/10"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/page/10" hreflang="en"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/page/10" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://MZCGVO503N-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.77ea11e8.css">
<link rel="preload" href="/assets/js/runtime~main.e33c6797.js" as="script">
<link rel="preload" href="/assets/js/main.29d6b400.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><div class="announcementBar_IbjG" style="background-color:#A9BCD0;color:#1A4E82" role="banner"><div class="announcementBarPlaceholder_NC_W"></div><div class="announcementBarContent_KsVm"><b>If you find our content useful, give it a ⭐️ on <a target="_blank" rel="noopener noreferrer" href="https://github.com/stackql/fullstackchronicles.io">GitHub</a>, if you want to contribute and become an author, submit a PR</b></div><button type="button" class="clean-btn close announcementBarClose_FG1z" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top navbarHideable_ObN2 navbar--primary"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Full Stack Chronicles</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/tags">Topics</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/tags">All Topics</a></li><li><a class="dropdown__link" href="/tags/gcp">Google Cloud Platform</a></li><li><a class="dropdown__link" href="/tags/aws">AWS</a></li><li><a class="dropdown__link" href="/tags/azure">Azure</a></li><li><a class="dropdown__link" href="/tags/snowflake">Snowflake</a></li><li><a class="dropdown__link" href="/tags/okta">Okta</a></li><li><a class="dropdown__link" href="/tags/openapi">OpenAPI</a></li><li><a class="dropdown__link" href="/tags/spark">Spark</a></li><li><a class="dropdown__link" href="/tags/kafka">Kafka</a></li><li><a class="dropdown__link" href="/tags/ci-cd">CI/CD</a></li></ul></div><a class="navbar__item navbar__link" href="/archive">Archive</a><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_S7eR colorModeToggle_vKtC"><button class="clean-btn toggleButton_rCf9 toggleButtonDisabled_Pu9x" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_qEbK"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent Posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/converting-google-discovery-docs-to-openapi3-specs">Converting Google Discovery Docs to OpenAPI3 Specs</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way">Recurse JavaScript Object to Get Values for a Given Key the Easy Way</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/dataops-with-container-images-and-multi-stage-builds">DataOps with Container Images and Multi-Stage Builds</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/using-the-snowflake-sql-api-with-typescript">Using the Snowflake SQL API with TypeScript</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/split-a-large-swagger-openapi-specification-into-smaller-documents">Split a large Open API or Swagger Specification into smaller documents</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python">Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/simple-cli-pkce-auth-using-okta">Simple CLI Application to Login to Okta using PKCE</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/cloudy-with-a-chance-of-big-data-has-moved">Cloudy with a Chance of Big Data has Moved</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/scaling-up-prefect-with-gitstorage">Scaling up Prefect with GitStorage</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family">Implementing a Serverless SFTP Gateway using the AWS Transfer Family</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/map-reduce-is-dead-long-live-map-reduce">Map Reduce is Dead, Long Live Map Reduce</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2019-09-01T00:00:00.000Z" itemprop="datePublished">September 1, 2019</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/image.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Map Reduce is Dead" src="/assets/images/image-37d5c469e52212a957fc248f57ef7b64.png" width="1058" height="262" class="img_E7b_"></p><p>Firstly, this is not another Hadoop obituary, there are enough of those out there already.</p><p>The generalized title of this article has been used as an expression to convey the idea that something old has been replaced by something new. In the case of the expression “the King is dead, long live the King” the inference is that although one monarch has passed, another monarch instantly succeeds him.</p><p>In the age of instant gratification and hype cycle driven ‘pump and dump’ investment we are very quick to discard technologies that don’t realise overzealous targets for sales or market share. In our continuous attempts to find the next big thing, we are quick to throw out the last big thing and everything associated with it.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="the-reports-of-my-death-have-been-greatly-exaggerated">The Reports of My Death Have Been Greatly Exaggerated<a class="hash-link" href="#the-reports-of-my-death-have-been-greatly-exaggerated" title="Direct link to heading">​</a></h2><p>A classic example of this is the notion that Map Reduce is dead. Largely proliferated by the Hadoop obituaries which seem to be growing exponentially with each day.</p><p>A common e-myth is that Google invented the Map Reduce pattern, which is completely incorrect. In 2004, Google described a framework distributed systems implementation of the Map Reduce pattern in a white paper named <em>“MapReduce: Simplified Data Processing on Large Clusters.”</em> – this would inspire the first-generation processing framework (MapReduce) in the Hadoop project. But neither Google nor Yahoo! nor contributors to the Hadoop project (which include the pure play vendors) created the Map Reduce algorithm or processing pattern and neither shall any one of these have the rights to kill it.</p><p>The origins of the Map Reduce pattern can be traced all the way back to the early foundations of functional programming beginning with Lambda Calculus in the 1930s to LISP in the 1960s. Map Reduce is an integral pattern in all of today’s functional and distributed systems programming. You only need to look at the support for <code>map()</code> and <code>reduce()</code> operators in some of the most popular languages today including Python, JavaScript, Scala, and many more languages that support functional programming.</p><p>As far as distributed processing frameworks go, the Map Reduce pattern and its <code>map()</code> and <code>reduce()</code> methods are very prominent as higher order functions in APIs such as Spark, Kafka Streams, Apache Samza and Apache Flink to name a few.</p><p>While the initial Hadoop adaptation of Map Reduce has been supplanted by superior approaches, the Map Reduce processing pattern is far from dead.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="on-the-fall-of-hadoop">On the fall of Hadoop...<a class="hash-link" href="#on-the-fall-of-hadoop" title="Direct link to heading">​</a></h2><p>There is so much hysteria around the fall of Hadoop, we need to be careful not to toss the baby out with the bath water. Hadoop served a significant role in bringing open source, distributed systems from search engine providers to academia all the way to the mainstream, and still serves an important purpose in many organizations data ecosystems today and will continue to do so for some time.</p><p>OK, it wasn’t the panacea to everything, but who said it was supposed to be? The Hadoop movement was hijacked by hysteria, hype, venture capital, over ambitious sales targets and financial engineering – this does not mean the technology was bad.</p><p>Hadoop spawned many significant related projects such as Spark, Kafka and Presto to name a few. These projects paved the way for cloud integration, which is now the dominant vector for data storage, processing, and analysis.</p><p>While the quest for world domination by the Hadoop pure play vendors may be over, the Hadoop movement (and the impact it has had on the enterprise data landscape) will live on.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/big-data">big-data</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/hadoop">hadoop</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/map-reduce">map-reduce</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ansible-tower-for-continuous-infrastructure">Ansible Tower for Continuous Infrastructure</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2019-08-29T00:00:00.000Z" itemprop="datePublished">August 29, 2019</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/datwiz" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80" alt="Chris Ottinger"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/datwiz" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Chris Ottinger</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Technologist</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><div class="markdown" itemprop="articleBody"><p>As infrastructure and teams scale, effective and robust configuration management requires growing beyond manual processes and local conventions. Fortunately, <a href="https://www.ansible.com/products/tower" target="_blank" rel="noopener noreferrer">Ansible Tower</a> (or the upstream Open Source project <a href="https://github.com/ansible/awx" target="_blank" rel="noopener noreferrer">Ansible AWX</a>) provides a perfect platform for configuration management at scale.</p><p>The <a href="https://docs.ansible.com/ansible-tower/index.html" target="_blank" rel="noopener noreferrer">Ansible Tower/AWX documentation</a> and tutorials provide comprehensive information about the individual components.  However, assembling all the moving pieces into a whole working solution can involve some trial and error and reverse engineering in order to understand how the components relate to one another.  Ansible Tower, like the core Ansible solution, offers flexibility in how features assembled to support different typed of workflows. The types of workflows can include once-off initial configurations, ad-hoc system maintenance, or continuous convergence.</p><p>Continuous convergence, also referred to as desired state, regularly re-applies the defined configuration to infrastructure. This tends to &#x27;correct the drift&#x27; often encountered when only applying the configuration on infrastructure setup. For example, a continuous convergence approach to configuration management could apply the desired configuration on a recurring schedule of every 30 minutes.  </p><p>Some continuous convergence workflow characteristics can include:</p><ul><li>Idempotent Ansible roles. If there are no required configuration deviations, run will report 0 changes.</li><li>A source code repository per Ansible role, similar to the Ansible Galaxy approach,</li><li>A source code repository for Ansible playbooks that include the individual Ansible roles,</li><li>A host configured to provide one unique service function only,</li><li>An Ansible playbook defined for each unique service function that gets applied to the host,</li><li>Playbooks applied to each host on a repeating schedule.</li></ul><p>One way to achieve a continuous convergence workflow combines the Ansible Tower components according to the following conceptual model.</p><p><a target="_blank" href="/assets/files/Ansible-AWX-Continuous-Convergence-f3d3ad2b3baa64932ac1276077ace883.png"><img loading="lazy" src="/assets/images/Ansible-AWX-Continuous-Convergence-f3d3ad2b3baa64932ac1276077ace883.png" width="1257" height="616" class="img_E7b_"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="the-workflow-components">The Workflow Components<a class="hash-link" href="#the-workflow-components" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="playbook-and-role-source-code">Playbook and Role Source Code<a class="hash-link" href="#playbook-and-role-source-code" title="Direct link to heading">​</a></h3><p><strong>Ansible roles</strong> contain the individual tasks, handlers, and content with a role responsible for the installation and configuration of a particular software service.</p><p><strong>Ansible playbooks</strong> configure a host for a particular service function in the environment acting as a wrapper for the individual role based configurations.  All the roles expected to be applied to a host must be defined in the playbook.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="source-code-repositories">Source Code Repositories<a class="hash-link" href="#source-code-repositories" title="Direct link to heading">​</a></h3><p><strong>Role git repositor</strong>ies contain the versioned definition of a role, e.g. one git repository per individual role.  The roles are pulled into the playbooks using the git reference and tags, which pegs the role version used within a playbook.</p><p><strong>Project git repositories</strong> group the individual playbooks into single collection, e.g. one git repository per set of playbooks.  As with roles, specific versions of project repositories are also identified by version tags. </p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="ansible-tower-server">Ansible Tower Server<a class="hash-link" href="#ansible-tower-server" title="Direct link to heading">​</a></h3><p>Two foundational concepts in Ansible Tower are projects and inventories. Projects provide access to playbooks and roles. Inventories provide the connection to &quot;real&quot; infrastructure.  Inventories and projects also provide authorisation scope for activities in Ansible Tower. For example, a given group can use the playbooks in Project X and apply jobs to hosts in Inventory Y.</p><p>Each <strong>Ansible Tower Project</strong> is backed by a project git repository.  Each repository contains the playbooks and included roles that can be applied by a given job.  The Project is the glue between the Ansible configuration tasks and the plays that apply the configuration.</p><p><strong>Ansible Tower Inventories</strong> are sets of hosts grouped for administration, similar to inventory sets used when applying playbooks manually.  One option is to group hosts into Inventories by environment.  For example, the hosts for development may be in one Inventory while the hosts for production may be in another Inventory.  User authorisation controls are applied at the Inventory level.</p><p><strong>Ansible Tower Inventory Groups</strong> define sub-sets of hosts within the larger Inventory.  These subsets can then be used to limit the scope of a playbook job.  One option is to group hosts within an Inventory by function.  For example, the hosts for web servers may be in one Inventory Group and the hosts for databases may be in another Inventory Group.  This enables one playbook to target one inventory group.  Inventory groups effectively provide metadata labels for hosts in the Inventory.</p><p>An <strong>Ansible Job Template</strong> determines the configuration to be applied to hosts.  The Job Template links a playbook from a project to an inventory.   The inventory scope can be optionally further limited by specifying inventory group limits.  A Job Template can be invoked either on an ad-hoc basis or via a recurring schedule.</p><p><strong>Ansible Job Schedules</strong> define the time and frequency at which the configuration specified in the Job Template is applied.  Each Job Template can be associated with one or more Job Schedules.  A schedule supports either once-off execution, for example during a defined change window, or regularly recurring execution.  A job schedule that applies the desired state configuration with a frequency of 30 minutes provides an example of a job schedule used for a continuous convergence workflow.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="real-infrastructure">&quot;Real&quot; Infrastructure<a class="hash-link" href="#real-infrastructure" title="Direct link to heading">​</a></h3><p>An <strong>Ansible Job Instance</strong> defines a single invocation of an Ansible Job Template, both for scheduled and ad-hoc invocations of the job template.  Outside of Ansible Tower, the Job Instance is the equivalent of executing the <code>ansible-playbook</code> command using an inventory file.</p><p>A <strong>Host</strong> is the actual target infrastructure resources configured by the job instance, applying an ansible playbook of included roles.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="a-note-on-ansible-variables">A note on Ansible Variables<a class="hash-link" href="#a-note-on-ansible-variables" title="Direct link to heading">​</a></h2><p>As with other features of Ansible and Ansible Tower, variables also offer flexibility in defining parameters and context when applying a configuration.  In addition to declaring and defining variables in roles and playbooks, variable definitions can also be defined in Ansible Tower job templates, inventory and inventory groups, and individual hosts.  Given the plethora of options for variable definition locations, without a set of conventions for managing variable values, debugging runtime issues with roles and playbooks can become difficult.  E.g. which value defined at which location was used when applying the role?</p><p>One example of variable definitions conventions could include:</p><ul><li>Variables shall be given default values in the role, .e.g. in the <code>../defaults/main.yml</code> file.</li><li>If the variable must have a &#x27;real&#x27; value supplied when applying the playbook, the variable shall be defined with an obvious placeholder value which will fail if not overridden.</li><li>Variables shall be described in the role <code>README.md</code> documentation</li><li>Do not apply variables at the host inventory level as host inventory can be transient.</li><li>Variables that select specific capabilities within a role shall be defined at the Ansible Tower Inventory Group.  For example, a role contains the configuration definition for both master and work nodes.  The Inventory Group variables are used to indicate which hosts must have the master configuration and applied and which must have the worker configuration applied.</li><li>Variables that define the environment context for configuration shall be defined in the Ansible Tower Job Template.</li></ul><p>Following these conventions, each of the possible variable definition options serves a particular purpose.  When an issue with variable definition does arise, the source is easily identified.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/ansible">ansible</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/ci-cd">ci-cd</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/continuous-infrastructure">continuous-infrastructure</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/managing-secrets-in-cicd-pipelines">Managing Secrets in CICD Pipelines</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2019-07-16T00:00:00.000Z" itemprop="datePublished">July 16, 2019</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/datwiz" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80" alt="Chris Ottinger"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/datwiz" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Chris Ottinger</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Technologist</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/Gitlab-Vault.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Gitlab Vault" src="/assets/images/Gitlab-Vault-c451dca4eb190e47c62f21989fc14d51.png" width="271" height="243" class="img_E7b_"></p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="overview">Overview<a class="hash-link" href="#overview" title="Direct link to heading">​</a></h2><p>With the adoption automation for deploying and managing application environments, protecting privileged accounts and credential secrets in a consistent, secure, and scalable manner becomes critical.  Secrets can include account usernames, account passwords and API tokens.  Good credentials management and secrets automation practices reduce the risk of secrets escaping into the wild and being used either intentionally (hacked) or unintentionally (accident).</p><ul><li>Reduce the likelihood of passwords slipping into source code commits and getting pushed to code repositories, especially public repositories such as github.</li><li>Minimise the secrets exposure surface area by reducing the number of people who require knowledge of credentials.  With an automated credentials management process that number can reach zero.</li><li>Limit the useful life of a secret by employing short expiry times and small time-to-live (TTL) values.  Automation enables reliable low-effort secret re-issue and rotation.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="objectives">Objectives<a class="hash-link" href="#objectives" title="Direct link to heading">​</a></h2><p>The following objectives have been considered in designing a secrets automation solution that can be integrated into an existing CICD environment.</p><ul><li>Integrate into an existing CICD environment without requiring an &quot;all or nothing&quot; implementation.  Allow existing jobs to operate alongside jobs that have been converted to the new secrets automation solution.</li><li>A single design that can be applied across different toolchains and deployment models.  For example, deployment to a Kubernetes environment can use the same secrets management process as an application installation on a virtual machine.  Similarly, the design can be used with different CICD tools, such as <a href="https://about.gitlab.com" target="_blank" rel="noopener noreferrer">GitLab-CI</a>, <a href="https://travis-ci.org" target="_blank" rel="noopener noreferrer">Travis-CI</a>, or other build and deploy automation tool.</li><li>Multi-cloud capable by limiting coupling to a specific hosting environment or cloud services provider.</li><li>The use of secrets (or not) can be decided at any point in time, without requiring changes to the CICD job definition, similar to the use of feature flags in applications.</li><li>Enable changes to secrets, either due to rotation or revocation, to be maintained from a central service point.  Avoid storing the same secret multiple times in different locations.</li><li>Secrets organised in predictable locations in a &quot;rest-ish&quot; fashion by treating secrets and credentials as attributes of entities.</li><li>Use environment variables as the standard interface between deployment orchestration and deployed application, following the 12 Factor App approach.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="solution">Solution<a class="hash-link" href="#solution" title="Direct link to heading">​</a></h2><ul><li>Secrets stored centrally in Hashicorp Vault.</li><li>CICD jobs retrieve secrets from Vault and configure the application deployment environment.</li><li>Deployed applications use the secrets supplied by CICD job to access backend services.</li></ul><p><a target="_blank" href="/assets/files/Screen-Shot-2019-07-16-at-17.03.47-d299ab2b399de970fe6ecdc17536fb1a.png"><img loading="lazy" alt="CICD Secrets with Vault" src="/assets/images/Screen-Shot-2019-07-16-at-17.03.47-d299ab2b399de970fe6ecdc17536fb1a.png" width="750" height="434" class="img_E7b_"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="storing-secrets">Storing Secrets<a class="hash-link" href="#storing-secrets" title="Direct link to heading">​</a></h2><p>Use <a href="https://www.vaultproject.io/" target="_blank" rel="noopener noreferrer">Vault by Hashicorp</a> as a centralised secrets storage service.  The CICD service retrieves secrets information for integration and deployment jobs.  Vault provides a flexible set of features to support numerous different workflows and available as either Vault Open Source or Vault Enterprise.  The secrets management pattern described uses the Vault Open Source version.  The workflow described here can be explored using Vault in the unsecured development mode, however, a properly configured and managed Vault service is required for production use.</p><p>Vault supports a number of secrets backends and access workflow models.  This solution makes use of the <a href="https://www.vaultproject.io/docs/auth/approle.html" target="_blank" rel="noopener noreferrer">Vault AppRole method</a>, which is designed to support machine-to-machine automated workflows.  With the AppRole workflow model human access to secrets is minimised through the use of access controls and temporary credentials with short TTL&#x27;s.  Within Vault, secrets are organised using an entity centric &quot;rest-ish&quot; style approach ensuring a given secret for a given service is stored in a single predictable location.</p><p>The use of Vault satisfies several of the design objectives:</p><ul><li>enables single point management of secrets. The secrets content is stored in a single location referenced at CICD job runtime.  On the next invocation, the CICD job retrieves the latest version of the secrets content.</li><li>enables storing secrets in predictable locations with file system directory style path location.  The &quot;rest-ish&quot; approach to organising secret locations enables storing a given secret only once.  Access policies provide the mechanism to limit CICD  visibility to only those secrets required for the CICD job.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="passing-secrets-to-applications">Passing Secrets to Applications<a class="hash-link" href="#passing-secrets-to-applications" title="Direct link to heading">​</a></h2><p>Use environment variables to pass secrets from the CICD service to the application environment.  </p><p>There are existing utilities available for populating a child process environment with Vault sourced secrets, such as <a href="https://github.com/channable/vaultenv" target="_blank" rel="noopener noreferrer">vaultenv</a> or <a href="https://github.com/hashicorp/envconsul" target="_blank" rel="noopener noreferrer">envconsul</a>.  This approach works well for running an application service.  However, with CICD, often there are often sets of tasks that require access to secrets information as opposed to a single command.  Using the child environment approach would require wrapping each command in a CICD job step with the env utility.  This works against the objective of introducing a secrets automation solution into existing CICD jobs without requiring substantial refactoring.  Similarly, some CICD solutions such as <a href="https://jenkins.io/" target="_blank" rel="noopener noreferrer">Jenkins</a> provide Vault integration plugins which pre-populate the environment with secrets content.  This meets the objective of minimal CICD job refactoring, but closely couples the solution to a particular CICD service stack, reducing portability.  </p><p>With a job script oriented CICD automation stack like GitLab-CI or Travis-CI, an alternative is to insert a job step at the beginning of a series of CICD tasks that will populated the required secret values into expected environment variables.  Subsequent tasks in the job can then execute without requiring refactoring.  The decision on whether to source a particular environment variable&#x27;s content directly from the CICD job setup or from the Vault secrets store can be made by adding an optional prefix to environment variables to be sourced from the Vault secrets store.  The prefixed instance of the environment variable contains the location or path to the required secret.  Secret locations are identified using the convention <code>/&lt;vault-secret-path&gt;/&lt;secret-key&gt;</code></p><ul><li>enables progressive implementation due to transparency of secret sourcing. Subsequent steps continue to rely on expected environment vars</li><li>enables use in any toolchain that supports use of environment variables to pass information to application environment. </li><li>CICD job steps not tied to a specific secrets store. An alternative secrets storage service could be supported by only requiring modification of the secret getter utility.</li><li>control of whether to source application environment variables from the CICD job directly or from the secrets engine is managed at the CICD job setup level as opposed to requiring CICD job refactoring to switch the content source.</li><li>continues the 12 Factor App approach of using environment variables to pass context to application environments.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="example-workflow">Example Workflow<a class="hash-link" href="#example-workflow" title="Direct link to heading">​</a></h2><p>An example workflow for a CICD job designed to use environment variables for configuring an application.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="assumptions">Assumptions<a class="hash-link" href="#assumptions" title="Direct link to heading">​</a></h3><p>The following are available in the CICD environment.</p><ul><li>A job script oriented CICD automation stack that executes job tasks as a series of shell commands, such as <a href="https://about.gitlab.com" target="_blank" rel="noopener noreferrer">GitLab-CI</a> or <a href="https://jenkins.io/doc/book/pipeline/" target="_blank" rel="noopener noreferrer">Jenkins Pipelines</a>.</li><li>A secrets storage engine with a python API, such as Hashicorp Vault.</li><li>CICD execution environment includes the <code>[get-vault-secrets-by-approle](https://github.com/datwiz/cicd-secrets-in-vault/blob/master/scripts/get-vault-secrets-by-approle)</code> utility script.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="workflow-steps">Workflow Steps<a class="hash-link" href="#workflow-steps" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-a-vault-secret">Add a Vault secret<a class="hash-link" href="#add-a-vault-secret" title="Direct link to heading">​</a></h3><p>Add a secret to Vault at the location <code>secret/fake-app/users/fake-users</code> with a key/value entry of <code>password=fake-password</code></p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-a-vault-access-policy">Add a Vault access policy<a class="hash-link" href="#add-a-vault-access-policy" title="Direct link to heading">​</a></h3><p>Add a Vault policy for the CICD job (or set of CICD jobs) that includes &#x27;read&#x27; access to the secret.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain"># cicd-fake-app-policy </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">path &quot;secret/data/fake-app/users/fake-user&quot; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    capabilities = [&quot;read&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">path &quot;secret/metadata/fake-app/users/fake-user&quot; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    capabilities = [&quot;list&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-a-vault-approle">Add a Vault appRole<a class="hash-link" href="#add-a-vault-approle" title="Direct link to heading">​</a></h3><p>Add a Vault appRole linked to the new policy.  This example specifies a new appRole with an secret-id TTL of 60 days and non-renewable access tokens with a TTL of 5 minutes.  The CICD job uses the access token to read secrets.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">vault write auth/approle/role/fake-role \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    secret_id_ttl=1440h \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    token_ttl=5m \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    token_max_ttl=5m \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    policies=cicd-fake-app-policy</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="read-the-vault-approle-id">Read the Vault approle-id<a class="hash-link" href="#read-the-vault-approle-id" title="Direct link to heading">​</a></h3><p>Retrieve the approle-id of the new appRole taking note of the returned approle-id.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">vault read auth/approle/role/fake-role</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-a-vault-approle-secret-id">Add a Vault appRole secret-id<a class="hash-link" href="#add-a-vault-approle-secret-id" title="Direct link to heading">​</a></h3><p>Add a secret-id for the appRole, taking note of the returned secret-id</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">vault write -f auth/approle/role/fake-role/secret-id</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-cicd-job-steps">Add CICD Job Steps<a class="hash-link" href="#add-cicd-job-steps" title="Direct link to heading">​</a></h3><p>In the CICD job definition insert job steps to retrieve secrets values a set variables in the job execution environment. These are the steps to add in a gitlab-ci.yml CICD job.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">script:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- get-vault-secrets-by-approle &gt; ${VAULT_VAR_FILE}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- source ${VAULT_VAR_FILE} &amp;&amp; rm ${VAULT_VAR_FILE}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>The helper script <code>get-vault-secrets-by-approle</code> could be executed and sourced in a single step, e.g. <code>source $(get-vault-secrets-by-approle)</code>.  However, when executed in a single statement all script output is processed by the <code>source</code> command and script error messages don&#x27;t get printed and captured in the job logs.  Splitting the read and environment var sourcing into 2 steps aids in troubleshooting.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-cicd-job-vars-for-vault-access">Add CICD job vars for Vault access<a class="hash-link" href="#add-cicd-job-vars-for-vault-access" title="Direct link to heading">​</a></h3><p>In the CICD job configuration add Vault access environment variables.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">VAULT_ADDR=https://vault.example.com:8200</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">VAULT_ROLE_ID=db02de05-fa39-4855-059b-67221c5c2f63</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">VAULT_SECRET_ID=6a174c20-f6de-a53c-74d2-6018fcceff64</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">VAULT_VAR_FILE=/var/tmp/vault-vars.sh</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="add-cicd-job-vars-for-vault-secrets">Add CICD job vars for Vault secrets<a class="hash-link" href="#add-cicd-job-vars-for-vault-secrets" title="Direct link to heading">​</a></h3><p>In the CICD job configuration add environment variables for the items to be sourced from vault secrets.  The secret path follows the convention of <code>&lt;secret-mount-path&gt;/&lt;secret-path&gt;/&lt;secret-key&gt;</code></p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">V_FAKE_PASSWORD=secret/fake-app/users/fake-user/password</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="remove-cicd-job-vars">Remove CICD job vars<a class="hash-link" href="#remove-cicd-job-vars" title="Direct link to heading">​</a></h3><p>In the CICD job configuration remove the previously used <code>FAKE_APP_PASSWORD</code> variable.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="execute-the-cicd-job">Execute the CICD job<a class="hash-link" href="#execute-the-cicd-job" title="Direct link to heading">​</a></h3><p>Kick off the CICD job.  Any CICD job configuration variables prefixed with &quot;<code>V_</code>&quot; results in the addition of a corresponding environment variable in the job execution environment with content sourced from Vault.</p><blockquote><p>Full source code can be found at:</p><p><a href="https://github.com/datwiz/cicd-secrets-in-vault" target="_blank" rel="noopener noreferrer">https://github.com/datwiz/cicd-secrets-in-vault</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/ci-cd">ci-cd</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gitlab-ci">gitlab-ci</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/hashicorp-vault">hashicorp-vault</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/jenkins">jenkins</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/secrets-management">secrets-management</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/change-data-capture-at-scale-using-spark">Change Data Capture at Scale using Spark</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2019-06-28T00:00:00.000Z" itemprop="datePublished">June 28, 2019</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/CDC-using-Spark.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="CDC using Spark" src="/assets/images/CDC-using-Spark-38cb48e3545c719f89f4380b50711cf3.png" width="256" height="251" class="img_E7b_"></p><p>Change Data Capture (CDC) is one of the most challenging processing patterns to implement at scale. I personally have had several cracks at this using various different frameworks and approaches, the most recent of which was implemented using Spark – and I think I have finally found the best approach. Even though the code examples referenced use Spark, the pattern is language agnostic – the focus is on the approach not the specific implementation (as this could be applied to any framework or runtime).</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</h5></div><div class="admonition-content"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>The first challenge you are faced with, is to compare a very large dataset (representing the current state of an object) with another potentially very large dataset (representing new or incoming data). Ideally, you would like the process to be configuration driven and accommodate such things as composite primary keys, or operational columns which you would like to restrict from change detection. You may also want to implement a pattern to segregate sensitive attributes from non-sensitive attributes.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="overview">Overview<a class="hash-link" href="#overview" title="Direct link to heading">​</a></h2><p>This pattern (and all my other recent attempts) is fundamentally based upon calculating a deterministic hash of the key and non-key attribute(s), and then using this hash as the basis for comparison. The difference between this pattern and my other attempts is in the distillation and reconstitution of data during the process, as well as breaking the pattern into discrete stages (designed to minimize the impact to other applications). This pattern can be used to process delta or full datasets.</p><p>A high-level flowchart representing the basic pattern is shown here:</p><p><a target="_blank" href="/assets/files/CDC-59f029c756f942661da9a4744801d227.png"><img loading="lazy" alt="CDC Flowchart" src="/assets/images/CDC-59f029c756f942661da9a4744801d227.png" width="382" height="1118" class="img_E7b_"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="the-example">The Example<a class="hash-link" href="#the-example" title="Direct link to heading">​</a></h2><p>The example provided uses the <a href="https://github.com/avensolutions/synthetic-cdc-data-generator" target="_blank" rel="noopener noreferrer">Synthetic CDC Data Generator application</a>, configuring an incoming set with 5 uuid columns acting as a composite key, and 10 random number columns acting as non key values. The initial days payload consists of 10,000 records, the subsequent days payload consists of another 10,000 records. From the initial dataset, a <code>DELETE</code> operation was performed at the source system for 20% of records, an <code>UPDATE</code> was performed on 40% of the records and the remaining 40% of records were unchanged. In this case the 20% of records that were deleted at the source, were replaced by new <code>INSERT</code> operations creating new keys.</p><p>After creating the synthesized day 1 and day 2 datasets, the files are processed as follows:</p><p>$ spark-submit cdc.py config.yaml data/day1 2019-06-18<br>
<!-- -->$ spark-submit cdc.py config.yaml data/day2 2019-06-19</p><p>Where <code>config.yaml</code> is the configuration for the dataset, data/day1 and data/day2 represent the different data files, and 2019-06-18 and 2019-06-19 represent a business effective date.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="the-results">The Results<a class="hash-link" href="#the-results" title="Direct link to heading">​</a></h2><p>You should see the following output from running the preceding commands for day 1 and day 2 respectively:</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="day-1">Day 1:<a class="hash-link" href="#day-1" title="Direct link to heading">​</a></h3><iframe width="100%" frameborder="0" id="gist-b75edc7825b46c12b328d78d47b4b902"></iframe><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="day-2">Day 2:<a class="hash-link" href="#day-2" title="Direct link to heading">​</a></h3><iframe width="100%" frameborder="0" id="gist-ca92e132105fb5bb381bf9dfca562bf4"></iframe><p>A summary analysis of the resultant dataset should show:</p><iframe width="100%" frameborder="0" id="gist-ded1f98dc4fce13c9bb3d12a51a46b94"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="pattern-details">Pattern Details<a class="hash-link" href="#pattern-details" title="Direct link to heading">​</a></h2><p>Details about the pattern and its implementation follow.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="current-and-historical-datasets">Current and Historical Datasets<a class="hash-link" href="#current-and-historical-datasets" title="Direct link to heading">​</a></h3><p>The output of each operation will yield a current dataset (that is the current stateful representation of a give object) and a historical dataset partition (capturing the net changes from the previous state in an appended partition).</p><p>This is useful, because often consumers will primarily query the latest state of an object. The change sets (or historical dataset partitions) can be used for more advanced analysis by sophisticated users.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="type-2-scds-sort-of">Type 2 SCDs (sort of)<a class="hash-link" href="#type-2-scds-sort-of" title="Direct link to heading">​</a></h3><p>Two operational columns are added to each current and historical object:</p><ul><li><code>OPERATION</code> : Represents the last known operation to the record, valid values include :<ul><li><code>I</code> (<code>INSERT</code>)</li><li><code>U</code> (<code>UPDATE</code>)</li><li><code>D</code> (<code>DELETE</code> – hard <code>DELETE</code>s, applies to full datasets only)</li><li><code>X</code> (Not supplied, applies to delta processing only)</li><li><code>N</code> (No change)</li></ul></li><li><code>EFF_START_DATE</code></li></ul><p>Since data structures on most big data or cloud storage platforms are immutable, we only store the effective start date for each record, this is changed as needed with each coarse-grained operation on the current object. The effective end date is inferred by the presence of a new effective start date (or change in the <code>EFF_START_DATE</code> value for a given record).</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="the-configuration">The Configuration<a class="hash-link" href="#the-configuration" title="Direct link to heading">​</a></h3><p>I am using a YAML document to store the configuration for the pattern. Important attributes to include in your configuration are a list of keys and non keys and their datatype (this implementation does type casting as well). Other important attributes include the table names and file paths for the current and historical data structures.</p><p>The configuration is read at the beginning of a routine as an input along with the path of an incoming data file (a CSV file in this case) and a business effective date (which will be used as the <code>EFF_START_DATE</code> for new or updated records).</p><p>Processing is performed using the specified key and non key attributes and the output datasets (current and historical) are written to columnar storage files (parquet in this case). This is designed to make subsequent access and processing more efficient.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="the-algorithm">The Algorithm<a class="hash-link" href="#the-algorithm" title="Direct link to heading">​</a></h3><p>I have broken the process into stages as follows:</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="stage-1--type-cast-and-hash-incoming-data">Stage 1 – Type Cast and Hash Incoming Data<a class="hash-link" href="#stage-1--type-cast-and-hash-incoming-data" title="Direct link to heading">​</a></h4><p>The first step is to create deterministic hashes of the configured key and non key values for incoming data. The hashes are calculated based upon a list of elements representing the key and non key values using the MD5 algorithm. The hashes for each record are then stored with the respective record. Furthermore, the fields are casted their target datatype as specified in the configuration. Both of these operations can be performed in a single pass of each row using a <code>map()</code> operation.</p><p>Importantly we only calculate hashes once upon arrival of new data, as the hashes are persisted for the life of the data – and the data structures are immutable – the hashes should never change or be invalidated.</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="stage-2--determine-inserts">Stage 2 – Determine INSERTs<a class="hash-link" href="#stage-2--determine-inserts" title="Direct link to heading">​</a></h4><p>We now compare Incoming Hashes with previously calculated hash values for the (previous day’s) current object. If no current object exists for the dataset, then it can be assumed this is a first run. In this case every record is considered as an <code>INSERT</code> with an <code>EFF_START_DATE</code> of the business effective date supplied.</p><p>If there is a current object, then the key and non key hash values (only the hash values) are read from the current object. These are then compared to the respective hashes of the incoming data (which should still be in memory).</p><p>Given the full outer join:</p><p>incoming<!-- -->_<!-- -->data(keyhash, nonkeyhash)
FULL OUTER JOIN<br>
<!-- -->current<!-- -->_<!-- -->data(keyhash, nonkeyhash)
ON keyhash</p><p>Keys which exist in the left entity which do not exist in the right entity must be the results of an INSERT operation.</p><p>Tag these records with an operation of <code>I</code> with an <code>EFF_START_DATE</code> of the business effective date, then rejoin only these records with their full attribute payload from the incoming dataset. Finally, write out these records to the current and historical partition in <code>overwrite</code> mode.</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="stage-3---determine-deletes-or-missing-records">Stage 3 - Determine DELETEs or Missing Records<a class="hash-link" href="#stage-3---determine-deletes-or-missing-records" title="Direct link to heading">​</a></h4><p>Referring the previous full outer join operation, keys which exist in the right entity (current object) which do not appear in the left entity (incoming data) will be the result of a (hard) <code>DELETE</code> operation if you are processing full snapshots, otherwise if you are processing deltas these would be missing records (possibly because there were no changes at the source).</p><p>Tag these records as <code>D</code> or <code>X</code> respectively with an <code>EFF_START_DATE</code> of the business effective date, rejoin these records with their full attribute payload from the current dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="stage-4---determine-updates-or-unchanged-records">Stage 4 - Determine UPDATEs or Unchanged Records<a class="hash-link" href="#stage-4---determine-updates-or-unchanged-records" title="Direct link to heading">​</a></h4><p>Again, referring to the previous full outer join, keys which exist in both the incoming and current datasets must be either the result of an <code>UPDATE</code> or they could be unchanged. To determine which case they fall under, compare the non key hashes. If the non key hashes differ, it must have been a result of an <code>UPDATE</code> operation at the source, otherwise the record would be unchanged.</p><p>Tag these records as <code>U</code> or <code>N</code> respectively with an <code>EFF_START_DATE</code> of the business effective date (in the case of an update - otherwise maintain the current <code>EFF_START_DATE</code>), rejoin these records with their full attribute payload from the incoming dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="key-callouts">Key Callouts<a class="hash-link" href="#key-callouts" title="Direct link to heading">​</a></h3><p>A summary of the key callouts from this pattern are:</p><ul><li>Use the RDD API for iterative record operations (such as type casting and hashing)</li><li>Persist hashes with the records</li><li>Use Dataframes for <code>JOIN</code> operations</li><li>Only perform <code>JOIN</code>s with the <code>keyhash</code> and <code>nonkeyhash</code> columns – this minimizes the amount of data shuffled across the network</li><li>Write output data in columnar (Parquet) format</li><li>Break the routine into stages, covering each operation, culminating with a <code>saveAsParquet()</code> action – this may seem expensive but for large datsets it is more efficient to break down DAGs for each operation</li><li>Use caching for objects which will be reused between actions</li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="metastore-integration">Metastore Integration<a class="hash-link" href="#metastore-integration" title="Direct link to heading">​</a></h4><p>Although I did not include this in my example, you could easily integrate this pattern with a metastore (such as a Hive metastore or AWS Glue Catalog), by using table objects and <code>ALTER TABLE</code> statements to add historical partitions.</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="further-optimisations">Further optimisations<a class="hash-link" href="#further-optimisations" title="Direct link to heading">​</a></h4><p>If the incoming data is known to be relatively small (in the case of delta processing for instance), you could consider a broadcast join where the smaller incoming data is distributed to all of the different Executors hosting partitions from the current dataset.</p><p>Also you could add a key to the column config to configure a column to be nullable or not.</p><p>Happy CDCing!</p><blockquote><p>Full source code for this article can be found at: <a href="https://github.com/avensolutions/cdc-at-scale-using-spark" target="_blank" rel="noopener noreferrer">https://github.com/avensolutions/cdc-at-scale-using-spark</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/big-data">big-data</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/cdc">cdc</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/pyspark">pyspark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/python">python</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/spark">spark</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/synthetic-cdc-data-generator">Synthetic CDC Data Generator</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2019-06-28T00:00:00.000Z" itemprop="datePublished">June 28, 2019</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><div class="markdown" itemprop="articleBody"><p>This is a simple routine to generate random data with a configurable number or records, key fields and non key fields to be used to create synthetic data for source change data capture (CDC) processing. The output includes an initial directory containing CSV files representing an initial data load, and an incremental directory containing CSV files representing incremental data.</p><p>Spark Training Courses from the AlphaZetta Academy</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p><p>Arguments (by position) include:</p><ul><li><code>no_init_recs</code> : the number of initial records to generate</li><li><code>no_incr_recs</code> : the number of incremental records on the second run - should be <code>&gt;= no_init_recs</code></li><li><code>no_keys</code> : number of key columns in the dataset – keys are generated as UUIDs</li><li><code>no_nonkeys</code> : number of non-key columns in the dataset – nonkey values are generated as random numbers</li><li><code>pct_del</code> : percentage of initial records deleted on the second run - between 0.0 and 1.0</li><li><code>pct_upd</code> : percentage of initial records updated on the second run - between 0.0 and 1.0</li><li><code>pct_unchanged</code> : percentage of records unchanged on the second run - between 0.0 and 1.0</li><li><code>initial_output</code> : folder for initial output in CSV format</li><li><code>incremental_output</code> : folder for incremental output in CSV format</li></ul><p>NOTE : <code>pct_del</code> + <code>pct_upd</code> + <code>pct_unchanged</code> must equal 1.0</p><p>Example usage:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">$ spark-submit synthetic-cdc-data-generator.py 100000 100000 2 3 0.2 0.4 0.4 data/day1 data/day2</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Example output from the <strong><em>day1</em></strong> run for the above configuration would look like this:</p><iframe width="100%" frameborder="0" id="gist-befb034da2b4f25a1dbbc0e9b4b8eef6"></iframe><p>Note that this routine can be run subsequent times producing different key and non key values each time, as the keys are UUIDs and the values are random numbers.</p><p>We will use this application to generate random input data to demonstrate CDC using Spark in a subsequent post, see you soon!</p><blockquote><p>Full source code can be found at: <a href="https://github.com/avensolutions/synthetic-cdc-data-generator" target="_blank" rel="noopener noreferrer">https://github.com/avensolutions/synthetic-cdc-data-generator</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/cdc">cdc</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/python">python</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/spark">spark</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/page/9"><div class="pagination-nav__label">Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/page/11"><div class="pagination-nav__label">Older Entries</div></a></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Blog</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/">Home</a></li><li class="footer__item"><a class="footer__link-item" href="/archive">Archive</a></li><li class="footer__item"><a class="footer__link-item" href="/tags">Tags</a></li></ul></div><div class="col footer__col"><div class="footer__title">Sponsors</div><ul class="footer__items"><li class="footer__item"><a href="https://stackql.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">StackQL<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://gammadata.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gamma Data<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.windrate.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">WindRate<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.e33c6797.js"></script>
<script src="/assets/js/main.29d6b400.js"></script>
</body>
</html>