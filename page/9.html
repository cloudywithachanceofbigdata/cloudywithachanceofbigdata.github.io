<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-rc.1">
<link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Full Stack Chronicles Blog Feed RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Full Stack Chronicles Blog Feed Atom Feed">
<link rel="alternate" type="application/json" href="/feed.json" title="Full Stack Chronicles Blog Feed JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0FVDC1E8G6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0FVDC1E8G6",{})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Full Stack Chronicles" href="/opensearch.xml"><title data-rh="true">Full Stack Chronicles | Full Stack Chronicles</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" name="twitter:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" property="og:url" content="https://fullstackchronicles.io/page/9"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="wot-verification" content="e7522390be4370727fac"><meta data-rh="true" property="og:title" content="Full Stack Chronicles | Full Stack Chronicles"><meta data-rh="true" name="description" content="Full stack design patterns and random musings"><meta data-rh="true" property="og:description" content="Full stack design patterns and random musings"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://fullstackchronicles.io/page/9"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/page/9" hreflang="en"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/page/9" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://MZCGVO503N-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.500c912b.css">
<link rel="preload" href="/assets/js/runtime~main.f1e5cb7b.js" as="script">
<link rel="preload" href="/assets/js/main.965b623c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ navbar--primary"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Full Stack Chronicles</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/tags">Topics</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/tags">All Topics</a></li><li><a class="dropdown__link" href="/tags/gcp">Google Cloud Platform</a></li><li><a class="dropdown__link" href="/tags/aws">AWS</a></li><li><a class="dropdown__link" href="/tags/azure">Azure</a></li><li><a class="dropdown__link" href="/tags/snowflake">Snowflake</a></li><li><a class="dropdown__link" href="/tags/okta">Okta</a></li><li><a class="dropdown__link" href="/tags/openapi">OpenAPI</a></li><li><a class="dropdown__link" href="/tags/spark">Spark</a></li><li><a class="dropdown__link" href="/tags/kafka">Kafka</a></li><li><a class="dropdown__link" href="/tags/ci-cd">CI/CD</a></li></ul></div><a class="navbar__item navbar__link" href="/archive">Archive</a><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent Posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/deno-in-five-minutes">Deno in 5 Minutes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/dbt-in-five-minutes">DBT in 5 Minutes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/loading-parquet-files-into-snowflake">Loading Parquet Files into Snowflake</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/analyze-developer-activity-with-stackql-jupyter-bigquery">Analyze Developer Activity with StackQL, Jupyter and BigQuery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/converting-google-discovery-docs-to-openapi3-specs">Converting Google Discovery Docs to OpenAPI3 Specs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way">Recurse JavaScript Object to Get Values for a Given Key the Easy Way</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/dataops-with-container-images-and-multi-stage-builds">DataOps with Container Images and Multi-Stage Builds</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/using-the-snowflake-sql-api-with-typescript">Using the Snowflake SQL API with TypeScript</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/split-a-large-swagger-openapi-specification-into-smaller-documents">Split a large Open API or Swagger Specification into smaller documents</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python">Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/spark-gcp-featured-image.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/spark-in-the-google-cloud-platform-part-2">Spark in the Google Cloud Platform Part 2</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-02-29T00:00:00.000Z" itemprop="datePublished">February 29, 2020</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Apache Spark in GCP" src="/assets/images/spark-gcp-featured-image-86e0bfd36db759253fff66591b594d8b.png" width="213" height="155" class="img_ev3q"></p><p>In the previous post in this series <a href="https://cloudywithachanceofbigdata.com/spark-in-the-google-cloud-platform-part-1/" target="_blank" rel="noopener noreferrer"><strong>Spark in the Google Cloud Platform Part 1</strong></a>, we started to explore the various ways in which we could deploy Apache Spark applications in GCP. The first option we looked at was deploying Spark using Cloud DataProc, a managed Hadoop cluster with various ecosystem components included.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</div><div class="admonitionContent_S0QG"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>In this post, we will look at another option for deploying Spark in GCP – <em>a Spark Standalone cluster running on GKE</em>.</p><p>Spark Standalone refers to the in-built cluster manager provided with each Spark release. Standalone can be a bit of a misnomer as it sounds like a single instance – which it is not, standalone simply refers to the fact that it is not dependent upon any other projects or components – such as Apache YARN, Mesos, etc.</p><p>A Spark Standalone cluster consists of a Master node or instance and one of more Worker nodes. The Master node serves as both a master and a cluster manager in the Spark runtime architecture.</p><p>The Master process is responsible for marshalling resource requests on behalf of applications and monitoring cluster resources.</p><p>The Worker nodes host one or many Executor instances which are responsible for carrying out tasks.</p><p>Deploying a Spark Standalone cluster on GKE is reasonably straightforward. In the example provided in this post we will set up a private network (VPC), create a GKE cluster, and deploy a Spark Master pod and two Spark Worker pods (in a real scenario you would typically have many Worker pods).</p><p>Once the network and GKE cluster have been deployed, the first step is to create Docker images for both the Master and Workers.</p><p>The <code>Dockerfile</code> below can be used to create an image capable or running either the Worker or Master daemons:</p><iframe width="100%" frameborder="0" id="gist-a2828409021205b3f6587c824c59928d"></iframe><p>Note the shell scripts included in the <code>Dockerfile</code>: <code>spark-master</code> and <code>spark-worker</code>. These will be used later on by K8S deployments to start the relative Master and Worker daemon processes in each of the pods.</p><p>Next, we will use Cloud Build to build an image using the <code>Dockerfile</code> are store this in GCR (Google Container Registry), from the Cloud Build directory in our project we will run:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud builds submit --tag gcr.io/spark-demo-266309/spark-standalone</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Next, we will create Kubernetes deployments for our Master and Worker pods.</p><p>Firstly, we need to get cluster credentials for our GKE cluster named ‘spark-cluster’:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud container clusters get-credentials spark-cluster --zone australia-southeast1-a --project spark-demo-266309</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now from within the <code>k8s-deployments\deploy</code> folder of our project we will use the <code>kubectl</code> command to deploy the Master pod, service and the Worker pods</p><p>Starting with the Master deployment, this will deploy our Spark Standalone image into a container running the Master daemon process:</p><iframe width="100%" frameborder="0" id="gist-31bca11627167e0cd963103e4c7f11d2"></iframe><p>To deploy the Master, run the following:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-master-deployment.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The Master will expose a web UI on port 8080 and an RPC service on port 7077, we will need to deploy a K8S service for this, the YAML required to do this is shown here:</p><iframe width="100%" frameborder="0" id="gist-a72d3c38d7a3f94e88c7affd28a3034b"></iframe><p>To deploy the Master service, run the following:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-master-service.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now that we have a Master pod and service up and running, we need to deploy our Workers which are preconfigured to communicate with the Master service.</p><p>The YAML required to deploy the two Worker pods is shown here:</p><iframe width="100%" frameborder="0" id="gist-97ceb93ed35959c41d80fb8c025a7ba1"></iframe><p>To deploy the Worker pods, run the following:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-worker-deployment.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can now inspect the Spark processes running on your GKE cluster.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get deployments</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Shows...</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-master   1/1     1            1           7m45s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-worker   2/2     2            2           9s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Shows...</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                            READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-master-f69d7d9bc-7jgmj    1/1     Running   0          8m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-worker-55965f669c-rm59p   1/1     Running   0          24s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-worker-55965f669c-wsb2f   1/1     Running   0          24s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Next, as we need to expose the Web UI for the Master process we will create a <em>LoadBalancer</em> resource. The YAML used to do this is provided here:</p><iframe width="100%" frameborder="0" id="gist-56ee86f50f329f99679ff243bb00fb07"></iframe><p>To deploy the LB, you would run the following:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-ui-lb.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>NOTE</strong> This is just an example, for simplicity we are creating an external <em>LoadBalancer</em> with a public IP, this configuration is likely not be appropriate in most real scenarios, alternatives would include an internal <em>LoadBalancer</em>, retraction of Authorized Networks, a jump host, SSH tunnelling or IAP.</p><p>Now you’re up and running!</p><p>You can access the Master web UI from the Google Console link shown here:</p><p><a target="_blank" href="/assets/files/master-ui-link-c9d78a7032459c01291494b1c26e34ff.png"><img loading="lazy" alt="Accessing the Spark Master UI from the Google Cloud Console" src="/assets/images/master-ui-link-c9d78a7032459c01291494b1c26e34ff.png" width="1070" height="611" class="img_ev3q"></a></p><p>The Spark Master UI should look like this:</p><p><a target="_blank" href="/assets/files/spark-master-ui-fa6eecc91847995dea15601166516004.png"><img loading="lazy" alt="Spark Master UI" src="/assets/images/spark-master-ui-fa6eecc91847995dea15601166516004.png" width="1070" height="668" class="img_ev3q"></a></p><p>Next we will exec into a Worker pod, get a shell:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec -it spark-worker-55965f669c-rm59p -- sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now from within the shell environment of a Worker – which includes all of the Spark client libraries, we will submit a simple Spark application:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark-submit --class org.apache.spark.examples.SparkPi \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> --master spark://10.11.250.98:7077 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">/opt/spark/examples/jars/spark-examples*.jar 10000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can see the results in the shell, as shown here:</p><p><a target="_blank" href="/assets/files/spark-application-example-0f11e071394077ac8c678ed7d3e93791.png"><img loading="lazy" alt="Spark Pi Estimator Example" src="/assets/images/spark-application-example-0f11e071394077ac8c678ed7d3e93791.png" width="1022" height="932" class="img_ev3q"></a></p><p>Additionally, as all of the container logs go to Stackdriver, you can view the application logs there as well:</p><p><a target="_blank" href="/assets/files/container-logs-in-stackdriver-350797a4dd5cceab0b4aa12445adeed4.png"><img loading="lazy" alt="Container Logs in StackDriver" src="/assets/images/container-logs-in-stackdriver-350797a4dd5cceab0b4aa12445adeed4.png" width="1070" height="668" class="img_ev3q"></a></p><p>This is a simple way to get a Spark cluster running, it is not without its downsides and shortcomings however, which include the limited security mechanisms available (SASL, network security, shared secrets).</p><p>In the final post in this series we will look at Spark on Kubernetes, using Kubernetes as the Spark cluster manager and interacting with Spark using the Kubernetes API and control plane, see you then.</p><blockquote><p>Full source code for this article is available at: <a href="https://github.com/gamma-data/spark-on-gcp" target="_blank" rel="noopener noreferrer">https://github.com/gamma-data/spark-on-gcp</a></p></blockquote><p>The infrastructure coding for this example uses Powershell and Terraform, and is deployed as follows:</p><div class="language-powershell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-powershell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">PS &gt; .\run.ps1 private-network apply &lt;gcp-project&gt; &lt;region&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">PS &gt; .\run.ps1 gke apply &lt;gcp-project&gt; &lt;region&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/apache-spark">apache-spark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cloud-dataproc">cloud-dataproc</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/dataproc">dataproc</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/gcp">gcp</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/gke">gke</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/kubernetes">kubernetes</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/spark">spark</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/spark-gcp-featured-image.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/spark-in-the-google-cloud-platform-part-1">Spark in the Google Cloud Platform Part 1</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-02-14T00:00:00.000Z" itemprop="datePublished">February 14, 2020</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Apache Spark in GCP" src="/assets/images/spark-gcp-featured-image-86e0bfd36db759253fff66591b594d8b.png" width="213" height="155" class="img_ev3q"></p><p>I have been an avid Spark enthusiast since 2014 (the early days..). Spark has featured heavily in every project I have been involved with from data warehousing, ETL, feature extraction, advanced analytics to event processing and IoT applications. I like to think of it as a Swiss army knife for distributed processing.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</div><div class="admonitionContent_S0QG"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>Curiously enough, the first project I had been involved with for some years that did not feature the Apache Spark project was a green field GCP project which got me thinking… where does Spark fit into the GCP landscape?</p><p>Unlike the other major providers who use Spark as the backbone of their managed distributed ETL services with examples such as AWS Glue or the Spark integration runtime option in Azure Data Factory, Google’s managed ETL solution is Cloud DataFlow. Cloud DataFlow which is a managed Apache Beam service does not use a Spark runtime (there is a Spark Runner however this is not an option when using CDF). So where does this leave Spark?</p><p>My summation is that although Spark is not a first-class citizen in GCP (as far as managed ETL), it is not a second-class citizen either. This article will discuss the various ways Spark clusters and applications can be deployed within the GCP ecosystem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="quick-primer-on-spark">Quick Primer on Spark<a class="hash-link" href="#quick-primer-on-spark" title="Direct link to heading">​</a></h2><p>Every Spark application contains several components regardless of deployment mode, the components in the Spark runtime architecture are:</p><ul><li>the Driver</li><li>the Master</li><li>the Cluster Manager</li><li>the Executor(s), which run on worker nodes or Workers</li></ul><p>Each component has a specific role in executing a Spark program and all of the Spark components run in Java virtual machines (JVMs).</p><p><a target="_blank" href="/assets/files/spark-runtime-0b55b3d8793bab097b517603866abe16.png"><img loading="lazy" alt="Spark Runtime Architecture" src="/assets/images/spark-runtime-0b55b3d8793bab097b517603866abe16.png" width="752" height="420" class="img_ev3q"></a></p><p>Cluster Managers schedule and manage distributed resources (compute and memory) across the nodes of the cluster. Cluster Managers available for Spark include:</p><ul><li>Standalone</li><li>YARN (Hadoop)</li><li>Mesos</li><li>Kubernetes</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="spark-on-dataproc">Spark on DataProc<a class="hash-link" href="#spark-on-dataproc" title="Direct link to heading">​</a></h2><p>This is perhaps the simplest and most integrated approach to using Spark in the GCP ecosystem.</p><p>DataProc is GCP’s managed Hadoop Service (akin to AWS EMR or HDInsight on Azure). DataProc uses Hadoop/YARN as the Cluster Manager. DataProc clusters can be deployed on a private network (VPC using RFC1918 address space), supports encryption at Rest using Google Managed or Customer Managed Keys in KMS, supports autoscaling and the use of Preemptible Workers, and can be deployed in a HA config.</p><p>Furthermore, DataProc clusters can enforce strong authentication using Kerberos which can be integrated into other directory services such as Active Directory through the use of cross realm trusts.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deployment">Deployment<a class="hash-link" href="#deployment" title="Direct link to heading">​</a></h3><p>DataProc clusters can be deployed using the <code>gcloud dataproc clusters create</code> command or using IaC solutions such as Terraform. For this article I have included an example in the source code using the <code>gcloud</code> command to deploy a DataProc cluster on a private network which was created using Terraform.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="integration">Integration<a class="hash-link" href="#integration" title="Direct link to heading">​</a></h3><p>The beauty of DataProc is its native integration into IAM and the GCP service plane. Having been a long-time user of AWS EMR, I have found that the usability and integration are in many ways superior in GCP DataProc. Let’s look at some examples...</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="iam-and-iap-tcp-forwarding">IAM and IAP (TCP Forwarding)<a class="hash-link" href="#iam-and-iap-tcp-forwarding" title="Direct link to heading">​</a></h4><p>DataProc is integrated into Cloud IAM using various coarse grained permissions use as <code>dataproc.clusters.use</code> and simplified IAM Roles such as <code>dataproc.editor</code> or <code>dataproc.admin</code>. Members with bindings to the these roles can perform tasks such as submitting jobs and creating workflow templates (which we will discuss shortly), as well as accessing instances such as the master node instance or instances in the cluster using IAP (TCP Forwarding) without requiring a public IP address or a bastion host.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dataproc-jobs-and-workflows">DataProc Jobs and Workflows<a class="hash-link" href="#dataproc-jobs-and-workflows" title="Direct link to heading">​</a></h4><p>Spark jobs can be submitted using the console or via <code>gcloud dataproc jobs submit</code> as shown here:</p><p><a target="_blank" href="/assets/files/dataproc-spark-job-953a699ceef00b9e3b0f05068a844a56.png"><img loading="lazy" alt="Submitting a Spark Job using gcloud dataproc jobs submit" src="/assets/images/dataproc-spark-job-953a699ceef00b9e3b0f05068a844a56.png" width="1097" height="499" class="img_ev3q"></a></p><p>Cluster logs are natively available in StackDriver and standard out from the Spark Driver is visible from the console as well as via <code>gcloud</code> commands.</p><p>Complex Workflows can be created by adding Jobs as Steps in Workflow Templates using the following command:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud dataproc workflow-templates add-job spark</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="optional-components-and-the-component-gateway">Optional Components and the Component Gateway<a class="hash-link" href="#optional-components-and-the-component-gateway" title="Direct link to heading">​</a></h4><p>DataProc provides you with a Hadoop cluster including YARN and HDFS, a Spark runtine – which includes Spark SQL and SparkR. DataProc also supports several optional components including Anaconda, Jupyter, Zeppelin, Druid, Presto, and more.</p><p>Web interfaces to some of these components as well as the management interfaces such as the Resource Manager UI or the Spark History Server UI can be accessed through the Component Gateway.</p><p>This is a Cloud IAM integrated gateway (much like IAP) which can allow access through an authenticated and authorized console session to web UIs in the cluster – without the need for SSH tunnels, additional firewall rules, bastion hosts, or public IPs. Very cool.</p><p>Links to the component UIs as well as built in UIs like the YARN Resource Manager UI are available directly from through the console.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="jupyter">Jupyter<a class="hash-link" href="#jupyter" title="Direct link to heading">​</a></h4><p>Jupyter is a popular notebook application in the data science and analytics communities used for reproducible research. DataProc’s Jupyter component provides a ready-made Spark application vector using PySpark. If you have also installed the Anaconda component you will have access to the full complement of scientific and mathematic Python packages such as Pandas and NumPy which can be used in Jupyter notebooks as well. Using the Component Gateway, Jupyer notebooks can be accessed directly from the Google console as shown here:</p><p><a target="_blank" href="/assets/files/dataproc-jupyter-notebook-14d165b5563c903961beb68bef757a82.png"><img loading="lazy" alt="Jupyter Notebooks using DataProc" src="/assets/images/dataproc-jupyter-notebook-14d165b5563c903961beb68bef757a82.png" width="1359" height="924" class="img_ev3q"></a></p><p>From this example you can see that I accessed source data from a GCS bucket and used HDFS as local scratch space.</p><p>Furthermore, notebooks are automagically saved in your integrated Cloud Storage DataProc staging bucket and can be shared amongst analysts or accessed at a later time. These notebooks also persist beyond the lifespan of the cluster.</p><p>Next up we will look at deploying a Spark Standalone cluster on a GKE cluster, see you then!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/apache-spark">apache-spark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/gcp">gcp</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/spark">spark</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/cloud-sql-federated-queries.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/query-cloud-sql-through-big-query">Query Cloud SQL through Big Query</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-02-08T00:00:00.000Z" itemprop="datePublished">February 8, 2020</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="cloudsql federated queries" src="/assets/images/cloud-sql-federated-queries-8af6a8b8a2536a9c78de7d33b3a5c626.png" width="150" height="157" class="img_ev3q"></p><p>This article demonstrates Cloud SQL federated queries for Big Query, a neat and simple to use feature.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="connecting-to-cloud-sql">Connecting to Cloud SQL<a class="hash-link" href="#connecting-to-cloud-sql" title="Direct link to heading">​</a></h2><p>One of the challenges presented when using Cloud SQL on a private network (VPC) is providing access to users. There are several ways to accomplish this which include:</p><ul><li>open the database port on the VPC Firewall (5432 for example for Postgres) and let users access the database using a command line or locally installed GUI tool <em>(may not be allowed in your environment)</em></li><li>provide a web based interface deployed on your VPC such as PGAdmin deployed on a GCE instance or GKE pod <em>(adds security and management overhead)</em></li><li>use the Cloud SQL proxy <em>(requires additional software to be installed and configured)</em></li></ul><p>In additional, all of the above solutions require direct IP connectivity to the instance which may not always be available. Furthermore each of these operations requires the user to present some form of authentication – in many cases the database user and password which then must be managed at an individual level.</p><p>Enter Cloud SQL federated queries for Big Query…</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="big-query-federated-queries-for-cloud-sql">Big Query Federated Queries for Cloud SQL<a class="hash-link" href="#big-query-federated-queries-for-cloud-sql" title="Direct link to heading">​</a></h2><p>Big Query allows you to query tables and views in Cloud SQL (currently MySQL and Postgres) using the Federated Queries feature. The queries could be authorized views in Big Query datasets for example.</p><p>This has the following advantages:</p><ul><li>Allows users to authenticate and use the GCP console to query Cloud SQL</li><li>Does not require direct IP connectivity to the user or additional routes or firewall rules</li><li>Leverages Cloud IAM as the authorization mechanism – rather that unmanaged db user accounts and object level permissions</li><li>External queries can be executed against a read replica of the Cloud SQL instance to offload query IO from the master instance</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="setting-it-up">Setting it up<a class="hash-link" href="#setting-it-up" title="Direct link to heading">​</a></h2><p>Setting up big query federated queries for Cloud SQL is exceptionally straightforward, a summary of the steps are provided below:</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-1-enable-a-public-ip-on-the-cloud-sql-instance">Step 1. Enable a Public IP on the Cloud SQL instance<a class="hash-link" href="#step-1-enable-a-public-ip-on-the-cloud-sql-instance" title="Direct link to heading">​</a></h3><p>This sounds bad, but it isn’t really that bad. You need to enable a public interface for Big Query to be able to establish a connection to Cloud SQL, however this is not accessed through the actual public internet – rather it is accessed through the Google network using the back end of the front end if you will.</p><p>Furthermore, you configure an empty list of authorized networks which effectively shields the instance from the public network, this can be configured in Terraform as shown here:</p><iframe width="100%" frameborder="0" id="gist-81c57a80a7e588b98ea7d294dbaee242"></iframe><p>This configuration change can be made to a running instance as well as during the initial provisioning of the instance.</p><p>As shown below you will get a warning dialog in the console saying that you have no authorized networks - this is by design.</p><p><a target="_blank" href="/assets/files/cloud-sql-publicip-screenshot-53f21feadbfaf3be93f69de3ce771c2f.png"><img loading="lazy" alt="Cloud SQL Public IP Enabled with No Authorized Networks" src="/assets/images/cloud-sql-publicip-screenshot-53f21feadbfaf3be93f69de3ce771c2f.png" width="1139" height="775" class="img_ev3q"></a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-2-create-a-big-query-dataset-which-will-be-used-to-execute-the-queries-to-cloud-sql">Step 2. Create a Big Query dataset which will be used to execute the queries to Cloud SQL<a class="hash-link" href="#step-2-create-a-big-query-dataset-which-will-be-used-to-execute-the-queries-to-cloud-sql" title="Direct link to heading">​</a></h3><p>Connections to Cloud SQL are defined in a Big Query dataset, this can also be use to control access to Cloud SQL using authorized views controlled by IAM roles.</p><iframe width="100%" frameborder="0" id="gist-8a4beaab134a1c72613347b5822d1724"></iframe><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-3-create-a-connection-to-cloud-sql">Step 3. Create a connection to Cloud SQL<a class="hash-link" href="#step-3-create-a-connection-to-cloud-sql" title="Direct link to heading">​</a></h3><p>To create a connection to Cloud SQL from Big Query you must first enable the BigQuery Connection API, this is done at a project level.</p><p>As this is a fairly recent feature there isn&#x27;t great coverage with either the <strong><code>bq</code></strong> tool or any of the Big Query client libraries to do this so we will need to use the console for now...</p><p>Under the <em><strong>Resources</strong></em> -&gt; <strong><em>Add Data</em></strong> link in the left hand panel of the Big Query console UI, select <strong><em>Create Connection</em></strong>. You will see a side info panel with a form to enter connection details for your Cloud SQL instance.</p><p>In this example I will setup a connection to a Cloud SQL read replica instance I have created:</p><p><a target="_blank" href="/assets/files/big-query-add-connection-67a0236996204c84c7608a8e6b8f4875.png"><img loading="lazy" src="/assets/images/big-query-add-connection-67a0236996204c84c7608a8e6b8f4875.png" width="959" height="775" class="img_ev3q"></a></p><p>Creating a Big Query Connection to Cloud SQL</p><p>More information on the Big Query Connections API can be found at: <a href="https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest" target="_blank" rel="noopener noreferrer">https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest</a></p><p>The following permissions are associated with connections in Big Query:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.create  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.get  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.list  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.use  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.update  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.delete</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>These permissions are conveniently combined into the following predefined roles:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">roles/bigquery.connectionAdmin    (BigQuery Connection Admin)         </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">roles/bigquery.connectionUser     (BigQuery Connection User)          </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="step-4-query-away">Step 4. Query away!<a class="hash-link" href="#step-4-query-away" title="Direct link to heading">​</a></h3><p>Now the connection to Cloud SQL can be accessed using the <strong><code>EXTERNAL_QUERY</code></strong> function in Big Query, as shown here:</p><p><a target="_blank" href="/assets/files/cloud-sql-federated-queries-screenshot-34e49ac369753d8551095e92b7fc6264.png"><img loading="lazy" alt="Querying Cloud SQL from Big Query" src="/assets/images/cloud-sql-federated-queries-screenshot-34e49ac369753d8551095e92b7fc6264.png" width="1373" height="730" class="img_ev3q"></a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/big-query">big-query</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/bigquery">bigquery</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cloudsql">cloudsql</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/gcp">gcp</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/googlecloudplatform">googlecloudplatform</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/cloudsql-featured-image.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/google-cloud-sql-availability-for-postgresql-read-replicas">Google Cloud SQL – Availability for PostgreSQL – Part II (Read Replicas)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-01-24T00:00:00.000Z" itemprop="datePublished">January 24, 2020</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="CloudSQL HA" src="/assets/images/cloudsql-featured-image-896f0c764d7310d88c5cc9461f3feb6c.png" width="151" height="151" class="img_ev3q"></p><p>In this post we will look at read replicas as an additional method to achieve multi zone availability for Cloud SQL, which gives us - in turn - the ability to offload (potentially expensive) IO operations such as user created backups or read operations without adding load to the master instance.</p><p>In the previous post in this series we looked at Regional availability for PostgreSQL HA using Cloud SQL:</p><p><a href="https://cloudywithachanceofbigdata.com/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql-part-i/" target="_blank" rel="noopener noreferrer"><strong>Google Cloud SQL – Availability, Replication, Failover for PostgreSQL – Part I</strong></a></p><p>Recall that this option was simple to implement and worked relatively seamlessly and transparently with respect to zonal failover.</p><p>Now let&#x27;s look at read replicas in Cloud SQL as an additional measure for availability.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deploying-read-replicas">Deploying Read Replica(s)<a class="hash-link" href="#deploying-read-replicas" title="Direct link to heading">​</a></h2><p>Deploying read replicas is slightly more involved than simple regional (high) availability, as you will need to define each replica or replicas as a separate Cloud SQL instance which is a slave to the primary instance (the master instance).</p><p>An example using Terraform is provided here, starting by creating the master instance:</p><iframe width="100%" frameborder="0" id="gist-34371a3c7edab140e70208cd7710c25a"></iframe><p>Next you would specify one or more read replicas (typically in a zone other than the zone the master is in):</p><iframe width="100%" frameborder="0" id="gist-980f2d6461db0613b4090413041b5ec5"></iframe><p>Note that several of the options supplied are omitted when creating a read replica database instance, such as the backup and maintenance options - as these operations cannot be performed on a read replica as we will see later.</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-1-212da608bf5ba2aa73198627a0cfe3e1.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-1-212da608bf5ba2aa73198627a0cfe3e1.png" alt="Cloud SQL Instances - showing master and replica"></a><figcaption class="figure-caption">Cloud SQL Instances - showing master and replica</figcaption></figure><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-2-ec13f6b5f93493f369db3f4c11987507.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-2-ec13f6b5f93493f369db3f4c11987507.png" alt="Cloud SQL Master Instance"></a><figcaption class="figure-caption">Cloud SQL Master Instance</figcaption></figure><p>Voila! You have just set up a master instance (the primary instance your application and/or users will connect to) along with a read replica in a different zone which will be asynchronously updated as changes occur on the master instance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="read-replicas-in-action">Read Replicas in Action<a class="hash-link" href="#read-replicas-in-action" title="Direct link to heading">​</a></h2><p>Now that we have created a read replica, lets see it in action. After connecting to the read replica (like you would any other instance), attempt to access a table that has <strong><em>not</em></strong> been created on the master as shown here:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-3-8d16fdd58297948424c733b59cb58ff5.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-3-8d16fdd58297948424c733b59cb58ff5.png" alt="SELECT operation from the replica instance"></a><figcaption class="figure-caption">SELECT operation from the replica instance</figcaption></figure><p>Now create the table and insert some data on the <strong><em>master</em></strong> instance:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-4-c207e904d4880b5f799bcfdabf7de36a.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-4-c207e904d4880b5f799bcfdabf7de36a.png" alt="Create a table and insert a record on the master instance"></a><figcaption class="figure-caption">Create a table and insert a record on the master instance</figcaption></figure><p>Now try the select operation on the <strong><em>replica</em></strong> instance:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-5-a9412ef5afba9855221998e5551cb19e.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-5-a9412ef5afba9855221998e5551cb19e.png" alt="SELECT operation from the replica instance (after changes have been made on the master)"></a><figcaption class="figure-caption">SELECT operation from the replica instance (after changes have been made on the master)</figcaption></figure><p>It works!</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="some-points-to-note-about-cloud-sql-read-replicas">Some Points to Note about Cloud SQL Read Replicas<a class="hash-link" href="#some-points-to-note-about-cloud-sql-read-replicas" title="Direct link to heading">​</a></h2><ul><li>Users connect to a read replica as a normal database connection (as shown above)</li><li>Google managed backups (using the console or <code>gcloud sql backups create ..</code> ) can <strong><em>NOT</em></strong> be performed against replica instances</li><li>Read replicas can be used to offload IO intensive operations from the the master instance - such as user managed backup operations (e.g. <code>pg_dump</code>)</li></ul><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-6-c367a62c5d3e480fa25ea4d0ea2669cf.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-6-c367a62c5d3e480fa25ea4d0ea2669cf.png" alt="pg_dump operation against a replica instance"></a><figcaption class="figure-caption">pg_dump operation against a replica instance</figcaption></figure><ul><li><strong>BE CAREFUL</strong> Despite their name, read replicas are <strong>NOT</strong> read only, updates can be made which will NOT propagate back to the master instance - you could get yourself in an awful mess if you allow users to perform <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>CREATE</code> or <code>DROP</code> operations against replica instances.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="promoting-a-read-replica">Promoting a Read Replica<a class="hash-link" href="#promoting-a-read-replica" title="Direct link to heading">​</a></h2><p>If required a read replica can be promoted to a standalone Cloud SQL instance, which is another DR option. Keep in mind however as the read replica is updated in an asynchronous manner, promotion of a read replica may result in a loss of data (hopefully not much but a loss nonetheless). Your application RPO will dictate if this is acceptable or not.</p><p>Promotion of a read replica is reasonably straightforward as demonstrated here using the console:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-7-a4ec8a31ed6601a9e7253ce408893a90.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-7-a4ec8a31ed6601a9e7253ce408893a90.png" alt="Promoting a read replica using the console"></a><figcaption class="figure-caption">Promoting a read replica using the console</figcaption></figure><p>You can also use the following <code>gcloud</code> command:</p><p> gcloud sql instances promote-replica  &lt;replica<!-- -->_<!-- -->instance<!-- -->_<!-- -->name&gt;</p><p>Once you click on the <em>Promote Replica</em> button you will see the following warning:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-8-310006339cb5a3d3c491dfb00fc86c88.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-8-310006339cb5a3d3c491dfb00fc86c88.png" alt=""></a><figcaption class="figure-caption"></figcaption></figure><p><em>Promoting a read replica using the console</em></p><p>This simply states that once you promote the replica instance your instance will become an independent instance with no further relationship with the master instance. Once accepted and the promotion process is complete, you can see that you now have two independent Cloud SQL instances (as advertised!):</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-9-5cbdcea2c489b064bbbc1bad3617fbce.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-9-5cbdcea2c489b064bbbc1bad3617fbce.png" alt="Promoted Cloud SQL instance"></a><figcaption class="figure-caption">Promoted Cloud SQL instance</figcaption></figure><p>Some of the options you would normally configure with a master instance would need to be configured on the promoted replica instance - such as high availability, maintenance and scheduled backups - but in the event of a zonal failure you would be back up and running with virtually no data loss!</p><blockquote><p>Full source code for this article is available at: <a href="https://github.com/gamma-data/cloud-sql-postgres-availability-tutorial" target="_blank" rel="noopener noreferrer">https://github.com/gamma-data/cloud-sql-postgres-availability-tutorial</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cloudsql">cloudsql</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/gcp">gcp</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/ha">ha</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/highavailability">highavailability</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/postgresql">postgresql</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/service-mesh-1.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/introducing-service-mesh-part-ii">Introducing Service Mesh Part II</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-01-21T00:00:00.000Z" itemprop="datePublished">January 21, 2020</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/tomklimovskigamma" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="http://2.gravatar.com/avatar/58faa98ad68138dd1997f828f00a882e?s=80" alt="Tom Klimovski"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/tomklimovskigamma" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Tom Klimovski</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Cloud Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Service Mesh" src="/assets/images/service-mesh-1-d7868fc724132b5e947edad350e8e1ed.png" width="151" height="106" class="img_ev3q"></p><p>This is a follow up to the previous post:</p><p><a href="https://cloudywithachanceofbigdata.com/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know/" target="_blank" rel="noopener noreferrer"><strong>Sick of hearing about Service Mesh? Here’s what you need to know...</strong></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="refresher">Refresher<a class="hash-link" href="#refresher" title="Direct link to heading">​</a></h2><p>A refresher on the data plane, and what the userspace proxy can perform:</p><ul><li><strong>Routing:</strong> Given a REST request for <code>/hello</code> from the local service instance, where should that request be sent?</li><li><strong>Load Balancing:</strong> Once routing has done its job, to which upstream service instance should the request be sent? With what timeout? If the request fails, should it be retried?</li><li><strong>Authorisation and Authentication:</strong> For requests that are incoming, can cryptographic functions determine the authenticity of that requests? Is the called allowed to invoke the requested endpoint?</li><li><strong>Observability:</strong> Detailed logging, statistics, distributed tracing data so that operators can understand the traffic flow and debug problems as they occur</li><li><strong>Service Discovery:</strong> What backend/upstream service instances are available?</li><li><strong>Health Checking:</strong> Are upstream service instances healthy and ready to accept traffic?</li></ul><p>The control plane is slightly less complex. For the data plane to act in a coordinated fashion, the control plane gives you the machinery to make that happen. This is the magical part of the service mesh; the control plane takes a set of isolated sidecar proxies and turns them into a distributed system. The control plane in turn provides an API to allow the user to modify and inspect the behaviour of the data plane.</p><p>You can see from the diagram below the proxies are right next to the service in the same node. We usually call those &#x27;sidecar&#x27; containers.</p><p><a target="_blank" href="/assets/files/control-data-plane-5cd2a8598fa552440a65343abd9e762c.png"><img loading="lazy" src="/assets/images/control-data-plane-5cd2a8598fa552440a65343abd9e762c.png" width="1068" height="689" class="img_ev3q"></a></p><p>The diagram above gives you a high level indication of what the service mesh would look like. What if I don&#x27;t have many services? Then the service mesh probably isn&#x27;t for you. That&#x27;s a whole lot of machinery to run a single proxy! Having said this, if your solution is running hundreds or thousands of services, then you&#x27;re going to require a whole heap of proxies.</p><p>So there you have it. The service mesh with its control and data plane. To put it simply, the goal of the control plane is to monitor and set a policy that will eventually be enacted by the data plane.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why">Why?<a class="hash-link" href="#why" title="Direct link to heading">​</a></h2><p>You&#x27;ve taken over a project, and the security team have mandated the use of the service mesh. You&#x27;ve never used it yourself before, and the confusion as to why we need another thing is getting you down. An additional thing next to my container that will add latency? And consume resources? And I have to maintain it?! Why would anyone need or want this?</p><p>While there are a few answers to this, the most important answer is something I alluded to in an example in part 1 of this series: this design is a great way to add additional logic into the system. Not only can you add additional logic (to containers possibly outside of your control) but you can do this uniformly across the entire mesh! <em>The service mesh gives you features that are critical for running software that&#x27;s uniform across your whole stack</em></p><p>The set of features that the service mesh can provide include reliability features (Retries, timeouts etc), observability features (latencies, volume etc) and security features (mTLS, access control etc).</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lets-break-it-down">Let&#x27;s break it down<a class="hash-link" href="#lets-break-it-down" title="Direct link to heading">​</a></h2><p><strong>Server-side software relies on these critical features</strong> If you&#x27;re building any type of modern server-side software that&#x27;s predicated on multiple services, think API&#x27;s and web-apps, and if you&#x27;re continually adding features to this in a short timeframe, then all the features listed above become critical for you. Your applications must be reliable, observable and most importantly secure. This is exactly what the service mesh helps you with.</p><p><strong>One view to rule them all</strong> The features mentioned above are language-agnostic, don&#x27;t care about your framework, who wrote it or any part of your development life cycle. They give you, your team and your company a consistent way to deploy changes across your service landscape</p><p><strong>Decoupled from application code</strong> It&#x27;s important to have a single place to include application and business logic, and not have the nightmare of managing that in multiple components of your system. The core stewardship of the functionality that the service mesh provides lies at the <em>platform level</em>. This includes maintenance, deployments, operation etc. The application can be updated and deployed by developers maintaining the application, and the service mesh can change without the application being involved.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="in-short">In short<a class="hash-link" href="#in-short" title="Direct link to heading">​</a></h2><p>Yes, while the features of the service mesh could be implemented as application code, this solution would not help in driving uniform features sets across the whole system, which is the value proposition for the service mesh.</p><p><em>If you&#x27;re a business-logic developer</em>, you probably don&#x27;t need to worry about the service mesh. Keep pumping out that new fangled business logic that makes the software oh-so-usable</p><p><em>If you&#x27;re in a platform role</em> and most likely using <em>Kubernetes</em>, then you should be right on top of the service mesh! That is unless your architecture dictates a monolith. You&#x27;re going to have a lot of services talking to one another, all tied together with an overarching dependency.</p><p><em>If you&#x27;re in a platform role with no Kubernetes</em> but a bunch of microservices, you should maybe care a little bit about the service mesh, but without the power of Kubernetes and the ease of deployment it brings, you&#x27;ll have to weigh up how you intend to manage all those proxies.</p><p>I hope you enjoyed this article, please feel free to reach out to me at:</p><p>Tom Klimovski<br>
<!-- -->Principal Consultant, Gamma Data<br>
<a href="mailto:tom.klimovski@gammadata.io" target="_blank" rel="noopener noreferrer">tom.klimovski@gammadata.io</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/k-8-s">k8s</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/kubernetes">kubernetes</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/service-mesh">service-mesh</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/servicemesh">servicemesh</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/page/8"><div class="pagination-nav__label">Newer Entries</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/page/10"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Blog</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">Home</a></li><li class="footer__item"><a class="footer__link-item" href="/archive">Archive</a></li><li class="footer__item"><a class="footer__link-item" href="/tags">Tags</a></li></ul></div><div class="col footer__col"><div class="footer__title">Sponsors</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackql.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">StackQL<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://gammadata.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gamma Data<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.windrate.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">WindRate<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.f1e5cb7b.js"></script>
<script src="/assets/js/main.965b623c.js"></script>
</body>
</html>