<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Cloudy with a chance of Big Data Blog Feed</title>
        <link>https://cloudywithachanceofbigdata.com/</link>
        <description>Cloud and data design patterns</description>
        <lastBuildDate>Sat, 18 Dec 2021 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Automating Snowflake Role Based Storage Integration for AWS]]></title>
            <link>https://cloudywithachanceofbigdata.com/automating-snowflake-role-based-storage-integration-for-aws</link>
            <guid>automating-snowflake-role-based-storage-integration-for-aws</guid>
            <pubDate>Sat, 18 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Automate the creation of a Storage Integration in Snowflake which allows a Snowflake External Stage to access objects in your AWS S3 bucket.]]></description>
            <content:encoded><![CDATA[<p>I have used the instructions <a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe-auto-s3.html">here</a> to configure Snowpipe for several projects.  </p><p>Although it is accurate, it is entirely click-ops oriented.  I like to automate (and script) everything, so I have created a fully automated implementation using PowerShell, the <code>aws</code> and <code>snowsql</code> CLIs.  </p><p>The challenge is that you need to go back and forth between AWS and Snowflake, exchanging information from each platform with the other.  </p><h2>Overview</h2><p>A Role Based Storage Integration in Snowflake allows a user (an AWS user arn) in your Snowflake account to use a role in your AWS account, which in turns enables access to S3 and KMS resources used by Snowflake for an external stage.  </p><p>The following diagram explains this (along with the PlantUML code used to create the diagram..):  </p><div values="[object Object],[object Object]"><div value="overview"><p><a href="images/snowflake-aws-storage-integration.png"><img src="images/snowflake-aws-storage-integration.png" alt="Snowflake S3 Storage Integration"/></a> </p></div><div value="plantuml"><pre><code class="language-plantuml">@startuml

skinparam rectangle&lt;&lt;boundary&gt;&gt; {
    Shadowing false
    StereotypeFontSize 0
    FontColor #444444
    BorderColor #444444
    BorderStyle dashed
}

skinparam defaultTextAlignment center

!$imgroot = &quot;https://github.com/avensolutions/plantuml-cloud-image-library/raw/main/images&quot;

!unquoted procedure $AwsIam($alias, $label, $techn, $descr=&quot;&quot;, $stereo=&quot;AWS IAM&quot;)
    rectangle &quot;==$label\n\n&lt;img:$imgroot/aws/SecurityIdentityCompliance/Iam.png&gt;\n//&lt;size:12&gt;[$techn]&lt;/size&gt;//&quot; &lt;&lt;$stereo&gt;&gt; as $alias #white
!endprocedure

!unquoted procedure $AwsS3($alias, $label, $techn, $descr=&quot;&quot;, $stereo=&quot;AWS S3&quot;)
    rectangle &quot;==$label\n\n&lt;img:$imgroot/aws/Storage/S3.png&gt;\n//&lt;size:12&gt;[$techn]&lt;/size&gt;//&quot; &lt;&lt;$stereo&gt;&gt; as $alias #white
!endprocedure

!unquoted procedure $Snowflake($alias, $label, $techn, $descr=&quot;&quot;, $stereo=&quot;Snowflake&quot;)
    rectangle &quot;==$label\n\n&lt;img:$imgroot/snowflake/snowflakeDB.png{scale=0.70}&gt;\n//&lt;size:12&gt;[$techn]&lt;/size&gt;//&quot; &lt;&lt;$stereo&gt;&gt; as $alias #white
!endprocedure

rectangle &quot;Snowflake&quot; &lt;&lt;boundary&gt;&gt; {
    $AwsIam(user, Snowflake IAM User, AWS IAM User)
    $Snowflake(int, Storage Integration, Storage Integration)
    $Snowflake(stage, External Stage, Stage)
}

rectangle &quot;AWS&quot; &lt;&lt;boundary&gt;&gt; {
    $AwsS3(bucket, Stage Bucket, AWS S3 Bucket)
    $AwsIam(role, Snowflake Access Role, IAM Role)
    $AwsIam(policy, Snowflake Access Policy, IAM Policy)
}

stage -UP-&gt; int : uses
int -RIGHT-&gt; user : uses
user -RIGHT-&gt; role : uses
policy -UP-&gt; role : attached to
role -RIGHT-&gt; bucket : allows access to

@enduml
</code></pre></div></div><h2>Setup</h2><p>Some prerequisites (removed for brevity):  </p><ol><li>set the following variables in your script:  </li></ol><ul><li><code>$accountid</code> – your AWS account ID</li><li><code>$bucketname</code> – the bucket you are letting Snowflake use as an External Stage</li><li><code>$bucketarn</code> – used in policy statements (you could easily derive this from the bucket name)</li><li><code>$kmskeyarn</code> – assuming you are used customer managed encryption keys, your Snowflake storage integration will need to use these to decrypt data in the stage</li><li><code>$prefix</code> – if you want to set up granular access (on a key/path basis)</li></ul><ol start="2"><li>Configure Snowflake access credentials using environment variables or using the <code>~/.snowsql/config</code> file (you should definitely use the <code>SNOWSQL_PWD</code> env var for your password however)</li><li>Configure access to AWS using <code>aws configure</code></li></ol><p>:::note</p><p>The actions performed in both AWS and Snowflake required privileged access on both platforms.</p><p>:::</p><h2>The Code</h2><p>I have broken this into steps, the complete code is included at the end of the article.  </p><h3>Create Policy Documents</h3><p>You will need to create the policy documents to allow the role you will create to access objects in the target S3 bucket, you will also need an initial “Assume Role” policy document which will be used to create the role and then updated with information you will get from Snowflake later.  </p><div id="73d507126c114e6ee7398226cf004f55"></div><h3>Create Snowflake Access Policy</h3><p>Use the <code>snowflake_policy_doc.json</code> policy document created in the previous step to create a managed policy, you will need the <code>arn</code> returned in a subsequent statement.  </p><div id="65be4f7c104f92fa3dbf9342813b3fd2"></div><h3>Create Snowflake IAM Role</h3><p>Use the initial <code>assume_role_policy_doc.json</code> created to create a new Snowflake access role, you will need the <code>arn</code> for this resource when you configure the Storage Integration in Snowflake.  </p><div id="e1bdd5316fe7cb106de1edcff77d8e2b"></div><h3>Attach S3 Access Policy to the Role</h3><p>Now you will attach the <code>snowflake-access-policy</code> to the <code>snowflake-access-role</code> using the <code>$policyarn</code> captured from the policy creation statement.  </p><div id="d2d54b43e379a26bd264a4c97939250c"></div><h3>Create Storage Integration in Snowflake</h3><p>Use the <code>snowsql</code> CLI to create a Storage Integration in Snowflake supplying the <code>$rolearn</code> captured from the role creation statement.  </p><div id="8e4617227bcd68be74c2a5d694c85f91"></div><h3>Get <code>STORAGE_AWS_IAM_USER_ARN</code> and <code>STORAGE_AWS_EXTERNAL_ID</code></h3><p>You will need the <code>STORAGE_AWS_IAM_USER_ARN</code> and <code>STORAGE_AWS_EXTERNAL_ID</code> values for the storage integration you created in the previous statement, these will be used to updated the assume role policy in your <code>snowflake-access-role</code>.  </p><div id="14dbf570030cad1a46d88d2e87006c8e"></div><h3>Update Snowflake Access Policy</h3><p>Using the <code>STORAGE_AWS_IAM_USER_ARN</code> and <code>STORAGE_AWS_EXTERNAL_ID</code> values retrieved in the previous statements, you will update the <code>assume-role-policy</code> for the <code>snowflake-access-role</code>.  </p><div id="944c39205e142de9a76266f7f3cd260b"></div><h3>Test the Storage Integration</h3><p>To test the connectivity between your Snowflake account and your AWS external stage using the Storage Integartion just created, create a stage as shown here:  </p><div id="99c24e8c80c6556fe381cf64c841f739"></div><p>Now list objects in the stage (assuming there are any).  </p><pre><code class="language-js">list @ledger_file_stage;
</code></pre><p>This should just work!  You can use your storage integration to create different stages for different paths in your External Stage bucket and use both of these objects to create Snowpipes for automated ingestion.  Enjoy!  </p><h3>Complete Code</h3><p>The complete code for this example is shown here:  </p><div id="5f4cba25f4eac380d63f5829c56d0306"></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Simplifying Large CloudFormation Templates using Jsonnet]]></title>
            <link>https://cloudywithachanceofbigdata.com/simplifying-large-cloudformation-templates-using-jsonnet</link>
            <guid>simplifying-large-cloudformation-templates-using-jsonnet</guid>
            <pubDate>Sun, 21 Nov 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[A simple pattern to break up large CloudFormation templates into smaller, more manageable modules using Jsonnet and GitLab CI.]]></description>
            <content:encoded><![CDATA[<p>CloudFormation templates in large environments can grow beyond a manageable point.  This article provides one approach to breaking up CloudFormation templates into modules which can be imported and used to create a larger template to deploy a complex AWS stack – using Jsonnet.  </p><p>Jsonnet is a json pre-processing and templating library which includes features including user defined and built-in functions, objects, and inheritance amongst others.  If you are not familiar with Jsonnet, here are some good resources to start with:  </p><ul><li><a href="https://jsonnet.org/">Jsonnet</a></li><li><a href="https://cloudywithachanceofbigdata.com/using-jsonnet-to-configure-multiple-environments">Blog Article: Using Jsonnet to Configure Multiple Environments</a></li><li><a href="https://docs.infraql.io/blog/using-the-jsonnet-map-function">Blog Article: Using the Jsonnet Map Function</a></li></ul><h2>Advantages</h2><p>Using Jsonnet you can use imports to break up large stacks into smaller files scoped for each resource.  This approach makes CloudFormation template easier to read and write and allows you to apply the DRY (Do Not Repeat Yourself) coding principle (not possible with native CloudFormation templates.  </p><p>Additionally, although as the template fragments are in Jsonnet format, you can add annotations or comments to your code similar to YAML (not possible with a JSON template alone), although the rendered template is in legal CloudFormation Json format.  </p><h2>Process Overview</h2><p>The process is summarised here: </p><p><a href="images/cloudformation-jsonnet.png"><img src="images/cloudformation-jsonnet.png" alt="CloudFormation and Jsonnet"/></a> </p><h2>Code</h2><p>This example will deploy a stack with a VPC and an S3 bucket with logging.  The project directory structure would look like this:  </p><pre><code class="language-bash">templates/
├─ includes/
│  ├─ vpc.libsonnet
│  ├─ s3landingbucket.libsonnet
│  ├─ s3loggingbucket.libsonnet
│  ├─ tags.libsonnet
├─ template.jsonnet
</code></pre><p>Lets look at all of the constituent files:  </p><h3><code>template.jsonnet</code></h3><p>This is the root document which will be processed by Jsonnet to render a legal CloudFormation JSON template.  It will import the other files in the includes directory.  </p><div id="8f2cc0c464de762f73b3f81c75a13832"></div><h3><code>includes/tags.libsonnet</code></h3><p>This code module is used to generate re-usable tags for other resources (DRY).  </p><div id="82e21743e845355ba0ef7240f1f7327a"></div><h3><code>includes/vpc.libsonnet</code></h3><p>This code module defines a VPC resource to be created with CloudFormation.  </p><div id="e79189bbc1cfb8b72bd860c6381f6130"></div><h3><code>includes/s3loggingbucket.libsonnet</code></h3><p>This code module defines an S3 bucket resource to be created in the stack which will be used for logging for other buckets.  </p><div id="187c97deca224617b064c4028ebbbee2"></div><h3><code>includes/s3landingbucket.libsonnet</code></h3><p>This code module defines an S3 landing bucket resource to be created in the stack.  </p><div id="c0dc5d868809f98ef672aca738bb1e5e"></div><h2>Testing</h2><p>To test the pre-processing, you will need a Jsonnet binary/executable for your environment.  You can find Docker images which include this for you, or you could build it yourself.  </p><p>Once you have a compiled binary, you can run the following to generate a rendered CloudFormation template.  </p><pre><code class="language-bash">jsonnet template.jsonnet -o template.json
</code></pre><p>You can validate this template using the AWS CLI as shown here:  </p><pre><code class="language-bash">aws cloudformation validate-template --template-body file://template.json
</code></pre><h2>Deployment</h2><p>In a previous article, <a href="https://cloudywithachanceofbigdata.com/aws-deployments-with-cloudformation-and-gitlab-ci">Simplified AWS Deployments with CloudFormation and GitLab CI</a>, I demonstrated an end-to-end deployment pipeline using GitLab CI.  Jsonnet pre-processing can be added to this pipeline as an initial ‘preprocess’ stage and job.  A snippet from the <code>.gitlab-ci.yml</code> file is included here:  </p><div id="14c4c2fdccb27884c69c31f7b3a17a99"></div><p>Enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Simplified AWS Deployments with CloudFormation and GitLab CI]]></title>
            <link>https://cloudywithachanceofbigdata.com/aws-deployments-with-cloudformation-and-gitlab-ci</link>
            <guid>aws-deployments-with-cloudformation-and-gitlab-ci</guid>
            <pubDate>Thu, 11 Nov 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[A simple pattern for deploying stacks in AWS using CloudFormation templates using GitLab CI which allows you to review changes before deploying.]]></description>
            <content:encoded><![CDATA[<p>Managing cloud deployments and IaC pipelines can be challenging.  I’ve put together a simple pattern for deploying stacks in AWS using CloudFormation templates using GitLab CI.  </p><p>This deployment framework enables you to target different environments based upon refs (branches or tags) for instance deploy to a dev environment for a push or merge into develop and deploy to prod on a push or merge into main, otherwise just lint/validate (e.g., for a push to a non-protected feature branch).  Templates are uploaded to a designated S3 bucket and staged for use in the pipeline and can be retained as an additional audit trail (in addition to the GitLab project history).  </p><p>Furthermore, you can review changes (by inspecting change set contents) before deploying, saving you from fat finger deployments 😊.  </p><h2>How it works</h2><p>The logic is described here:  </p><div values="[object Object],[object Object]"><div value="flow"><p><a href="images/gitlabci-cloudformation-flow.png"><img src="images/gitlabci-cloudformation-flow.png" alt="GitLab CI"/></a> </p></div><div value="plantuml"><pre><code class="language-plantuml">@startuml

partition prepare {
  (*) --&gt; === S1 ===
  === S1 === --&gt; &quot;Validate Template&quot;
  --&gt; === S2 ===
  === S1 === --&gt; &quot;Check Stack State&quot;
  --&gt; === S2 ===
}

partition publish {
  --&gt; &quot;Publish Template to S3&quot;
}

partition plan {
  --&gt; &quot;Stack Exists?&quot;
  --&gt; === S3 ===
  === S3 === --&gt; [Yes] &quot;Create Change Set&quot;
  === S3 === --&gt; [No] === S4 ===
  &quot;Create Change Set&quot; --&gt; === S4 ===
}

partition deploy {
  --&gt; &quot;MANUAL: Review Changes&quot;
  --&gt; &quot;Deploy Change Set&quot;
}

--&gt;(*)

@enduml
</code></pre></div></div><p>The pipleline looks like this in GitLab:  </p><p><a href="images/gitlab-ci.png"><img src="images/gitlab-ci.png" alt="GitLab CI"/></a>  </p><h2>Prerequisites</h2><p>You will need to set up GitLab CI variables for <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> and optionally <code>AWS_DEFAULT_REGION</code>.  You can do this via <strong>Settings -&gt; CI/CD -&gt; Variables</strong> in your GitLab project.   As <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> are secrets, they should be configured as <code>protected</code> (as they are only required for protected branches) and <code>masked</code> so they are not printed in job logs.</p><h2><code>.gitlab-ci.yml</code> code</h2><p>The GitLab CI code is shown here:  </p><div id="d561e9f002048b4e4be4043cf185d1bd"></div><h2>Reviewing change sets (plans) and applying</h2><p>Once a pipeline is triggered for an existing stack it will run hands off until a change set (plan) is created.  You can inspect the plan by clicking on the Plan GitLab CI job where you would see output like this:  </p><p><a href="images/gitlab-ci-cloudformation-plan.png"><img src="images/gitlab-ci-cloudformation-plan.png" alt="Change Set"/></a>  </p><p>If you are OK with the changes proposed, you can simply hit the play button on the last stage of the pipeline (Deploy).  Voilà, stack deployed, enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Wordpress to Jamstack]]></title>
            <link>https://cloudywithachanceofbigdata.com/from-wordpress-to-jamstack</link>
            <guid>from-wordpress-to-jamstack</guid>
            <pubDate>Sun, 26 Sep 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[I started this blog a few years back to chronicle my journeys through building cloud data platforms, along the way I gathered some friends to share their experiences as well.  The easiest platform to start this blog on was Wordpress.  This worked, but wasnt really aligned with the way myself and my collegues worked,and didnt really align with the types of things we were writing about in blog articles or embracing as general principles... e.g. 'everything-as-code', 'gitops', etc.]]></description>
            <content:encoded><![CDATA[<p>I started this blog a few years back to chronicle my journeys through building cloud data platforms, along the way I gathered some friends to share their experiences as well.  The easiest platform to start this blog on was Wordpress.  This worked, but wasnt really aligned with the way myself and my collegues worked,and didnt really align with the types of things we were writing about in blog articles or embracing as general principles... e.g. &#x27;everything-as-code&#x27;, &#x27;gitops&#x27;, etc.  </p><p>Enter Static Site Generators and Jamstack architecture.  Not only does a Jamstack, SSG architecture for a blog site (or docs site or any other site), allow you to manage every aspect of your web property as code, but as a static site has several other benefits inlcuding increased performance, easier distribution (using CDNs), better security (no origin server required), all this as well as being SEO friendly (and optimised in many cases).  </p><p>But moving the site from Wordpress to a SSG must be an onerous task.. wrong.  </p><p>I moved this blog over a weekend which was quite simple in the end, here are the steps:  </p><ol><li><p>Export your Wordpress site (Tools-&gt;Export), make sure to select <em>All Content</em>.  </p></li><li><p>Use <a href="https://github.com/lonekorean/wordpress-export-to-markdown">wordpress-export-to-markdown</a> to convert your posts to a basic Markdown format with frontmatter, does a pretty good job</p></li><li><p>Choose and deploy a Static Site Generator (I chose <a href="https://docusaurus.io/">Docusaurus</a>, but there are several other alternatives available such as VuePress, Jekyll, etc)  </p></li><li><p>Drop your Markdown docs into your SSG content (blogs) directory (converted in step 2)  </p></li><li><p>You will probably need to do some fine tuning as some things may not export cleanly, but 99% of the content will be fine  </p></li><li><p>Deploy your new blog site, I am using GitHub Pages, but you could use anything similar - Netlify, Vercel, Digital Ocean, Azure Static Web Apps, etc or implement your own custom CI routine to build your project and push it to an object storage bucket configured to serve a static web site (such as Google Cloud Storage and AWS S3)  </p></li></ol><p>Thats it!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Jsonnet to Configure Multiple Environments]]></title>
            <link>https://cloudywithachanceofbigdata.com/using-jsonnet-to-configure-multiple-environments</link>
            <guid>using-jsonnet-to-configure-multiple-environments</guid>
            <pubDate>Thu, 24 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Everytime I start a new project I try and optimise how the application can work across multiple envronments. For those who don't have the luxury of developing everything in docker containers or isolated spaces, you will know my pain. How do I write code that can run on my local dev environment, migrate to the shared test and ci environment and ultimately still work in production.]]></description>
            <content:encoded><![CDATA[<p>Everytime I start a new project I try and optimise how the application can work across multiple envronments. For those who don&#x27;t have the luxury of developing everything in docker containers or isolated spaces, you will know my pain. How do I write code that can run on my local <code>dev</code> environment, migrate to the shared <code>test</code> and <code>ci</code> environment and ultimately still work in <code>production</code>.</p><p>In the past I tried exotic options like dynamically generating <code>YAML</code> or <code>JSON</code> using Jinja. I then graduated to <code>HOCON</code> which made my life so much easier. This was until I stumbled across <a href="https://jsonnet.org/">Jsonnet</a>. For those who have not seen this in action, think JSON meets Jinja meets HOCON (a Frankenstein creation that I have actually built in the past)</p><p>To get a feel for how it looks, below is a contrived example where I require 3 environments (dev, test and production) that have different paths, databases and vault configuration.</p><p>Essentially, when this config is run through the Jsonnet templating engine, it will expect a variable &#x27;<code>ENV</code>&#x27; to ultimately refine the <code>environment</code> entry to the one we specifically want to use.</p><p>A helpful thing I like to do with my programs is give users a bit of information as to what environments can be used. For me, running a cli that requires args should be as informative as possible - so listing out all the environments is mandatory. I achieve this with a little trickery and a lot of help from the <a href="https://click.palletsprojects.com/">click</a> package!</p><pre><code class="language-jsonnet">local exe = &quot;application.exe&quot;;

local Environment(prefix) = {
  root: &quot;/usr/&quot; + prefix + &quot;/app&quot;,
  path: self.root + &quot;/bin/&quot; + exe,
  database: std.asciiUpper(prefix) + &quot;_DB&quot;,
  tmp_dir: &quot;/tmp/&quot; + prefix
};

local Vault = {
  local uri = &quot;http://127.0.0.1:8200/v1/secret/app&quot;,
  _: {},
  dev: {
      secrets_uri: uri,
      approle: &quot;local&quot;
  },
  tst: {
      secrets_uri: uri,
      approle: &quot;local&quot;
  },
  prd: {
      secrets_uri: &quot;https://vsrvr:8200/v1/secret/app&quot;,
      approle: &quot;sa_user&quot;
  }
};

{

  environments: {
    _: {},
    dev: Environment(&quot;dev&quot;) + Vault[std.extVar(&quot;ENV&quot;)],
    tst: Environment(&quot;tst&quot;) + Vault[std.extVar(&quot;ENV&quot;)],
    prd: Environment(&quot;prd&quot;) + Vault[std.extVar(&quot;ENV&quot;)]
  },

  environment: $[&quot;environments&quot;][std.extVar(&quot;ENV&quot;)],
}
</code></pre><p>The trick I perform is to have a placeholder entry &#x27;<code>_</code>&#x27; that I use to initially render the template. I then use the generated JSON file and get all the environment keys so I can feed that directly into click.</p><pre><code class="language-python">from typing import Any, Dict
import click
import json
import _jsonnet
from pprint import pprint

ENV_JSONNET = &#x27;environment.jsonnet&#x27;
ENV_PFX_PLACEHOLDER = &#x27;_&#x27;

def parse_environment(prefix: str) -&gt; Dict[str, Any]:
    _json_str = _jsonnet.evaluate_file(ENV_JSONNET, ext_vars={&#x27;ENV&#x27;: prefix})
    return json.loads(_json_str)

_config = parse_environment(prefix=ENV_PFX_PLACEHOLDER)

_env_prefixes = [k for k in _config[&#x27;environments&#x27;].keys() if k != ENV_PFX_PLACEHOLDER]


@click.command(name=&quot;EnvMgr&quot;)
@click.option(
    &quot;-e&quot;,
    &quot;--environment&quot;,
    required=True,
    type=click.Choice(_env_prefixes, case_sensitive=False),
    help=&quot;Which environment this is executing on&quot;,
)
def cli(environment: str) -&gt; None:
    config = parse_environment(environment)
    pprint(config[&#x27;environment&#x27;])


if __name__ == &quot;__main__&quot;:
    cli()
</code></pre><p>This now allows me to execute the application with both list checking (has the user selected an allowed environment?) and the autogenerated help that click provides.</p><p>Below shows running the cli with no arguments:</p><pre><code class="language-shell">$&gt; python cli.py

Usage: cli.py [OPTIONS]
Try &#x27;cli.py --help&#x27; for help.

Error: Missing option &#x27;-e&#x27; / &#x27;--environment&#x27;. Choose from:
        dev,
        prd,
        tst
</code></pre><p>Executing the application with a valid environment:</p><pre><code class="language-shell">$&gt; python cli.py -e dev

{&#x27;approle&#x27;: &#x27;local&#x27;,
 &#x27;database&#x27;: &#x27;DEV_DB&#x27;,
 &#x27;path&#x27;: &#x27;/usr/dev/app/bin/application.exe&#x27;,
 &#x27;root&#x27;: &#x27;/usr/dev/app&#x27;,
 &#x27;secrets_uri&#x27;: &#x27;http://127.0.0.1:8200/v1/secret/app&#x27;,
 &#x27;tmp_dir&#x27;: &#x27;/tmp/dev&#x27;}
</code></pre><p>Executing the application with an invalid environment:</p><pre><code class="language-shell">$&gt; python cli.py -e prd3

Usage: cli.py [OPTIONS]
Try &#x27;cli.py --help&#x27; for help.

Error: Invalid value for &#x27;-e&#x27; / &#x27;--environment&#x27;: &#x27;prd3&#x27; is not one of &#x27;dev&#x27;, &#x27;prd&#x27;, &#x27;tst&#x27;.
</code></pre><p>This is only the tip of what Jsonnet can provide, I am continually learning more about the templating engine and the tool.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use BigQuery to trigger Cloud Run]]></title>
            <link>https://cloudywithachanceofbigdata.com/use-bigquery-to-trigger-cloud-run</link>
            <guid>use-bigquery-to-trigger-cloud-run</guid>
            <pubDate>Sat, 19 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[So you're using BigQuery (BQ). It's all set up and humming perfectly. Maybe now, you want to run an ELT job whenever a new table partition is created, or maybe you want to retrain your ML model whenever new rows are inserted into the BQ table.]]></description>
            <content:encoded><![CDATA[<p>So you&#x27;re using BigQuery (BQ). It&#x27;s all set up and humming perfectly. Maybe now, you want to run an ELT job whenever a new table partition is created, or maybe you want to retrain your ML model whenever new rows are inserted into the BQ table.</p><p>In my previous article on <a href="https://cloudywithachanceofbigdata.com/eventarc-the-state-of-eventing-in-google-cloud/">EventArc</a>, we went through how Logging can help us create eventing-type functionality in your application. Let&#x27;s take it a step further and walk through how we can couple BigQuery and Cloud Run.</p><p>In this article you will learn how to</p><ul><li>Tie together BigQuery and Cloud Run</li><li>Use BigQuery&#x27;s audit log to trigger Cloud Run</li><li>With those triggers, run your required code</li></ul><h2>Let&#x27;s go!</h2><p>Let&#x27;s create a temporary dataset within BigQuery named <code>tmp_bq_to_cr</code>.</p><p>In that same dataset, let&#x27;s create a table in which we will insert some rows to test our BQ audit log. Let&#x27;s grab some rows from a BQ public dataset to create this table:</p><pre><code class="language-sql">CREATE OR REPLACE TABLE tmp_bq_to_cr.cloud_run_trigger AS
SELECT
 date, country_name, new_persons_vaccinated, population
 from `bigquery-public-data.covid19_open_data.covid19_open_data`
 where country_name=&#x27;Australia&#x27;
 AND
 date &gt; &#x27;2021-05-31&#x27;
LIMIT 100
</code></pre><p>Following this, let&#x27;s run an insert query that will help us build our mock database trigger:</p><pre><code class="language-sql">INSERT INTO tmp_bq_to_cr.cloud_run_trigger
VALUES(&#x27;2021-06-18&#x27;, &#x27;Australia&#x27;, 3, 1000)
</code></pre><p>Now, in another browser tab let&#x27;s navigate to <a href="https://console.cloud.google.com/logs/query;query=bigquery.v2?_ga=2.187390252.-505923201.1592376029">BQ Audit Events</a> and look for our <code>INSERT INTO</code> event:</p><p><a href="images/bq-insert-event.png"><img src="images/bq-insert-event.png" alt="BQ-insert-event"/></a></p><p>There will be several audit logs for any given BQ action. Only after a query is parsed does BQ know which table we want to interact with, so the initial log will, for e.g., not have the table name.</p><p>We don&#x27;t want any old audit log, so we need to ensure we look for a unique set of attributes that clearly identify our action, such as in the diagram above.</p><p>In the case of inserting rows, the attributes are a combination of</p><ul><li>The method is <code>google.cloud.bigquery.v2.JobService.InsertJob</code></li><li>The name of the table being inserted to is the <code>protoPayload.resourceName</code></li><li>The dataset id is available as <code>resource.labels.dataset_id</code></li><li>The number of inserted rows is <code>protoPayload.metadata.tableDataChanged.insertedRowsCount</code></li></ul><h2>Time for some code</h2><p>Now that we&#x27;ve identified the payload that we&#x27;re looking for, we can write the action for Cloud Run. We&#x27;ve picked Python and Flask to help us in this instance. (<a href="https://github.com/GoogleCloudPlatform/bigquery-oreilly-book/blob/master/blogs/cloud_run/main.py">full code is on GitHub</a>).</p><p>First, let&#x27;s filter out the noise and find the event we want to process</p><pre><code class="language-python">@app.route(&#x27;/&#x27;, methods=[&#x27;POST&#x27;])
def index():
    # Gets the Payload data from the Audit Log
    content = request.json
    try:
        ds = content[&#x27;resource&#x27;][&#x27;labels&#x27;][&#x27;dataset_id&#x27;]
        proj = content[&#x27;resource&#x27;][&#x27;labels&#x27;][&#x27;project_id&#x27;]
        tbl = content[&#x27;protoPayload&#x27;][&#x27;resourceName&#x27;]
        rows = int(content[&#x27;protoPayload&#x27;][&#x27;metadata&#x27;]
                   [&#x27;tableDataChange&#x27;][&#x27;insertedRowsCount&#x27;])
        if ds == &#x27;cloud_run_tmp&#x27; and \
           tbl.endswith(&#x27;tables/cloud_run_trigger&#x27;) and rows &gt; 0:
            query = create_agg()
            return &quot;table created&quot;, 200
    except:
        # if these fields are not in the JSON, ignore
        pass
    return &quot;ok&quot;, 200
</code></pre><p>Now that we&#x27;ve found the event we want, let&#x27;s execute the action we need. In this example, we&#x27;ll aggregate and write out to a new table <code>created_by_trigger</code>:</p><pre><code class="language-python">def create_agg():
    client = bigquery.Client()
    query = &quot;&quot;&quot;
    CREATE OR REPLACE TABLE tmp_bq_to_cr.created_by_trigger AS
    SELECT
      count_name, SUM(new_persons_vaccinated) AS n
    FROM tmp_bq_to_cr.cloud_run_trigger
    &quot;&quot;&quot;
    client.query(query)
    return query
</code></pre><p>The Dockerfile for the container is simply a basic Python container into which we install Flask and the BigQuery client library:</p><pre><code class="language-docker">FROM python:3.9-slim
RUN pip install Flask==1.1.2 gunicorn==20.0.4 google-cloud-bigquery
ENV APP_HOME /app
WORKDIR $APP_HOME
COPY *.py ./
CMD exec gunicorn --bind :$PORT main:app
</code></pre><h2>Now we Cloud Run</h2><p>Build the container and deploy it using a couple of gcloud commands:</p><pre><code class="language-bash">SERVICE=bq-cloud-run
PROJECT=$(gcloud config get-value project)
CONTAINER=&quot;gcr.io/${PROJECT}/${SERVICE}&quot;
gcloud builds submit --tag ${CONTAINER}
gcloud run deploy ${SERVICE} --image $CONTAINER --platform managed
</code></pre><h2>I always forget about the permissions</h2><p>In order for the trigger to work, the Cloud Run service account will need the following permissions:</p><pre><code class="language-bash">gcloud projects add-iam-policy-binding $PROJECT \
    --member=&quot;serviceAccount:service-${PROJECT_NO}@gcp-sa-pubsub.iam.gserviceaccount.com&quot;\
    --role=&#x27;roles/iam.serviceAccountTokenCreator&#x27;

gcloud projects add-iam-policy-binding $PROJECT \
    --member=serviceAccount:${SVC_ACCOUNT} \
    --role=&#x27;roles/eventarc.admin&#x27;
</code></pre><h3>Finally, the event trigger</h3><pre><code class="language-bash">gcloud eventarc triggers create ${SERVICE}-trigger \
  --location ${REGION} --service-account ${SVC_ACCOUNT} \
  --destination-run-service ${SERVICE}  \
  --event-filters type=google.cloud.audit.log.v1.written \
  --event-filters methodName=google.cloud.bigquery.v2.JobService.InsertJob \
  --event-filters serviceName=bigquery.googleapis.com
</code></pre><p>Important to note here is that we&#x27;re triggering on <em>any</em> Insert log created by BQ That&#x27;s why in this action we had to filter these events based on the payload.</p><h1>Take it for a spin</h1><p>Now, try out the BigQuery -&gt; Cloud Run trigger and action. Go to the BigQuery console and insert a row or two:</p><pre><code class="language-sql">INSERT INTO tmp_bq_to_cr.cloud_run_trigger
VALUES(&#x27;2021-06-18&#x27;, &#x27;Australia&#x27;, 5, 25000)
</code></pre><p>Watch as a new table called <code>created_by_trigger</code> gets created! You have successfully triggered a Cloud Run action on a database event in BigQuery.</p><p>Enjoy!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Static Web App Review]]></title>
            <link>https://cloudywithachanceofbigdata.com/azure-static-web-app-review</link>
            <guid>azure-static-web-app-review</guid>
            <pubDate>Fri, 18 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[The Azure Static Web App feature is relatively new in the Azure estate which has recently become generally available, I thought I would take it for a test drive and discuss my findings.]]></description>
            <content:encoded><![CDATA[<p>The Azure Static Web App feature is relatively new in the Azure estate which has recently become generally available, I thought I would take it for a test drive and discuss my findings.</p><p>I am a proponent of the JAMStack architecture for front end applications and a user of CD enabled CDN services like Netlify, so this Azure feature was naturally appealing to me.</p><p>Azure SWAs allow you to serve static assets (like JavaScript) without a origin server, meaning you don’t need a web server, are able to streamline content distribution and web app performance, and reduce the attack surface area of your application.</p><p>The major advantage to using is simplicity, no scaffolding or infra requirements and it is seamlessly integrated into your CI/CD processes (natively if you are using GitHub).</p><h2>Deploying Static Web Apps in Azure</h2><p>Pretty simple to setup, aside from a name and a resource group, you just need to supply:</p><ul><li>a <strong>location</strong> (Azure region to be used for serverless back end APIs via Azure Function Apps) note that this is not a location where the static web is necessarily running</li><li>a GitHub or GitLab <strong>repo URL</strong></li><li>the <strong>branch</strong> you wish to use to trigger production deployments (e.g. <code>main</code>)</li><li>a <strong>path</strong> to your code within your app (e.g. where your <code>package.json</code> file is located)</li><li>an <strong>output folder</strong> (e.g. <code>dist</code>) this should not exist in your repo</li><li>a project or personal access <strong>token</strong> for your GitHub account (alternatively you can perform an interactive OAuth2.0 consent if using the portal)</li></ul><p>An example is shown here:</p><div id="eef5a25ed01327a180711fd64370c457"></div><h2>GitHub Actions</h2><p>Using the consent provided (either using the OAuth flow or by providing a token), Azure Static Web Apps will automagically create the GitHub Actions workflow to deploy your application on a push or merge event to your repo. This includes providing scoped API credentials to Azure to allow access to the Static Web App resource using secrets in GitHub (which are created automagically as well). An example workflow is shown here:</p><div id="8e7ad2bdd9ba351368c5aedad289e972"></div><h2>Preview or Staging Releases</h2><p>Similar to the functionality in analogous services like Netlify, you can configure preview releases of your application to be deployed from specified branches on pull request events.</p><h2>Routes and Authorization</h2><p>Routes (for SPAs) need to be provided to Azure by using a file named <code>staticwebapp.config.json</code> located in the application root of your repo (same level as you <code>package.json</code> file). You can also specify response codes and whether the rout requires authentication as shown here:</p><div id="7dd3bcf05474da551b3d311ae0729e18"></div><h2>Pros</h2><ul><li>Globally distributed CDN</li><li>Increased security posture, reduced attack surface area</li><li>Simplified architecture and deployment</li><li>No App Service Plan required – cost reduction</li><li>Enables Continuous Deployment – incl preview/staging environments</li><li>TLS and DNS can be easily configured for your app</li></ul><h2>Cons</h2><ul><li>Serverless API locations are limited</li><li>Integration with other VCS/CI/CD systems like GitLab would need to be custom built (GitHub and Azure DevOps is integrated)</li></ul><p>Overall, this is a good feature for deploying SPAs or PWAs in Azure.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing the Metadata Hub (MDH)]]></title>
            <link>https://cloudywithachanceofbigdata.com/introducing-the-metadata-hub-mdh</link>
            <guid>introducing-the-metadata-hub-mdh</guid>
            <pubDate>Tue, 15 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Metadata Hub (MDH) is intended to be the source of truth for metadata around the Company’s platform. It has the ability to load metadata configuration from yaml, and serve that information up via API. It will also be the store of information for pipeline information while ingesting files into the platform.]]></description>
        </item>
        <item>
            <title><![CDATA[Masking Private Keys in CI/CD Pipelines in GitLab]]></title>
            <link>https://cloudywithachanceofbigdata.com/masking-private-keys-in-ci-cd-pipelines-in-gitlab</link>
            <guid>masking-private-keys-in-ci-cd-pipelines-in-gitlab</guid>
            <pubDate>Tue, 15 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Big fan of GitLab (and GitLab CI in particular). I had a recent requirement to push changes to a wiki repo associated with a GitLab project through a GitLab CI pipeline (using the SaaS version of GitLab) and ran into a conundrum…]]></description>
            <content:encoded><![CDATA[<p>Big fan of GitLab (and GitLab CI in particular). I had a recent requirement to push changes to a wiki repo associated with a GitLab project through a GitLab CI pipeline (using the SaaS version of GitLab) and ran into a conundrum…</p><p>Using the GitLab SaaS version - deploy tokens can’t have write api access, so the next best solution is to use deploy keys, adding your public key as a deploy key and granting this key write access to repositories is relatively straightforward.</p><p>This issue is when you attempt to create a masked GitLab CI variable using the private key from your keypair, you get this…</p><p><a href="images/masked-variable.png"><img src="images/masked-variable.png"/></a></p><p>I was a bit astonished to see this to be honest… Looks like it has been raised as an issue several times over the last few years but never resolved (the root cause of which is something to do with newline characters or base64 encoding or the overall length of the string).</p><p>I came up with a solution! Not pretty but effective, masks the variable so that it cannot be printed in CI logs as shown here:</p><p><a href="images/ci-ssh-key.png"><img src="images/ci-ssh-key.png"/></a></p><h2>Setup</h2><p>Add a masked and protected GitLab variable for each line in the private key, for example:</p><p><a href="images/masked-vars.png"><img src="images/masked-vars.png"/></a></p><h2>The Code</h2><p>Add the following block to your <code>.gitlab-ci.yml</code> file:</p><div id="b5260f14ecc0bf0d080c80297d0b475c"></div><p>now within Jobs in your pipeline you can simply do this to clone, push or pull from a remote GitLab repo:</p><div id="c96e211544f7cb4ef3ca4e90dc8e36e3"></div><p>as mentioned not pretty, but effective and no other cleaner options as I could see…</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Simple Tasker: Configuration driven orchestration]]></title>
            <link>https://cloudywithachanceofbigdata.com/simple-tasker-configuration-driven-orchestration</link>
            <guid>simple-tasker-configuration-driven-orchestration</guid>
            <pubDate>Tue, 15 Jun 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Recently I found myself at a client that were using a third party tool to scan all their enterprise applications in order to collate their data lineage. They had spent two years onboarding applications to the tool, resulting in a large technical mess that was hard to debug and impossible to extend. As new applications were integrated onto the platform, developers were forced to think of new ways of connecting and tranforming the data so it could be consumed.]]></description>
            <content:encoded><![CDATA[<p>Recently I found myself at a client that were using a third party tool to scan all their enterprise applications in order to collate their data lineage. They had spent two years onboarding applications to the tool, resulting in a large technical mess that was hard to debug and impossible to extend. As new applications were integrated onto the platform, developers were forced to think of new ways of connecting and tranforming the data so it could be consumed.</p><p>The general approach was: <code>setup scanner</code> -&gt; <code>scan application</code> -&gt; <code>modify results</code> -&gt; <code>upload results</code> -&gt; <code>backup results</code> -&gt; <code>cleanup workspace</code> -&gt; <code>delete anything older than &#x27;X&#x27; days</code></p><p>Each developer had their own style of doing this - involving shell scripts, python scripts, SQL and everything in between. Worse, there was slabs of code replicated across the entire repository, with variables and paths changed depending on the use case.</p><p>My tasks was to create a framework that could orchestrate the scanning and adhered to the following philosophies:</p><ul><li>DRY (Don&#x27;t Repeat Yourself)</li><li>Config driven</li><li>Version controlled</li><li>Simple to extend</li><li>Idempotent</li></ul><p>It also had to be written in Python as that was all the client was skilled in.</p><p>After looking at what was on the market (Airflow and Prefect being the main contenders) I decided to roll my own simplified orchestrator that required as little actual coding as possible and could be setup by configuration.</p><p>In choosing a configuration format, I settled on <a href="https://github.com/lightbend/config/blob/master/HOCON.md">HOCON</a> as it closely resembled JSON but has advanced features such as interpolation, substitions and the ability to include other hocon files - this would drastically reduce the amount of boilerplate configuration required.</p><p>Because I had focused so heavily on being configuration driven, I also needed the following charecteristics to be delivered:</p><ul><li>Self discovery of task types (more on this later)</li><li>Configuration validation at startup</li></ul><h2>Tasks and self discovery</h2><p>As I wanted anyone to be able to rapidly extend the framework by adding tasks, I needed to reduce as much repetition and boilerplate as possible. Ideally, I wanted a developer to just have to think about writing code and not have to deal with how to integrate this.</p><p>To achieve this, we needed a way of registering new &#x27;tasks&#x27; that would become available to the framework. I wanted a developer to simply have to subclass the main Task class and implement a run function - the rest would be taken care of.</p><pre><code class="language-python">class TaskRegistry:

    def __init__(self) -&gt; None:
        self._registry = {}

    def register(self, cls: type) -&gt; None:
        n = getattr(cls, &#x27;task_name&#x27;, cls.__name__).lower()
        self._registry[n] = cls

    def registered(self) -&gt; List[str]:
        return list(self._registry.keys())

    def has(self, name: str) -&gt; bool:
        return name in self._registry

    def get(self, name: str) -&gt; type:
        return self._registry[name]

    def create(self, name: str, *args, **kwargs) -&gt; object:
        try:
            return self._registry[name](*args, **kwargs)
        except KeyError:
            raise ClassNotRegisteredException(name)


registry = TaskRegistry()
</code></pre><p>Once the registry was instantiated, any new Tasks that inherited from &#x27;Task&#x27; would automatically be added to the registry. We could then use the <code>create(name)</code> function to instantiate any class - essentially a pythonic <a href="https://en.wikipedia.org/wiki/Factory_method_pattern">Factory Method</a></p><pre><code class="language-python">class Task(ABC):

    def __init__(self) -&gt; None:
        self.logger = logging.getLogger(self.__class__.__name__)

    def __init_subclass__(cls) -&gt; None:
        registry.register(cls)

    @abstractmethod
    def run(self, **kwargs) -&gt; bool:
        raise NotImplementedError
</code></pre><p>For the framework to automatically register the classes, it was important to follow the project structure. As long as the task resided in the &#x27;tasks&#x27; module, we could scan this at runtime and register each task.</p><pre><code class="language-text">└── simple_tasker
    ├── __init__.py
    ├── cli.py
    └── tasks
        ├── __init__.py
        ├── archive.py
        └── shell_script.py
</code></pre><p>This was achieved with a simple dynamic module importer</p><pre><code class="language-python">modules = glob.glob(join(dirname(__file__), &quot;*.py&quot;))

for f in modules:
    if isfile(f) and not f.endswith(&quot;__init__.py&quot;):
        __import__(f&quot;{Task.__module__}.{basename(f)[:-3]}&quot;)
</code></pre><h2>The configuration</h2><p>In designing how the configuration would bind to the task, I needed to capture the <code>name</code> (what object to instanticate) and what <code>args</code> to pass to the instantiated run function. I decided to model it as below with everything under a &#x27;tasks&#x27; array</p><pre><code class="language-text">tasks: [
    {
        name: shell_script
        args: {
            script_path: uname
            script_args: -a
        }
    },
    {
        name: shell_script
        args: {
            script_path: find
            script_args: [${CWD}/simple_tasker/tasks, -name, &quot;*.py&quot;]
        }
    },
    {
        name: archive
        args: {
            input_directory_path: ${CWD}/simple_tasker/tasks
            target_file_path: /tmp/${PLATFORM}_${TODAY}.tar.gz
        }
    }
]
</code></pre><h2>Orchestration and validation</h2><p>As mentioned previously, one of the goals was to ensure the configuration was valid prior to any execution. This meant that the framework needed to validate whether tha task <code>name</code> referred to a registered task, and that all mandatory <code>arguments</code> were addressed in the configuration. Determining whether the task was registered was just a simple key check, however to validate the arguments to the run required some inspection - I needed to get all args for the run function and filter out &#x27;self&#x27; and any asterisk args (<!-- -->*<!-- -->args, <!-- -->*<!-- -->*<!-- -->kwargs)</p><pre><code class="language-python">def get_mandatory_args(func) -&gt; List[str]:

    mandatory_args = []
    for k, v in inspect.signature(func).parameters.items():
        if (
            k != &quot;self&quot;
            and v.default is inspect.Parameter.empty
            and not str(v).startswith(&quot;*&quot;)
        ):
            mandatory_args.append(k)

    return mandatory_args
</code></pre><p>And finally onto the actual execution bit. The main functionality required here is to validate that the config was defined correctly, then loop through all tasks and execute them - passing in any args.</p><pre><code class="language-python">class Tasker:

    def __init__(self, path: Path, env: Dict[str, str] = None) -&gt; None:

        self.logger = logging.getLogger(self.__class__.__name__)
        self._tasks = []

        with wrap_environment(env):
            self._config = ConfigFactory.parse_file(path)


    def __validate_config(self) -&gt; bool:

        error_count = 0

        for task in self._config.get(&quot;tasks&quot;, []):
            name, args = task[&quot;name&quot;].lower(), task.get(&quot;args&quot;, {})

            if registry.has(name):
                for arg in get_mandatory_args(registry.get(name).run):
                    if arg not in args:
                        print(f&quot;Missing arg &#x27;{arg}&#x27; for task &#x27;{name}&#x27;&quot;)
                        error_count += 1
            else:
                print(f&quot;Unknown tasks &#x27;{name}&#x27;&quot;)
                error_count += 1

            self._tasks.append((name, args))

        return error_count == 0

    def run(self) -&gt; bool:

        if self.__validate_config():

            for name, args in self._tasks:
                exe = registry.create(name)
                self.logger.info(f&quot;About to execute: &#x27;{name}&#x27;&quot;)
                if not exe.run(**args):
                    self.logger.error(f&quot;Failed tasks &#x27;{name}&#x27;&quot;)
                    return False

            return True
        return False
</code></pre><h2>Putting it together - sample tasks</h2><p>Below are two examples of how easy it is to configure the framework. We have a simple folder archiver that will tar/gz a directory based on 2 input parameters.</p><pre><code class="language-python">class Archive(Task):

    def __init__(self) -&gt; None:
        super().__init__()

    def run(self, input_directory_path: str, target_file_path: str) -&gt; bool:

        self.logger.info(f&quot;Archiving &#x27;{input_directory_path}&#x27; to &#x27;{target_file_path}&#x27;&quot;)

        with tarfile.open(target_file_path, &quot;w:gz&quot;) as tar:
            tar.add(
                input_directory_path,
                arcname=os.path.basename(input_directory_path)
            )
        return True
</code></pre><p>A more complex example would be the ability to execute shell scripts (or os functions) by passing in some optional variables and variables that can either be a string or list.</p><pre><code class="language-python">class ShellScript(Task):

    task_name = &quot;shell_script&quot;

    def __init__(self) -&gt; None:
        super().__init__()

    def run(
        self,
        script_path: str,
        script_args: Union[str, List[str]] = None,
        working_directory_path: str = None
    ) -&gt; bool:

        cmd = [script_path]

        if isinstance(script_args, str):
            cmd.append(script_args)
        else:
            cmd += script_args

        try:

            result = subprocess.check_output(
                cmd,
                stderr=subprocess.STDOUT,
                cwd=working_directory_path
            ).decode(&quot;utf-8&quot;).splitlines()

            for o in result:
                self.logger.info(o)

        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            self.logger.error(e)
            return False

        return True
</code></pre><p>You can view the entire implementation <a href="https://github.com/mpstella/simple_tasker">here</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Okta Admin Command Line Interface]]></title>
            <link>https://cloudywithachanceofbigdata.com/okta-admin-command-line-interface</link>
            <guid>okta-admin-command-line-interface</guid>
            <pubDate>Sun, 30 May 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Identity and Access Management is a critical component of any application or SaaS architecture. I’m currently doing a spike of the Okta solution for an application development project I am on. Okta is a comprehensive solution built on the open OAuth2 and OIDC protocols, as well as supporting more conventional identity federation approaches such as SAML.]]></description>
            <content:encoded><![CDATA[<p>Identity and Access Management is a critical component of any application or SaaS architecture. I’m currently doing a spike of the Okta solution for an application development project I am on. Okta is a comprehensive solution built on the open OAuth2 and OIDC protocols, as well as supporting more conventional identity federation approaches such as SAML.</p><p>Okta has a clean and easy to use web-based Admin interface which can be used to create applications, users, claims, identity providers and more.</p><p>During my spike, which was done in a crash and burn test Okta organisation, I had associated my user account with a Microsoft Identity Provider for SSO, and subsequently had issues accessing the Microsoft Account my user was associated with, as a result I managed to lock myself (the super admin) out of the Okta Admin Console.</p><p>Fortunately, prior to doing this I had created an API token for my user. So, I went about looking at ways I could interact with Okta programmatically. My first inclination was to use a simple CLI for Okta to get me out of jail… but I found there wasn’t one that suited. There are, however, a wealth of SDKs for Okta across multiple front-end and back-end oriented programming languages (such as JavaScript, Golang, Python and more).</p><p>Being in lockdown and having some free time on my hands, I decided to create a simple open source command line tool which could be used to administer an Okta organisation. The result of this weekend lockdown is <code>okta-admin</code>…</p><p><a href="images/okta-admin-screenshot.png"><img src="images/okta-admin-screenshot.png" alt="okta-admin cli"/></a></p><p>For this project I used the <a href="https://github.com/okta/okta-sdk-golang">Golang SDK for Okta</a>, along with the <a href="https://github.com/spf13/cobra">Cobra</a> and <a href="https://github.com/spf13/viper">Viper</a> Golang packages (used by <code>docker</code>, <code>kubectl</code> and other popular command line utilities). To provide a query interface to JSON response payloads I use <a href="https://github.com/tidwall/gjson">GJson</a>.</p><p>Will keep adding to this so stay tuned...</p><blockquote><p>Complete source code for this project is available at <a href="https://github.com/gammastudios/okta-admin">https://github.com/gammastudios/okta-admin</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Enumerating all roles for a user in Snowflake]]></title>
            <link>https://cloudywithachanceofbigdata.com/enumerating-all-roles-for-a-user-in-snowflake</link>
            <guid>enumerating-all-roles-for-a-user-in-snowflake</guid>
            <pubDate>Tue, 23 Mar 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Snowflake allows roles to be assigned to other roles, so when a user is assigned to a role, they may inherit the ability to use countless other roles.]]></description>
            <content:encoded><![CDATA[<p>Snowflake allows roles to be assigned to other roles, so when a user is assigned to a role, they may inherit the ability to use countless other roles.</p><p><strong>Challenge:</strong> recursively enumerate all roles for a given user</p><p>One solution would be to create a complex query on the <code>“SNOWFLAKE&quot;.&quot;ACCOUNT_USAGE&quot;.&quot;GRANTS_TO_ROLES&quot;</code> object.</p><p>An easier solution is to use a stored procedure to recurse through grants for a given user and return an <code>ARRAY</code> of roles for that user.</p><p>This is a good programming exercise in tail call recursion (sort of) in JavaScript. Here is the code:</p><div id="9b9985dbf8163ade22b71f2ccf20cb51"></div><p>To call the stored proc, execute:</p><div id="fbbfaa3b67af828e4d905411567cd031"></div><p>One drawback of stored procedures in Snowflake is that they can only have scalar or array return types and cannot be used directly in a SQL query, however you can use the <code>table(result_scan(last_query_id()))</code> trick to get around this, as shown below where we will pivot the <code>ARRAY</code> into a record set with the array elements as rows:</p><div id="6a7e8bc552b87ab1e039f22bacf1b65f"></div><p><strong>IMPORTANT</strong></p><p>This query <strong>must</strong> be the next statement run immediately after the <code>CALL</code> statement and cannot be run again until you run another <code>CALL</code> statement.</p><p>More adventures with Snowflake soon!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EventArc: The state of eventing in Google Cloud]]></title>
            <link>https://cloudywithachanceofbigdata.com/eventarc-the-state-of-eventing-in-google-cloud</link>
            <guid>eventarc-the-state-of-eventing-in-google-cloud</guid>
            <pubDate>Sun, 28 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[When defining event-driven architectures, it's always good to keep up with how the landscape is changing. How do you connect microservices in your architecture? Is Pub/Sub the end-game for all events? To dive a bit deeper, let's talk through the benefits of having a single orchestrator, or perhaps a choreographer is better?]]></description>
            <content:encoded><![CDATA[<p>When defining event-driven architectures, it&#x27;s always good to keep up with how the landscape is changing. How do you connect microservices in your architecture? Is Pub/Sub the end-game for all events? To dive a bit deeper, let&#x27;s talk through the benefits of having a single <em>orchestrator</em>, or perhaps a choreographer is better?</p><h2>Orchestration versus choreography refresher</h2><p>My colleague <a href="https://www.linkedin.com/in/jeffreyaven/">@jeffreyaven</a> did a recent post explaining this concept in simple terms, which is worth reviewing, see:</p><p><a href="https://cloudywithachanceofbigdata.com/microservices-concepts-orchestration-versus-choreography/"><strong>Microservices Concepts: Orchestration versus Choreography</strong></a></p><p>Should there really be a central orchestrator controlling all interactions between services.....or, should each service work independently and only interact through events?</p><ul><li><strong>Orchestration</strong> is usually viewed as a domain-wide central service that defines the flow and control of communication between services. In this paradigm, in becomes easier to change and ultimately monitor policies across your org.</li><li><strong>Choreography</strong> has each service registering and emitting events as they need to. It doesn&#x27;t direct or define the flow of communication, but using this method usually has a central broker passing around messages and allows services to be truly independent.</li></ul><p>Enter <a href="https://cloud.google.com/workflows">Workflows</a>, which is suited for centrally orchestrated services. Not only Google Cloud service such as Cloud Functions and Cloud Run, but also external services.</p><p>How about choreography? <a href="https://cloud.google.com/pubsub">Pub/Sub</a> and <a href="https://cloud.google.com/blog/products/serverless/build-event-driven-applications-in-cloud-run">Eventarc</a> are both suited for this. We all know and love Pub/Sub, <em>but how do I use EventArc?</em></p><h2>What is Eventarc?</h2><p>Announced in October-2020, it was introduced as eventing functionality that enables you, the developer, to send events <em>to</em> Cloud Run from more than 60 Google Cloud sources.</p><h3>But how does it work?</h3><p>Eventing is done by reading those sweet sweet Audit Logs, from various sources, and sending them to Cloud Run services as events in <a href="https://cloudevents.io/">Cloud Events</a> format. Quick primer on Cloud Events: its a specification for describing event data in a common way. The specification is now under the <a href="https://cncf.io/">Cloud Native Computing Foundation</a>. Hooray! It can also read events from Pub/Sub topics for custom applications. Here&#x27;s a diagram I graciously ripped from <a href="https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud">Google Cloud Blog</a>:</p><p><a href="images/CloudEvents_fig1.png"><img src="images/CloudEvents_fig1.png" alt="Eventarc"/></a></p><h3>Why do I need Eventarc? I have the Pub/Sub</h3><p>Good question. Eventarc provides and easier path to receive events not only from Pub/Sub topics but from a number of Google Cloud sources with its Audit Log and Pub/Sub integration. Actually, <em>any</em> service that has Audit Log integration can be an event source for Eventarc. Beyond easy integration, it provides consistency and structure to how events are generated, routed and consumed. Things like:</p><h4><strong>Triggers</strong></h4><p>They specify routing rules from events sources, to event sinks. Listen for new object creation in GCS and route that event to a service in Cloud Run by creating an Audit-Log-Trigger. Create triggers that also listen to Pub/Sub. Then list <strong>all</strong> triggers in one, central place in Eventarc:</p><p><code>gcloud beta eventarc triggers list</code></p><h4><strong>Consistency with eventing format and libraries</strong></h4><p>Using the CloudEvent-compliant specification will allow for event data in a common way, increasing the movement towards the goal of consistency, accessibility and portability. Makes it easier for different languages to read the event and Google Events Libraries to parse fields.</p><p>This means that the long-term vision of Eventarc to be the <strong>hub</strong> of events, enabling a unified eventing story for Google Cloud and beyond.</p><p><a href="images/CloudEvents_fig2.png"><img src="images/CloudEvents_fig2.png" alt="Eventarc producers and consumers"/></a></p><p>In the future, you can excpect to forego Audit Log and read these events directly and send these out to even more sinks within GCP and any HTTP target.</p><hr/><p>This article written on inspiration from <a href="https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud">https://cloud.google.com/blog/topics/developers-practitioners/eventarc-unified-eventing-experience-google-cloud</a>. Thanks Mete Atamel!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Microservices Concepts: Orchestration versus Choreography]]></title>
            <link>https://cloudywithachanceofbigdata.com/microservices-concepts-orchestration-versus-choreography</link>
            <guid>microservices-concepts-orchestration-versus-choreography</guid>
            <pubDate>Fri, 26 Feb 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[One of the foundational concepts in microservices architecture and design patterns is the concept of Orchestration versus Choreography. Before we look at a reference implementation of each of these patterns, it is worthwhile starting with an analogy.]]></description>
            <content:encoded><![CDATA[<p>One of the foundational concepts in microservices architecture and design patterns is the concept of Orchestration versus Choreography. Before we look at a reference implementation of each of these patterns, it is worthwhile starting with an analogy.</p><p>This is often likened to a Jazz band versus a Symphony Orchestra.</p><p>A modern symphony orchestra is normally comprised of sections such as strings, brass, woodwind and percussion sections. The sections are orchestrated by a conductor, usually placed at a central point with respect to each of the sections. The conductor instructs each section to perform their components of the overall symphony.</p><p>By contrast, a Jazz band does not have a conductor and also features improvisation, with different musicians improvising based upon each other. Choreography, although more aligned to dance, can involve improvisation. In both cases there is still an intended output and a general framework as to how the composition will be executed, however unlike a symphony orchestra there is a degree of spontaneity.</p><p><em>Now back to technology and microservices…</em></p><p>In the Orchestration model, there is a central orchestration service which controls the interactions between other services, in other words the flow and control of communication and/or message passing between services is controlled by an orchestrator (much like the conductor in a symphony orchestra). On the plus side, this model enables easier monitoring and policy enforcement across the system. A generalisation of the Orchestration model is shown below:</p><p><a href="images/orchestration.png"><img src="images/orchestration.png" alt="Orchestration model"/></a></p><p>By contrast, in the Choreography model, each service works independently and interacts with other services through events. In this model each service registers and emits events as they need to. The flow (of communication between services) is not predefined, much like a Jazz band. This model often includes a central broker for message passing between services, but the services operate independently of each other and are not controlled by a central service (an orchestrator). A generalisation of the Choreography model is shown below:</p><p><a href="images/choreography.png"><img src="images/choreography.png" alt="Choreography model"/></a></p><p>We will post subsequent articles with implementations of these patterns, but it is worthwhile getting a foundational understanding first.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using the Azure CLI to Create an API using a Function App within API Management]]></title>
            <link>https://cloudywithachanceofbigdata.com/using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management</link>
            <guid>using-the-azure-cli-to-create-an-api-using-a-function-app-within-api-management</guid>
            <pubDate>Wed, 06 Jan 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[Function Apps, Logic Apps and App Services can be used to expose APIs within Azure API Management which is an easy way to deploy serverless microservices. You can see this capability in the Azure portal below within API Management:]]></description>
            <content:encoded><![CDATA[<p>Function Apps, Logic Apps and App Services can be used to expose APIs within Azure API Management which is an easy way to deploy serverless microservices. You can see this capability in the Azure portal below within API Management:</p><p><a href="images/apimamanagement-add-fnapp.png"><img src="images/apimamanagement-add-fnapp.png" alt="Add a new API using a Function App as a back end"/></a></p><p>Like most readers, I like to script everything, so I was initially frustrated when I couldn’t replicate this operation in the Azure cli, REST, PowerShell, or any of the other SDKs or IaC tools. Others shared my frustration as seen <a href="https://feedback.azure.com/forums/248703-api-management/suggestions/36832033-programmatically-import-azure-function-into-apim">here</a>.</p><p>I was nearly resigned to using click ops in the portal (arrrgh) before I worked out this workaround.</p><h2>The Solution</h2><p>There is a bit more prep work required to automate this process, but it is well worth it.</p><ol><li>Create an OpenApi (or Swagger spec or WADL) specification document, as seen below (use the absolute URL for your Function App in the <code>url</code> parameter):</li></ol><div id="077e8f313e6f44393df71057c8af7850"></div><ol start="2"><li>Use the <code>az apim api import</code> function (not the <code>az apim api create</code> function), as shown here:</li></ol><div id="1f5eec542bd5ec01dbb9a06472e8e59b"></div><ol start="3"><li>Associate the API with a product (which is how you can rate limit APIs)</li></ol><div id="4ad9c81b97ee97fb2cb6f794c2ae820f"></div><p>That’s it! You can now access your function via the API gateway using the gateway url or via the developer portal as seen below:</p><p><a href="images/apimamanagement-test-api.png"><img src="images/apimamanagement-test-api.png" alt="Function App API in API Management in the Azure Portal"/></a></p><p><a href="images/apimamanagement-dev-portal.png"><img src="images/apimamanagement-dev-portal.png" alt="Function App API in the Dev Portal"/></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Great Expectations (for your data...)]]></title>
            <link>https://cloudywithachanceofbigdata.com/great-expectations-for-your-data</link>
            <guid>great-expectations-for-your-data</guid>
            <pubDate>Fri, 27 Nov 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This article provides an introduction to the Great Expectations Python library for data quality management (https://github.com/great-expectations/great\expectations).]]></description>
            <content:encoded><![CDATA[<p>This article provides an introduction to the Great Expectations Python library for data quality management (<a href="https://github.com/great-expectations/great_expectations">https://github.com/great-expectations/great<!-- -->_<!-- -->expectations</a>).</p><p>So what are expectations when it comes to data (and data quality)...</p><p>An expectation is a falsifiable, verifiable statement about data. Expectations provide a language to talk about data characteristics and data quality - humans to humans, humans to machines and machines to machines.</p><p>The <strong>great expectations</strong> project includes predefined, codified expectations such as:</p><pre><code>expect_column_to_exist 
expect_table_row_count_to_be_between 
expect_column_values_to_be_unique 
expect_column_values_to_not_be_null 
expect_column_values_to_be_between 
expect_column_values_to_match_regex 
expect_column_mean_to_be_between 
expect_column_kl_divergence_to_be_less_than
</code></pre><p>… and many more</p><p>Expectations are both data tests and docs! Expectations can be presented in a machine-friendly JSON, for example:</p><div id="317fa68cc27e4e364ab238a93f6ed361"></div><p>Great Expectations provides validation results of defined expectations, which can dramatically shorten your development cycle.</p><p>[<img src="images/validation_failed_unexpected_values.gif" alt="validation results in great expectations"/>](validation results in great expectations)</p><p>Nearly 50 built in expectations allow you to express how you understand your data, and you can add custom expectations if you need a new one. A machine can test if a dataset conforms to the expectation.</p><h2>OK, enough talk, let&#x27;s go!</h2><pre><code class="language-bash">pyenv virtualenv 3.8.2 ge38
pip install great-expectations
</code></pre><p>tried with Python 3.7.2, but had issues with library <code>lgzm</code> on my local machine</p><p>once installed, run the following in the python repl shell:</p><div id="78211408899d5d0d4b1a088d039fe1d3"></div><p>showing the data in the dataframe should give you the following:</p><div id="fbdeda83bfe9af7ceb33a36a3f4a29e0"></div><p>as can be seen, a collection of random integers in each column for our initial testing. Let&#x27;s pipe this data in to great-expectations...</p><div id="0bf00ff1bfad316f83f1b458aa2ad01b"></div><p>yields the following output...</p><div id="83264e645d220ebb0f0a529a2a139be9"></div><p>this shows that there are 0 unexpected items in the data we are testing. Great!</p><p>Now let&#x27;s have a look at a negative test. Since we&#x27;ve picked the values at random, there are bound to be duplicates. Let&#x27;s test that:</p><div id="0a75fe9700677d0329d96da44c54dca5"></div><p>yields...</p><div id="0c714a928064124ec15c9733d9e5ff29"></div><p>The JSON schema has metadata information about the result, of note is the result section which is specific to our query, and shows the percentage that failed the expectation.</p><p>Let&#x27;s progress to something more real-world, namely creating exceptions that are run on databases. Armed with our basic understanding of great-expectations, let&#x27;s...</p><ul><li>set up a postgres database</li><li>initiate a new Data Context within great-expectations</li><li>write test-cases for the data</li><li>group those test-cases and</li><li>run it</li></ul><h2>Setting up a Database</h2><div id="30dc4a230656f36c4c9b7b208b792329"></div><p>if you don&#x27;t have it installed,</p><div id="b127a57c499e63c003ac6c8bb4408768"></div><p>wait 15 minutes for download the internet. Verify postgres running with <code>docker ps</code>, then connect with</p><div id="d33c81448fcfddb608f79a69a75202a1"></div><p>Create some data</p><div id="13069c3111039ac60281f291dc1e6bd8"></div><p>Take data for a spin</p><div id="f62857c6dbaaaf1d784aa94c5914c9f5"></div><p>should yield</p><div id="52e11ea7319f8344674999ec1b36ab0e"></div><p>Now time for <code>great-expectations</code></p><p>Great Expectations relies on the library <code>sqlalchemy</code> and <code>psycopg2</code> to connect to your data.</p><div id="6bd28c03b4013f4301b1af2c74dcd947"></div><p>once done, let&#x27;s set up <code>great-expectations</code></p><div id="4210011daa1fb3c738875f917ebafbf3"></div><p>should look like below:</p><div id="1f1a698cf6a6785598db1133212f30fe"></div><p>let&#x27;s set up a few other goodies while we&#x27;re here</p><div id="51f684d6db8149950f28cc32afa2f461"></div><p><strong>Congratulations! Great Expectations is now set up</strong></p><p>You should see a file structure as follows:</p><p><a href="images/ge_tree_structure.png"><img src="images/ge_tree_structure.png" alt="great expectations tree structure"/></a></p><p>If you didn&#x27;t generate a suite during the set up based on <code>app.order</code>, you can do so now with</p><p><code>great_expectations suite new</code></p><p>when created, looking at <code>great_expectations/expectations/app/order/warning.json</code> should yield the following:</p><div id="24d105be1cacacbd89b9e4cbac6f4d21"></div><p>as noted in the content section, this expectation config is created by the tool by looking at 1000 rows of the data. We also have access to the data-doc site which we can open in the browser at <code>great_expectations/uncommitted/data_docs/local_site/index.html</code></p><p><a href="images/index-page.png"><img src="images/index-page.png" alt="great expectations index page"/></a></p><p>Clicking on <code>app.order.warning</code>, you&#x27;ll see the sample expectation shown in the UI</p><p><a href="images/ge-app.order-screen.png"><img src="images/ge-app.order-screen.png" alt="great expectations app order screen"/></a></p><p>Now, let&#x27;s create our own <code>expectation</code> file and take it for a spin. We&#x27;ll call this one <code>error</code>.</p><p><a href="images/ge_suite_new.png"><img src="images/ge_suite_new.png" alt="great expectations new suite"/></a></p><p>This should also start a <code>jupyter notebook</code>. If for some reason you need to start it back up again, you can do so with</p><div id="b4c9fed39b89d939127e4b381c49f274"></div><p>Go ahead and hit <code>run</code> on your first cell.</p><p><a href="images/jupyter-edit-suite.png"><img src="images/jupyter-edit-suite.png" alt="Editing a suite with Jupyter"/></a></p><p>Let&#x27;s keep it simple and test the <code>customer_order_id</code> column is in a set with the values below:</p><div id="fea43e4e566795213fc1c5bbcda317ad"></div><p>using the following expectations function in your Table Expectation(s). You may need to click the <code>+</code> sign in the toolbar to insert a new cell, as below:</p><div id="38be05ccb27c21da2b4213d6a63afd83"></div><p><a href="images/add-table-expectation.png"><img src="images/add-table-expectation.png" alt="Adding table expectation"/></a></p><p>As we can see, appropriate json output that describes the output of our expectation. Go ahead and run the final cell, which will save our work and open a newly minted data documentation UI page, where you&#x27;ll see the expectations you defined in human readable form.</p><p><a href="images/saved-suite.png"><img src="images/saved-suite.png" alt="Saved suite"/></a></p><h2>Running the test cases</h2><p>In Great Expectations, running a set of expectations (test cases) is called a <code>checkpoint</code>. Let&#x27;s create a new checkpoint called <code>first_checkpoint</code> for our <code>app.order.error</code> expectation as shown below:</p><div id="322483724633e8a4513c5d6ae67298ae"></div><p>Let&#x27;s take a look at our checkpoint definition.</p><div id="08594ad6420d46fe60be41d9d949605c"></div><div id="9cdd4da31f9fdef1246092fa5b65e90c"></div><p>Above you can see the <code>validation_operator_name</code> which points to a definition in <code>great_expectations.yml</code>, and the <code>batches</code> where we defined the data source and what expectations to run against.</p><p>Let&#x27;s have a look at <code>great_expectations.yml</code>. We can see the <code>action_list_operator</code> defined and all the actions it contains:</p><p><a href="images/ge_action_list_operator.png"><img src="images/ge_action_list_operator.png" alt="List operators"/></a></p><p>Let&#x27;s run our checkpoint using</p><div id="050501513ee0f5fa13ab522ea7b9242e"></div><p><a href="images/validate-checkpoint.png"><img src="images/validate-checkpoint.png" alt="Validate checkpoint"/></a></p><p>Okay cool, we&#x27;ve set up an expectation, a checkpoint and shown a successful status! But what does a failure look like? We can introduce a failure by logging in to postgres and inserting a <code>customer_11</code> that we&#x27;ll know will fail, as we&#x27;ve specific our expectation that <code>customer_id</code> should only have two values..</p><div id="5bce4ba20ab8bfae5910db5cd6cc66f4"></div><p>Here are the commands to make that happen, as well as the command to re-run our checkpoint:</p><div id="4ad6f1753d65a4f099467bd9fd760067"></div><div id="fa94ded27b212e099177bee9d6a2cd36"></div><p>Run checkpoint again, this time it should fail</p><div id="12fb3e62a5bbcd205ee84ff7445e2657"></div><p><a href="images/failed-checkpoint.png"><img src="images/failed-checkpoint.png" alt="Failed checkpoint"/></a></p><p><strong>As expected, it failed.</strong></p><h2>Supported Databases</h2><p>In it&#x27;s current implementation <code>version 0.12.9</code>, the supported databases our of the box are:</p><div id="4816fb9a621d2d660f746b62ab54ba59"></div><p>It&#x27;s great to be BigQuery supported out of the box, but what about Google Spanner and Google BigTable? Short-answer; currently not supported. See tickets <a href="https://github.com/googleapis/google-cloud-python/issues/3022">https://github.com/googleapis/google-cloud-python/issues/3022</a>.</p><p>With respect to BigTable, it may not be possible as SQLAlchemy can only manage SQL-based RDBSM-type systems, while BigTable (and HBase) are NoSQL non-relational systems.</p><h2>Scheduling</h2><p>Now that we have seen how to run tests on our data, we can run our checkpoints from bash or a python script(generated using great_expectations checkpoint script first_checkpoint). This lends itself to easy integration with scheduling tools like airflow, cron, prefect, etc.</p><h2>Production deployment</h2><p>When deploying in production, you can store any sensitive information(credentials, validation results, etc) which are part of the uncommitted folder in cloud storage systems or databases or data stores depending on your infratructure setup. Great Expectations has a lot of options</p><h2>When not to use a data quality framework</h2><p>This tool is great and provides a lot of advanced data quality validation functions, but it adds another layer of complexity to your infrastructure that you will have to maintain and trouble shoot in case of errors. It would be wise to use it only when needed.</p><h2>In general</h2><p>Do not use a data quality framework, if simple SQL based tests at post load time works for your use case. Do not use a data quality framework, if you only have a few (usually &lt; 5) simple data pipelines.</p><p>Do use it when you have data that needs to be tested in an automated and a repeatable fashion. As shown in this article, Great Expectations has a number of options that can be toggled to suit your particular use-case.</p><h2>Conclusion</h2><p>Great Expectations shows a lot of promise, and it&#x27;s an active project so expect to see features roll out frequently. It&#x27;s been quite easy to use, but I&#x27;d like to see all it&#x27;s features work in a locked-down enterprise environment.</p><p>Tom Klimovski<br/>
<!-- -->Principal Consultant, Gamma Data<br/>
<a href="mailto:tom.klimovski@gammadata.io">tom.klimovski@gammadata.io</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Multi Cloud Diagramming with PlantUML]]></title>
            <link>https://cloudywithachanceofbigdata.com/multi-cloud-diagramming-with-plantuml</link>
            <guid>multi-cloud-diagramming-with-plantuml</guid>
            <pubDate>Mon, 26 Oct 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Following on from the recent post GCP Templates for C4 Diagrams using PlantUML, cloud architects are often challenged with producing diagrams for architectures spanning multiple cloud providers, particularly as you elevate to enterprise level diagrams.]]></description>
            <content:encoded><![CDATA[<p>Following on from the recent post <a href="https://cloudywithachanceofbigdata.com/gcp-templates-for-c4-diagrams-using-plantuml/">GCP Templates for C4 Diagrams using PlantUML</a>, cloud architects are often challenged with producing diagrams for architectures spanning multiple cloud providers, particularly as you elevate to enterprise level diagrams.</p><p>In this post, with the magic of <code>!includeurl</code> we have brought PlantUML template libraries together for AWS, Azure and GCP icon sets, allowing us to produce multi cloud C4 diagrams using PlantUML like this one:</p><p><a href="images/Example-Multi-Cloud-PlantUML-C4-Diagram.png"><img src="images/Example-Multi-Cloud-PlantUML-C4-Diagram.png" alt="Multi Cloud Architecture Diagram using PlantUML"/></a></p><p>Creating a multi cloud diagram is simple, start by adding the following <code>include</code> statements after the <code>@startuml</code> label in a new PlantUML C4 diagram:</p><div id="5319b6b041f8b8f54c922a9a5b9b6e7c"></div><p>Then add references to the required services from different providers…</p><div id="6ed55cd1b4e3b2e7027f8236af4aa112"></div><p>Then include the predefined resources from your different cloud providers in your diagram as shown here (describing a client server application over a cloud to cloud VPN between Azure and GCP)...</p><div id="600aecff7094d7843771770b7048cb2c"></div><p>Happy multi-cloud diagramming!</p><blockquote><p>Full source code is available at:</p><p><a href="https://github.com/gamma-data/plantuml-multi-cloud-diagrams">https://github.com/gamma-data/plantuml-multi-cloud-diagrams</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud Bigtable Primer Part II – Row Key Selection and Schema Design]]></title>
            <link>https://cloudywithachanceofbigdata.com/cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design</link>
            <guid>cloud-bigtable-primer-part-ii-row-key-selection-and-schema-design</guid>
            <pubDate>Sun, 13 Sep 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a follow up to the original Cloud Bigtable primer where we discussed the basics of Cloud Bigtable:]]></description>
            <content:encoded><![CDATA[<p>This is a follow up to the original Cloud Bigtable primer where we discussed the basics of Cloud Bigtable:</p><p><a href="https://cloudywithachanceofbigdata.com/cloud-bigtable-primer-part-i/"><strong>Cloud Bigtable Primer - Part I</strong></a></p><p>In this article we will cover schema design and row key selection in Bigtable – arguably the most critical design decision to make when employing Bigtable in a cloud data architecture.</p><h2>Quick Review</h2><p>Recall from the previous post where the Bigtable data model was introduced that tables in Bigtable are comprised of rows and columns - much like a table in any other RDBMS. Every row is uniquely identified by a rowkey – again akin to a primary key in a table in an RDBMS. But this is where the similarities end.</p><p>Unlike a table in an RDBMS, columns only ever exist when they are inserted, and <code>NULLs</code> are not stored. See the illustration below:</p><p><a href="images/bigtable-data-model.png"><img src="images/bigtable-data-model.png"/></a></p><h2>Row Key Selection</h2><p>Data in Bigtable is distributed by row keys. Row keys are physically stored in tablets in lexographic order. Recall that row keys are your ONLY indexes to data in Bigtable.</p><h3>Selection Considerations</h3><p>As row keys are your only indexes to retrieve or update rows in Bigtable, row key design must take the access patterns for the data to be stored and served via Bigtable into consideration, specifically the following must be considered when designing a Bigtable application:</p><ul><li>Search patterns (returning data for a specific entity)</li><li>Scan patterns (returning batches of data)</li></ul><p>Queries that use the row key, a row prefix, or a row range are the most efficient. Queries that do not include a row key will typically scan GB or TB of data and would not be suitable for operational use cases.</p><h3>Row Key Performance</h3><p>Row key performance will be biased towards your specific access patterns and application functional requirements. For example if you are performing sequential reads or scan operations then sequential keys will perform the best, however their write performance will not be optimal. Conversely, random keys (such as a <code>uuid</code>) will perform best for writes but poor for scan or sequential read operations.</p><p>Adding salts to keys (or additional data), similar to the use of salts in cryptography as well as promoting other field keys to be part of a composite row key can help achieve a “Goldilocks” scenario for both reads and writes, see the diagram below:</p><p><a href="images/keys.png"><img src="images/keys.png"/></a></p><h3>Using Reverse Timestamps</h3><p>Use reverse timestamps when your most common query is for the latest values. Typically you would append the reverse timestamp to the key, this will ensure that the same related records are grouped together, for instance if you are storing events for a customer using the customer id along with an appended reverse timestamp (for example <code>&lt;customer_id&gt;#&lt;reverse_ts&gt;</code>) would allow you to quickly serve the latest events for a customer in descending order as within each group (<code>customer_id</code>), rows will be sorted so most recent insert will be located at the top.<br/>
<!-- -->A reverse timestamp can be generalised as:</p><p><code>Long.MAX_VALUE - System.currentTimeMillis()</code></p><h3>Schema Design Tips</h3><p>Some general tips for good schema design using Bigtable are summarised below:</p><ul><li>Group related data for more efficient reads using column families</li><li>Distribute data evenly for more efficient writes</li><li>Place identical values in the adjoining rows for more efficient compression using row keys</li></ul><p>Following these tips will give you the best possible performance using Bigtable.</p><h3>Use the Key Visualizer to profile performance</h3><p>Google provides a neat tool to visualize your row key distribution in Cloud Bigtable. You need to have at least 30 GB of data in your table to enable this feature.</p><p>The Key Visualizer is shown here:</p><p><a href="images/image.png"><img src="images/image.png" alt="Bigtable Key Visualizer"/></a></p><p>The Key Visualizer will help you find and prevent hotspots, find rows with too much data and show if your key schema is balanced.</p><h3>Summary</h3><p>Bigtable is one of the original and best (massively) distributed NoSQL platforms available. Schema and moreover row key design play a massive part in ensuring low latency and query performance. Go forth and conquer with Cloud Bigtable!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GCP Templates for C4 Diagrams using PlantUML]]></title>
            <link>https://cloudywithachanceofbigdata.com/gcp-templates-for-c4-diagrams-using-plantuml</link>
            <guid>gcp-templates-for-c4-diagrams-using-plantuml</guid>
            <pubDate>Fri, 14 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[I am a believer in the mantra of **“Everything-as-Code”**, this includes diagrams and other architectural artefacts. Enter PlantUML…]]></description>
            <content:encoded><![CDATA[<p>I am a believer in the mantra of <em><strong>“Everything-as-Code”</strong></em>, this includes diagrams and other architectural artefacts. Enter PlantUML…</p><h2>PlantUML</h2><p><a href="https://plantuml.com/">PlantUML</a> is an open-source tool which allows users to create UML diagrams from an intuitive DSL (Domain Specific Language). PlantUML is built on top of Graphviz and enables software architects and designers to use code to create Sequence Diagrams, Use Case Diagrams, Class Diagrams, State and Activity Diagrams and much more.</p><h2>C4 Diagrams</h2><p>PlantUML can be extended to support the <a href="https://c4model.com/">C4 model</a> for visualising software architecture. Which describes software in different layers including Context, Container, Component and Code diagrams.</p><h2>GCP Architecture Diagramming using C4</h2><p>PlantUML and C4 can be used to produce cloud architectures, there are official libraries available through PlantUML for Azure and AWS service icons, however these do not exist for GCP yet. There are several open source libraries available, however I have made an attempt to simplify the implementation.</p><p>The code below can be used to generate a C4 diagram describing a GCP architecture including official GCP service icons:</p><pre><code>@startuml
!define GCPPuml https://raw.githubusercontent.com/gamma-data/GCP-C4-PlantUML/master/templates

!includeurl GCPPuml/C4\_Context.puml
!includeurl GCPPuml/C4\_Component.puml
!includeurl GCPPuml/C4\_Container.puml
!includeurl GCPPuml/GCPC4Integration.puml
!includeurl GCPPuml/GCPCommon.puml

!includeurl GCPPuml/Networking/CloudDNS.puml
!includeurl GCPPuml/Networking/CloudLoadBalancing.puml
!includeurl GCPPuml/Compute/ComputeEngine.puml
!includeurl GCPPuml/Storage/CloudStorage.puml
!includeurl GCPPuml/Databases/CloudSQL.puml

title Sample C4 Diagram with GCP Icons

Person(publisher, &quot;Publisher&quot;)
System\_Ext(device, &quot;User&quot;)

Boundary(gcp,&quot;gcp-project&quot;) {
  CloudDNS(dns, &quot;Managed Zone&quot;, &quot;Cloud DNS&quot;)
  CloudLoadBalancing(lb, &quot;L7 Load Balancer&quot;, &quot;Cloud Load Balancing&quot;)
  CloudStorage(bucket, &quot;Static Content Bucket&quot;, &quot;Cloud Storage&quot;)
  Boundary(region, &quot;gcp-region&quot;) {
    Boundary(zonea, &quot;zone a&quot;) {
      ComputeEngine(gcea, &quot;Content Server&quot;, &quot;Compute Engine&quot;)
      CloudSQL(csqla, &quot;Dynamic Content&quot;, &quot;Cloud SQL&quot;)
    }
    Boundary(zoneb, &quot;zone b&quot;) {
      ComputeEngine(gceb, &quot;Content Server&quot;, &quot;Compute Engine&quot;)
      CloudSQL(csqlb, &quot;Dynamic Content\\n(Read Replica)&quot;, &quot;Cloud SQL&quot;)
    }
  }
}

Rel(device, dns, &quot;resolves name&quot;)
Rel(device, lb, &quot;makes request&quot;)
Rel(lb, gcea, &quot;routes request&quot;)
Rel(lb, gceb, &quot;routes request&quot;)
Rel(gcea, bucket, &quot;get static content&quot;)
Rel(gceb, bucket, &quot;get static content&quot;)
Rel(gcea, csqla, &quot;get dynamic content&quot;)
Rel(gceb, csqla, &quot;get dynamic content&quot;)
Rel(csqla, csqlb, &quot;replication&quot;)
Rel(publisher,bucket,&quot;publish static content&quot;)

@enduml
</code></pre><p>The preceding code generates the diagram below:</p><p><a href="images/Sample-C4-Diagram-with-GCP-Icons.png"><img src="images/Sample-C4-Diagram-with-GCP-Icons.png"/></a></p><p>Additional services can be added and used in your diagrams by adding them to your includes, such as:</p><pre><code>!includeurl GCPPuml/DataAnalytics/BigQuery.puml
!includeurl GCPPuml/DataAnalytics/CloudDataflow.puml
!includeurl GCPPuml/AIandMachineLearning/AIHub.puml
!includeurl GCPPuml/AIandMachineLearning/CloudAutoML.puml
!includeurl GCPPuml/DeveloperTools/CloudBuild.puml
!includeurl GCPPuml/HybridandMultiCloud/Stackdriver.puml
!includeurl GCPPuml/InternetofThings/CloudIoTCore.puml
!includeurl GCPPuml/Migration/TransferAppliance.puml
!includeurl GCPPuml/Security/CloudIAM.puml
&#x27; and more…
</code></pre><blockquote><p>The complete template library is available at:</p><p><a href="https://github.com/gamma-data/GCP-C4-PlantUML">https://github.com/gamma-data/GCP-C4-PlantUML</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud Bigtable Primer - Part I]]></title>
            <link>https://cloudywithachanceofbigdata.com/cloud-bigtable-primer-part-i</link>
            <guid>cloud-bigtable-primer-part-i</guid>
            <pubDate>Tue, 04 Aug 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Bigtable is one of the foundational services in the Google Cloud Platform and to this day one of the greatest contributions to the big data ecosystem at large. It is also one of the least known services available, with all the headlines and attention going to more widely used services such as BigQuery.]]></description>
            <content:encoded><![CDATA[<p>Bigtable is one of the foundational services in the Google Cloud Platform and to this day one of the greatest contributions to the big data ecosystem at large. It is also one of the least known services available, with all the headlines and attention going to more widely used services such as BigQuery.</p><h2>Background</h2><p>In 2006 (pre Google Cloud Platform), Google released a white paper called <em><strong>“Bigtable: A Distributed Storage System for Structured Data”</strong></em>, this paper set out the reference architecture for what was to become Cloud Bigtable. This followed several other whitepapers including the GoogleFS and MapReduce whitepapers released in 2003 and 2004 which provided abstract reference architectures for the Google File System (now known as <strong><em>Colossus</em></strong>) and the MapReduce algorithm. These whitepapers inspired a generation of open source distributed processing systems including Hadoop. Google has long had a pattern of publicising a generalized overview of their approach to solving different storage and processing challenges at scale through white papers.</p><p><a href="assets/bigtable-osdi06.pdf"><img src="images/bigtable-whitepaper.png" alt="Bigtable Whitepaper 2006"/></a></p><p>The Bigtable white paper inspired a wave of open source distributed key/value oriented NoSQL data stores including Apache HBase and Apache Cassandra.</p><h2>What is Bigtable?</h2><p>Bigtable is a distributed, petabyte scale NoSQL database. More specifically, Bigtable is…</p><h3>a map</h3><p>At its core Bigtable is a distributed map or an associative array indexed by a row key, with values in columns which are created only when they are referenced. Each value is an uninterpreted byte array.</p><h3>sorted</h3><p>Row keys are stored in lexographic order akin to a clustered index in a relational database.</p><h3>sparse</h3><p>A given row can have any number of columns, not all columns must have values and NULLs are not stored. There may also be gaps between keys.</p><h3>multi-dimensional</h3><p>All values are versioned with a timestamp (or configurable integer). Data is not updated in place, it is instead superseded with another version.</p><h2>When (and when not) to use Bigtable</h2><ul><li>You need to do many thousands of operations per second on TB+ scale data</li><li>Your access patterns are well known and simple</li><li>You need to support random write or random read operations (or sequential reads) - each using a row key as the primary identifier</li></ul><h3>Don’t use Bigtable if…</h3><ul><li>You need explicit JOIN capability, that is joining one or more tables</li><li>You need to do ad-hoc analytics</li><li>Your access patterns are unknown or not well defined</li></ul><h3>Bigtable vs Relational Database Systems</h3><p>The following table compares and contrasts Bigtable against relational databases (both transaction oriented and analytic oriented databases):</p><table><thead><tr><th> </th><th>Bigtable</th><th>RDBMS (OLTP)</th><th>RDBMS (DSS/MPP)</th></tr></thead><tbody><tr><td>Data Layout</td><td>Column Family Oriented</td><td>Row Oriented</td><td>Column Oriented</td></tr><tr><td>Transaction Support</td><td>Single Row Only</td><td>Yes</td><td>Depends (but usually no)</td></tr><tr><td>Query DSL</td><td>get/put/scan/delete</td><td>SQL</td><td>SQL</td></tr><tr><td>Indexes</td><td>Row Key Only</td><td>Yes</td><td>Yes (typically PI based)</td></tr><tr><td>Max Data Size</td><td>PB+</td><td>&#x27;00s GB  to TB</td><td>TB+</td></tr><tr><td>Read/Write Throughput</td><td>&quot;&#x27;000</td><td>000s queries/s&quot;</td><td>&#x27;000s queries/s</td></tr></tbody></table><h2>Bigtable Data Model</h2><p><strong><em>Tables</em></strong> in Bigtable are comprised of rows and columns (sounds familiar so far..). Every row is uniquely identified by a <strong><em>rowkey</em></strong> (like a primary key..again sounds familiar so far).</p><p><strong><em>Columns</em></strong> belong to <strong><em>Column Families</em></strong> and only exist when inserted, NULLs are not stored - this is where it starts to differ from a traditional RDBMS. The following image demonstrates the data model for a fictitious table in Bigtable.</p><p><a href="images/bigtable-data-model.png"><img src="images/bigtable-data-model.png" alt="Bigtable Data Model"/></a></p><p>In the previous example, we created two Column Families (<strong><em>cf1</em></strong> and <strong><em>cf2</em></strong>). These are created during table definition or update operations (akin to DDL operations in the relational world). In this case, we have chosen to store primary attributes, like name, etc in cf1 and features (or derived attributes) in cf2 like indicators.</p><h3>Cell versioning</h3><p>Each cell has a timestamp/version associated with it, multiple versions of a row can exist. Versions are naturally stored in descending order.</p><p>Properties such as the max age for a cell or the maximum number of versions to be stored for any given cell are set on the Column Family. Versions are compacted through a process called <strong><em>Garbage Collection</em></strong> - not to be confused with Java Garbage Collection (albeit same idea).</p><table><thead><tr><th>Row Key</th><th>Column</th><th>Value</th><th>Timestamp</th></tr></thead><tbody><tr><td>123</td><td>cf1:status</td><td>ACTIVE</td><td>2020-06-30T08.58.27.560</td></tr><tr><td>123</td><td>cf1:status</td><td>PENDING</td><td>2020-06-28T06.20.18.330</td></tr><tr><td>123</td><td>cf1:status</td><td>INACTIVE</td><td>2020-06-27T07.59.20.460</td></tr></tbody></table><h2>Bigtable Instances, Clusters, Nodes and Tables</h2><p>Bigtable is a &quot;no-ops&quot; service, meaning you do not need to configure machine types or details about the underlying infrastructure, save a few sizing or performance options - such as the number of nodes in a cluster or whether to use solid state hard drives (SSD) or the magnetic alternative (HDD). The following diagram shows the relationships and cardinality for Cloud Bigtable.</p><p><a href="images/bigtable-instances-and-nodes.png"><img src="images/bigtable-instances-and-nodes.png" alt="Bigtable Instances, Clusters and Nodes"/></a></p><p><strong><em>Clusters</em></strong> and <strong><em>nodes</em></strong> are the physical compute layer for Bigtable, these are zonal assets, zonal and regional availability can be achieved through replication which we will discuss later in this article.</p><p><strong><em>Instances</em></strong> are a virtual abstraction for clusters, Tables belong to instances (not clusters). This is due to Bigtables underlying architecture which is based upon a separation of storage and compute as shown below.</p><p><a href="images/bigtable-storage-and-compute.png"><img src="images/bigtable-storage-and-compute.png" alt="Bigtable Separation of Storage and Compute"/></a></p><p>Bigtables separation of storage and compute allow it to scale horizontally, as nodes are stateless they can be increased to increase query performance. The underlying storage system in inherently scalable.</p><h3>Physical Storage &amp; Column Families</h3><p>Data (Columns) for Bigtable is stored in <strong><em>Tablets</em></strong> (as shown in the previous diagram), which store &quot;regions&quot; of row keys for a particular Column Family. Columns consist of a column family prefix and qualifier, for instance:</p><pre><code>cf1:col1
</code></pre><p>A table can have one or more Column Families. Column families must be declared at schema definition time (could be a create or alter operation). A cell is an intersection of a row key and a version of a column within a column family.</p><p>Storage settings (such as the compaction/garbage collection properties mentioned before) can be specified for each Column Family - which can differ from other column families in the same table.</p><h3>Bigtable Availability and Replication</h3><p><strong><em>Replication</em></strong> is used to increase availability and durability for Cloud Bigtable – this can also be used to segregate read and write operations for the same table.</p><p>Data and changes to tables are replicated across multiple regions or multiple zones within the same region, this replication can be blocking (single row transactions) or non blocking (eventually consistent). However all clusters within a Bigtable instance are considered primary (writable).</p><p>Requests are routed using <strong><em>Application Profiles</em></strong>, a <strong><em>single-cluster routing</em></strong> policy can be used for manual failover, whereas a <strong><em>multi-cluster routing</em></strong> is used for automatic failover.</p><h3>Backup and Export Options for Bigtable</h3><p>Managed backups can be taken at a table level, new tables can be created from backups. The backups cannot be exported, however table level export and import operations are available via pre-baked Dataflow templates for data stored in GCS in the following formats:</p><ul><li>SequenceFiles</li><li>Avro Files</li><li>Parquet Files</li><li>CSV Files</li></ul><h2>Accessing Bigtable</h2><p>Bigtable data and admin functions are available via:</p><ul><li><code>cbt</code> (optional component of the Google SDK)</li><li><code>hbase shell</code> (REPL shell)</li><li>Happybase API (Python API for Hbase)</li><li>SDK libraries for:<ul><li>Golang</li><li>Python</li><li>Java</li><li>Node.js</li><li>Ruby</li><li>C#, C++, PHP, and more</li></ul></li></ul><p>As Bigtable is not a cheap service, there is a local emulator available which is great for application development. This is part of the Cloud SDK, and can be started using the following command:</p><pre><code>gcloud beta emulators bigtable start
</code></pre><p>In the next article in this series we will demonstrate admin and data functions as well as the local emulator.</p><blockquote><p>Next Up : Part II - Row Key Selection and Schema Design in Bigtable</p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automated GCS Object Scanning Using DLP with Notifications Using Slack]]></title>
            <link>https://cloudywithachanceofbigdata.com/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack</link>
            <guid>automated-gcs-object-scanning-using-dlp-with-notifications-using-slack</guid>
            <pubDate>Mon, 01 Jun 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a follow up to a previous blog, Google Cloud Storage Object Notifications using Slack in which we used Slack to notify us of new objects being uploaded to GCS.]]></description>
            <content:encoded><![CDATA[<p>This is a follow up to a previous blog, <a href="https://cloudywithachanceofbigdata.com/google-cloud-storage-object-notifications-using-slack/"><strong>Google Cloud Storage Object Notifications using Slack</strong></a> in which we used Slack to notify us of new objects being uploaded to GCS.</p><p>In this article we will take things a step further, where uploading an object to a GCS bucket will trigger a DLP inspection of the object and if any preconfigured info types (such as credit card numbers or API credentials) are present in the object, a Slack notification will be generated.</p><p>As DLP scans are “jobs”, meaning they run asynchronously, we will need to trigger scans and inspect results using two separate Cloud Functions (one for triggering a scan <!-- -->[<code>gcs-dlp-scan-trigger</code>]<!-- --> and one for inspecting the results of the scan <!-- -->[<code>gcs-dlp-evaluate-results</code>]<!-- -->) and a Cloud PubSub topic <!-- -->[<code>dlp-scan-topic</code>]<!-- --> which is used to hold the reference to the DLP job.</p><p>The process is described using the sequence diagram below:</p><p><a href="images/dlp-notifications-using-slack.png"><img src="images/dlp-notifications-using-slack.png"/></a></p><h2>The Code</h2><p>The <code>gcs-dlp-scan-trigger</code> Cloud Function fires when a new object is created in a specified GCS bucket. This function configures the DLP scan to be executed, including the DLP info types (for instance <code>CREDIT_CARD_NUMBER</code>, <code>EMAIL_ADDRESS</code>, <code>ETHNIC_GROUP</code>, <code>PHONE_NUMBER</code>, etc) a and likelihood of that info type existing (for instance <code>LIKELY</code>). DLP scans determine the probability of an info type occurring in the data, they do not scan every object in its entirety as this would be too expensive.</p><p>The primary function executed in the <code>gcs-dlp-scan-trigger</code> Cloud Function is named <code>inspect_gcs_file</code>. This function configures and submits the DLP job, supplying a PubSub topic to which the DLP Job Name will be written, the code for the <code>inspect_gcs_file</code> is shown here:</p><div id="913a4457f43bc7b80e4405dd01f7b64d"></div><p>At this stage the DLP job is created an running asynchronously, the next Cloud Function, <code>gcs-dlp-evaluate-results</code>, fires when a message is sent to the PubSub topic defined in the DLP job. The <code>gcs-dlp-evaluate-results</code> reads the DLP Job Name from the PubSub topic, connects to the DLP service and queries the job status, when the job is complete, this function checks the results of the scan, if the <code>min_likliehood</code> threshold is met for any of the specified info types, a Slack message is generated. The code for the main method in the <code>gcs-dlp-evaluate-results</code> function is shown here:</p><div id="ab377f6c3e448ae7c623d057239e05ed"></div><p>Finally, a Slack webhook is used to send the message to a specified Slack channel in a workspace, this is done using the <code>send_slack_notification</code> function shown here:</p><div id="15d9e7c0922c26b680bed81abfcbadff"></div><p>An example Slack message is shown here:</p><p><a href="images/gcs-dlp-results-slack-notification.png"><img src="images/gcs-dlp-results-slack-notification.png" alt="Slack Notification for Sensitive Data Detected in a Newly Created GCS Object"/></a></p><blockquote><p>Full source code can be found at: <a href="https://github.com/gamma-data/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack">https://github.com/gamma-data/automated-gcs-object-scanning-using-dlp-with-notifications-using-slack</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[JSON Wrangling with Go]]></title>
            <link>https://cloudywithachanceofbigdata.com/json-wrangling-with-go</link>
            <guid>json-wrangling-with-go</guid>
            <pubDate>Wed, 22 Apr 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Golang is a fantastic language but at first glance it is a bit clumsy when it comes to JSON in contrast to other languages such as Python or Javascript. Having said that once you master the concepts involved with JSON wrangling using Go it is equally as functional – with added type safety and performance.]]></description>
            <content:encoded><![CDATA[<p>Golang is a fantastic language but at first glance it is a bit clumsy when it comes to JSON in contrast to other languages such as Python or Javascript. Having said that once you master the concepts involved with JSON wrangling using Go it is equally as functional – with added type safety and performance.</p><p>In this article we will build a program in Golang to parse a JSON file containing a collection held in a named key – without knowing the structure of this object, we will expose the schema for the object including data types and recurse the object for its values.</p><p>This example uses a great Go package called <code>tablewriter</code> to render the output of these operations using a table style result set.</p><p>The program has <code>describe</code> and <code>select</code> verbs as operation types; describe shows the column names in the collection and their respective data types, select prints the keys and values as a tabular result set with column headers for the keys and rows containing their corresponding values.</p><p>Starting with this:</p><div id="cceeb5b667ccfe8a9e20437d3f1dde42"></div><p>We will end up with this when performing a <code>describe</code> operation:</p><div id="fbd04c220a70d439df3a14d4a4f48a3e"></div><p>And this when performing a <code>select</code> operation:</p><div id="0b795b13b160cfbcd6796243c0fbb238"></div><p>Now let’s talk about how we got there…</p><h2>The JSON package</h2><p>Support for JSON in Go is provided using the <code>encoding/json</code> package, this needs to be imported in your program of course… You will also need to import the <code>reflect</code> package – more on this later. <code>io/ioutil</code> is required to read the data from a file input, there are other packages included in the program that are removed for brevity:</p><div id="def7e02eac07ded8b80ff807cf023989"></div><h2>Reading the data…</h2><p>We will read the data from the JSON file into a variable called <code>body</code>, note that we are not attempting to deserialize the data at this point. This is also a good opportunity to handle any runtime or IO errors that occur here as well.</p><div id="74a2c2c839a30ed8cc66d83d3ddde3b4"></div><h2>The interface…</h2><p>We will declare an empty interface called <code>data</code> which will be used to decode the json object (of which the structure is not known), we will also create an abstract interface called <code>colldata</code> to hold the contents of the collection contained inside the JSON object that we are specifically looking for:</p><div id="32555f65af4be1fc2504f2d11e15aa19"></div><h2>Validating…</h2><p>Next we need to validate that the input is a valid JSON object, we can use the <code>json.Valid(body)</code> method to do this:</p><div id="c7afe41fcca4ba1e3ed009044cea76de"></div><h2>Unmarshalling…</h2><p>Now the interesting bits, we will deserialize the JSON object to the empty data interface we created earlier using the <code>json.Unmarshal()</code> method:</p><div id="2579ec79be915fb89e91ea0977bfbff6"></div><p>Note that this operation is another opportunity to catch unexpected errors and handle them accordingly.</p><h2>Checking the type of the object using reflection…</h2><p>Now that we have serialized the JSON object into the data interface, there are several ways we can inspect the type of the object (which could be a map or an array). One such way is to use reflection. Reflection is the ability of a program to inspect itself at runtime. An example is shown here:</p><div id="1ccd077de0fdee8973e25ac79719cbf5"></div><p>This instruction would produce the following output for our <code>zones.json</code> file:</p><div id="04c1b3ae79e969e4be32ef7fa1c07736"></div><h2>The type switch…</h2><p>Another method to decode the type of the data object (and any objects nested as elements or keys within the data object), is to use the type switch, an example of a type switch function is shown here:</p><div id="2e7a3d62ec6f7c71a9c01bfa8d360e4e"></div><h2>Finding the nested collection and recursing it…</h2><p>The aim of the program is to find a collection (an array of maps) nested in a JSON object. The maps with each element of the array are unknown at runtime and are discovered through recursion.</p><p>If we are performing a describe operation, we only need to parse the first element of the collection to get the key names and the data type of the values (for which we will use the same <code>getObjectType</code> function to perform a type switch.</p><p>If we are performing a select operation, we need to parse the first element to get the column names (the keys in the map) and then we need to recurse each element to get the values for each key.</p><p>If the element contains a key named id or name, we will place this at the beginning of the resultant record, as maps are unordered by definition.</p><h2>The output…</h2><p>As mentioned, we are using the <code>tablewriter</code> package to render the output of the collection as a pretty printed table in our terminal. As wrap around can get pretty ugly an additional <code>maxfieldlen</code> argument is provided to truncate the values if needed.</p><h2>In summary…</h2><p>Although it is a bit more involved than some other languages, once you get your head around processing JSON in Go, the possibilities are endless!</p><blockquote><p>Full source code can be found at: <a href="https://github.com/gamma-data/json-wrangling-with-golang">https://github.com/gamma-data/json-wrangling-with-golang</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Forseti Terraform Validator: Enforcing resource policy compliance in your CI pipeline]]></title>
            <link>https://cloudywithachanceofbigdata.com/forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline</link>
            <guid>forseti-terraform-validator-enforcing-resource-policy-compliance-in-your-ci-pipeline</guid>
            <pubDate>Sat, 18 Apr 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[Terraform is a powerful tool for managing your Infrastructure as Code. Declare your resources once, define their variables per environment and sleep easy knowing your CI pipeline will take care of the rest.]]></description>
            <content:encoded><![CDATA[<p>Terraform is a powerful tool for managing your Infrastructure as Code. Declare your resources once, define their variables per environment and sleep easy knowing your CI pipeline will take care of the rest.</p><p>But… one night you wake up in a sweat. The details are fuzzy but you were browsing your favourite cloud provider’s console - probably GCP ;) - and thought you saw a bucket had been created outside of your allowed locations! Maybe it even had risky access controls.</p><p>You go brush it off and try to fall back to sleep, but you can’t quite push the thought from your mind that somewhere in all that Terraform code, someone <em>could</em> be declaring resources in unapproved locations, and your CICD pipeline would do nothing to stop it. Oh the regulatory implications.</p><h2>Enter Terraform Validator by Forseti</h2><p>Terraform Validator by Forseti allows you to declare your Policy as Code, check compliance of your Terraform plans against said Policy, and automatically fail violating plans in a CI step. All without setting up servers or agents.</p><p>You’re going to learn how to enforce policy on GCP resources like BigQuery, IAM, networks, MySQL, Google Kubernetes Engine (GKE) and more. If you’re particularly crafty, you may be able to go beyond GCP.</p><p>Forseti’s suite of solutions are GCP focused and allow a wide range of live config validation, monitoring and more using the Policy Library we’re going to set up. These additional capabilities require additional infrastructure. But we’re going one step at a time, starting with enforcing policy during deployment.</p><h2>Getting Started</h2><p>Let’s assume you already have an established CICD pipeline that uses Terraform, or that you are content to validate your Terraform plans locally for now. In that case, we need just two things:</p><ol><li>A Policy Library</li><li>Terraform Validator</li></ol><p>It’s that simple! No new servers, agents, firewall rules, extra service accounts or other nonsense. Just add Policy Library, the Validator tool and you can enforce policy on your Terraform deployments.</p><p>We’re going to tinker with some existing GCP-focused sample policies (aka Constraints) that Forseti makes available. These samples cover a wide range of resources and use cases, so it is easy to adjust what’s provided to define your own Constraints.</p><h2>Policy Library</h2><p>First let&#x27;s open up some of Forseti&#x27;s pre-defined constraints. We’ll copy them into our own Git repository and adjust to create policies that match our needs. Repeatable and configurable - that’s Policy as Code at work.</p><h3>Concepts</h3><p>In the world of Forseti and in particular Terraform Validator, Policies are defined and understood via easy to read YAML files known as Constraints</p><p>There is just enough information in a Constraint file for to make clear its purpose and effect, and by tinkering lightly with a pre-written Constraint you can achieve a lot without looking too deeply into the inner workings . But there’s more happening than meets the eye.</p><p>Constraints are built on Templates - which are like forms with some extra bits waiting to be completed to make a Constraint. Except there’s a lot more hidden away that’s pretty cool if you want to understand it.</p><p>Think of a Template as a ‘Class’ in the OOP sense, and of a Constraint as an instantiated Template with all the key attributes defined.</p><p>E.g. A generic Template for policy on bucket locations and a Constraint to specify which locations are relevant in a given instance. Again, buckets and locations are just the basic example - the potential applications are far greater.</p><p>Now the real magic is that just like a ‘Class’, a Template contains logic that makes everything abstracted away in the Constraint possible. Templates contain inline Rego (ray-go), borrowed lovingly by Forseti from the Open Policy Agent (OPA) team.</p><p>Learn more about Rego and OPA <a href="https://www.openpolicyagent.org/docs/latest/policy-language/">here</a> to understand the relationship to our Terraform Validator.</p><p>But let’s begin.</p><h3>Set up your Policies</h3><h4>Create your Policy Library repository</h4><p>Create your Policy Library repository by cloning <a href="https://github.com/forseti-security/policy-library">https://github.com/forseti-security/policy-library</a> into your own VCS.</p><p>This repo contains templates and sample constraints which will form the basis of your policies. So get it into your Git environment and clone it to local for the next step.</p><h4>Customise sample constraints to fit your needs</h4><p>As discussed in Concepts, Constraints are defined Templates, which make use of Rego policy language. Nice. So let’s take a sample Constraint, put it in our Policy Library and set the values to what we need. It’s that easy - no need to write new templates or learn Rego if your use case is covered.</p><p>In a new branch…</p><ol><li>Copy the sample Constraint <code>storage_location.yaml</code> to your Constraints folder.  </li></ol><pre><code class="language-bash">$ cp policy-library/samples/storage_location.yaml policy-library/policies/constraints/storage_location.yaml
</code></pre><ol start="2"><li>Replace the sample location (<code>asia-southeast1</code>) in <code>storage_location.yaml</code> with <code>australia-southeast1</code>.  </li></ol><pre><code class="language-yaml">  spec:  
    severity: high  
    match:  
      target: [&quot;organization/*&quot;]  
    parameters:  
      mode: &quot;allowlist&quot;  
      locations:  
      - australia-southeast1  
      exemptions: []
</code></pre><ol start="3"><li>Push back to your repo - not Forseti’s!  </li></ol><pre><code class="language-bash">$ git push https://github.com/&lt;your-repository&gt;/policy-library.git
</code></pre><h4>Policy review</h4><p>There you go - you’ve customised a sample Constraint. Now you have your own instance of version controlled Policy-as-Code and are ready to apply the power of OPA’s Rego policy language that lies within the parent Template. Impressively easy right?</p><p>That’s a pretty simple example. You can browse the rest of Forseti’s Policy Library to view other sample Constraints, Templates and the Rego logic that makes all of this work. These can be adjusted to cover all kinds of use cases across GCP resources.</p><p>I suggest working with and editing the <a href="https://github.com/forseti-security/policy-library/tree/master/samples">sample Constraints</a> before making any changes to Templates.</p><p>If you were to write Rego and Templates from scratch, you might even be able to enforce Policy as Code against non-GCP Terraform code.</p><h2>Terraform Validator</h2><p>Now, let’s set up the Terraform Validator tool and have it compare a sample piece of Terraform code against the Constraint we configured above. Keep in mind you’ll want to translate what’s done here into steps in your CICD pipeline.</p><p>Once the tool is in place, we really just run <code>terraform plan</code> and feed the output into Terraform Validator. The Validator compares it to our Constraints, runs all the abstracted logic we don’t need to worry about and returns 0 or 2 when done for pass / fail respectively. Easy.</p><p>So using Terraform if I try to make a bucket in <code>australia-southeast1</code> it should pass, if I try to make one in the US it should fail. Let’s set up the tool, write some basic Terraform and see how we go.</p><h3>Setup Terraform Validator</h3><p>Check for the latest version of <code>terraform-validator</code> from the official terraform-validator GCS bucket.</p><p>Very important when using tf version 0.12 or greater. This is the easy way - you can pull from the <a href="https://github.com/GoogleCloudPlatform/terraform-validator">Terraform Validator Github</a> and make it yourself too.</p><pre><code class="language-bash">$ gsutil ls -r gs://terraform-validator/releases
</code></pre><p>Copy the latest version to the working dir</p><pre><code class="language-bash">$ gsutil cp gs://terraform-validator/releases/2020-03-05/terraform-validator-linux-amd64 .
</code></pre><p>Make it executable</p><pre><code class="language-bash">$ chmod 755 terraform-validator-linux-amd64
</code></pre><p>Ready to go!</p><h3>Review your Terraform code</h3><p>We’re going to make a ridiculously simple piece of Terraform that tries to create one bucket in our project to keep things simple.</p><pre><code># main.tf

resource &quot;google_storage_bucket&quot; &quot;tf-validator-demo-bucket&quot; {  
  name          = &quot;tf-validator-demo-bucket&quot;
  location      = &quot;US&quot;
  force_destroy = true

  lifecycle_rule {
    condition {
      age = &quot;3&quot;
    }
    action {
      type = &quot;Delete&quot;
    }
  }
}
</code></pre><p>This is a pretty standard bit of Terraform for a GCS bucket, but made very simple with all the values defined directly in <code>main.tf</code>. Note the location of the bucket - it violates our Constraint that was set to the <code>australia-southeast1</code> region.</p><h3>Make the Terraform plan</h3><p>Warm up Terraform.<br/>
<!-- -->Double check your Terraform code if there are any hiccups.</p><pre><code class="language-bash">$ terraform init
</code></pre><p>Make the Terraform plan and store output to file.</p><pre><code class="language-bash">$ terraform plan --out=terraform.tfplan
</code></pre><p>Convert the plan to JSON</p><pre><code class="language-bash">$ terraform show -json ./terraform.tfplan &gt; ./terraform.tfplan.json
</code></pre><h3>Validate the non-compliant Terraform plan against your Constraints, for example</h3><pre><code class="language-bash">$ ./terraform-validator-linux-amd64 validate ./tfplan.tfplan.json --policy-path=../repos/policy-library/
</code></pre><p>TA-DA!</p><pre><code>Found Violations:

Constraint allow_some_storage_location on resource //storage.googleapis.com/tf-validator-demo-bucket: //storage.googleapis.com/tf-validator-demo-bucket is in a disallowed location.
</code></pre><h3>Validate the compliant Terraform plan against your Constraints</h3><p>Let’s see what happens if we repeat the above, changing the location of our GCS bucket to <code>australia-southeast1</code>.</p><pre><code class="language-bash">$ ./terraform-validator-linux-amd64 validate ./tfplan.tfplan.json --policy-path=../repos/policy-library/
</code></pre><p>Results in..</p><pre><code>No violations found.
</code></pre><p>Success!!!</p><p>Now all that’s left to do for your Policy as Code CICD pipeline is to configure the rest of your Constraints and run this check before you go ahead and <code>terraform apply</code>. Be sure to make the <code>apply</code> step dependent on the outcome of the Validator.</p><h2>Wrap Up</h2><p>We’ve looked at how to apply Policy as Code to validate our Infrastructure as Code. Sounds pretty modern and DevOpsy doesn’t it.</p><p>To recap, we learned about Constraints, which are fully defined instances of Policy as Code. They’re based on YAML Templates that refer to the OPA policy language Rego, but we didn’t have to learn it :)</p><p>We created our own version controlled Policy Library.</p><p>Using the above learning and some handy pre-existing samples, we wrote policies (Constraints) for GCP infrastructure, specifying a whitelist for locations in which GCS buckets could be deployed.</p><p>As mentioned there are <a href="https://github.com/forseti-security/policy-library/tree/master/samples">dozens upon dozens of samples</a> across BigQuery, IAM, networks, MySQL, Google Kubernetes Engine (GKE) and more to work with.</p><p>Of course, we stored these configured Constraints in our version-controlled Policy Library.</p><ul><li>We looked at a simple set of Terraform code to define a GCS bucket, and stored the Terraform plan to a file before applying it.</li><li>We ran Forseti’s Terraform Validator against the Terraform plan file, and had the Validator compare the plan to our Policy Library.</li><li>We saw that the results matched our expectations! Compliance with the location specified in our Constraint passed the Validator’s checks, and non-compliance triggered a violation.</li></ul><p>Awesome. And the best part is that all this required no special permissions, no infrastructure for servers or agents and no networking.</p><p>All of that comes with the full Forseti suite of Inventory taking Config Validation of already deployed resources. We might get to that next time.</p><p>References:</p><p><a href="https://github.com/GoogleCloudPlatform/terraform-validator">https://github.com/GoogleCloudPlatform/terraform-validator</a> <a href="https://github.com/forseti-security/policy-library">https://github.com/forseti-security/policy-library</a> <a href="https://www.openpolicyagent.org/docs/latest/policy-language/">https://www.openpolicyagent.org/docs/latest/policy-language/</a> <a href="https://cloud.google.com/blog/products/identity-security/using-forseti-config-validator-with-terraform-validator">https://cloud.google.com/blog/products/identity-security/using-forseti-config-validator-with-terraform-validator</a> <a href="https://forsetisecurity.org/docs/latest/concepts/">https://forsetisecurity.org/docs/latest/concepts/</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creating a Site to Site VPN Connection Between GCP and Azure with Google Private Access]]></title>
            <link>https://cloudywithachanceofbigdata.com/creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access</link>
            <guid>creating-a-site-to-site-vpn-connection-between-gcp-and-azure-with-google-private-access</guid>
            <pubDate>Fri, 27 Mar 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This article demonstrates creating a site to site IPSEC VPN connection between a GCP VPC network and an Azure Virtual Network, enabling private RFC1918 network connectivity between virtual networks in both clouds. This is done using a single PowerShell script leveraging Azure PowerShell and gcloud commands in the Google SDK.]]></description>
        </item>
        <item>
            <title><![CDATA[Spark in the Google Cloud Platform Part 2]]></title>
            <link>https://cloudywithachanceofbigdata.com/spark-in-the-google-cloud-platform-part-2</link>
            <guid>spark-in-the-google-cloud-platform-part-2</guid>
            <pubDate>Sat, 29 Feb 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[In the previous post in this series Spark in the Google Cloud Platform Part 1, we started to explore the various ways in which we could deploy Apache Spark applications in GCP. The first option we looked at was deploying Spark using Cloud DataProc, a managed Hadoop cluster with various ecosystem components included.]]></description>
            <content:encoded><![CDATA[<p>In the previous post in this series <a href="https://cloudywithachanceofbigdata.com/spark-in-the-google-cloud-platform-part-1/"><strong>Spark in the Google Cloud Platform Part 1</strong></a>, we started to explore the various ways in which we could deploy Apache Spark applications in GCP. The first option we looked at was deploying Spark using Cloud DataProc, a managed Hadoop cluster with various ecosystem components included.</p><p>:::note Spark Training Courses</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/">Data Transformation and Analysis Using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/">Stream and Event Processing using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/">Advanced Analytics Using Apache Spark</a></p><p>:::</p><p>In this post, we will look at another option for deploying Spark in GCP – <em>a Spark Standalone cluster running on GKE</em>.</p><p>Spark Standalone refers to the in-built cluster manager provided with each Spark release. Standalone can be a bit of a misnomer as it sounds like a single instance – which it is not, standalone simply refers to the fact that it is not dependent upon any other projects or components – such as Apache YARN, Mesos, etc.</p><p>A Spark Standalone cluster consists of a Master node or instance and one of more Worker nodes. The Master node serves as both a master and a cluster manager in the Spark runtime architecture.</p><p>The Master process is responsible for marshalling resource requests on behalf of applications and monitoring cluster resources.</p><p>The Worker nodes host one or many Executor instances which are responsible for carrying out tasks.</p><p>Deploying a Spark Standalone cluster on GKE is reasonably straightforward. In the example provided in this post we will set up a private network (VPC), create a GKE cluster, and deploy a Spark Master pod and two Spark Worker pods (in a real scenario you would typically have many Worker pods).</p><p>Once the network and GKE cluster have been deployed, the first step is to create Docker images for both the Master and Workers.</p><p>The <code>Dockerfile</code> below can be used to create an image capable or running either the Worker or Master daemons:</p><div id="a2828409021205b3f6587c824c59928d"></div><p>Note the shell scripts included in the <code>Dockerfile</code>: <code>spark-master</code> and <code>spark-worker</code>. These will be used later on by K8S deployments to start the relative Master and Worker daemon processes in each of the pods.</p><p>Next, we will use Cloud Build to build an image using the <code>Dockerfile</code> are store this in GCR (Google Container Registry), from the Cloud Build directory in our project we will run:</p><pre><code>gcloud builds submit --tag gcr.io/spark-demo-266309/spark-standalone
</code></pre><p>Next, we will create Kubernetes deployments for our Master and Worker pods.</p><p>Firstly, we need to get cluster credentials for our GKE cluster named ‘spark-cluster’:</p><pre><code>gcloud container clusters get-credentials spark-cluster --zone australia-southeast1-a --project spark-demo-266309
</code></pre><p>Now from within the <code>k8s-deployments\deploy</code> folder of our project we will use the <code>kubectl</code> command to deploy the Master pod, service and the Worker pods</p><p>Starting with the Master deployment, this will deploy our Spark Standalone image into a container running the Master daemon process:</p><div id="31bca11627167e0cd963103e4c7f11d2"></div><p>To deploy the Master, run the following:</p><pre><code>kubectl create -f spark-master-deployment.yaml
</code></pre><p>The Master will expose a web UI on port 8080 and an RPC service on port 7077, we will need to deploy a K8S service for this, the YAML required to do this is shown here:</p><div id="a72d3c38d7a3f94e88c7affd28a3034b"></div><p>To deploy the Master service, run the following:</p><pre><code>kubectl create -f spark-master-service.yaml
</code></pre><p>Now that we have a Master pod and service up and running, we need to deploy our Workers which are preconfigured to communicate with the Master service.</p><p>The YAML required to deploy the two Worker pods is shown here:</p><div id="97ceb93ed35959c41d80fb8c025a7ba1"></div><p>To deploy the Worker pods, run the following:</p><pre><code>kubectl create -f spark-worker-deployment.yaml
</code></pre><p>You can now inspect the Spark processes running on your GKE cluster.</p><pre><code>kubectl get deployments
</code></pre><p>Shows...</p><pre><code>NAME           READY   UP-TO-DATE   AVAILABLE   AGE
 spark-master   1/1     1            1           7m45s
 spark-worker   2/2     2            2           9s
</code></pre><pre><code>kubectl get pods
</code></pre><p>Shows...</p><pre><code>NAME                            READY   STATUS    RESTARTS   AGE
 spark-master-f69d7d9bc-7jgmj    1/1     Running   0          8m
 spark-worker-55965f669c-rm59p   1/1     Running   0          24s
 spark-worker-55965f669c-wsb2f   1/1     Running   0          24s
</code></pre><p>Next, as we need to expose the Web UI for the Master process we will create a <em>LoadBalancer</em> resource. The YAML used to do this is provided here:</p><div id="56ee86f50f329f99679ff243bb00fb07"></div><p>To deploy the LB, you would run the following:</p><pre><code>kubectl create -f spark-ui-lb.yaml
</code></pre><p><strong>NOTE</strong> This is just an example, for simplicity we are creating an external <em>LoadBalancer</em> with a public IP, this configuration is likely not be appropriate in most real scenarios, alternatives would include an internal <em>LoadBalancer</em>, retraction of Authorized Networks, a jump host, SSH tunnelling or IAP.</p><p>Now you’re up and running!</p><p>You can access the Master web UI from the Google Console link shown here:</p><p><a href="images/master-ui-link.png"><img src="images/master-ui-link.png" alt="Accessing the Spark Master UI from the Google Cloud Console"/></a></p><p>The Spark Master UI should look like this:</p><p><a href="images/spark-master-ui.png"><img src="images/spark-master-ui.png" alt="Spark Master UI"/></a></p><p>Next we will exec into a Worker pod, get a shell:</p><pre><code>kubectl exec -it spark-worker-55965f669c-rm59p -- sh
</code></pre><p>Now from within the shell environment of a Worker – which includes all of the Spark client libraries, we will submit a simple Spark application:</p><pre><code>spark-submit --class org.apache.spark.examples.SparkPi \
 --master spark://10.11.250.98:7077 \
/opt/spark/examples/jars/spark-examples*.jar 10000
</code></pre><p>You can see the results in the shell, as shown here:</p><p><a href="images/spark-application-example.png"><img src="images/spark-application-example.png" alt="Spark Pi Estimator Example"/></a></p><p>Additionally, as all of the container logs go to Stackdriver, you can view the application logs there as well:</p><p><a href="images/container-logs-in-stackdriver.png"><img src="images/container-logs-in-stackdriver.png" alt="Container Logs in StackDriver"/></a></p><p>This is a simple way to get a Spark cluster running, it is not without its downsides and shortcomings however, which include the limited security mechanisms available (SASL, network security, shared secrets).</p><p>In the final post in this series we will look at Spark on Kubernetes, using Kubernetes as the Spark cluster manager and interacting with Spark using the Kubernetes API and control plane, see you then.</p><blockquote><p>Full source code for this article is available at: <a href="https://github.com/gamma-data/spark-on-gcp">https://github.com/gamma-data/spark-on-gcp</a></p></blockquote><p>The infrastructure coding for this example uses Powershell and Terraform, and is deployed as follows:</p><pre><code class="language-powershell">PS &gt; .\run.ps1 private-network apply &lt;gcp-project&gt; &lt;region&gt;
PS &gt; .\run.ps1 gke apply &lt;gcp-project&gt; &lt;region&gt;
</code></pre>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Spark in the Google Cloud Platform Part 1]]></title>
            <link>https://cloudywithachanceofbigdata.com/spark-in-the-google-cloud-platform-part-1</link>
            <guid>spark-in-the-google-cloud-platform-part-1</guid>
            <pubDate>Fri, 14 Feb 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[I have been an avid Spark enthusiast since 2014 (the early days..). Spark has featured heavily in every project I have been involved with from data warehousing, ETL, feature extraction, advanced analytics to event processing and IoT applications. I like to think of it as a Swiss army knife for distributed processing.]]></description>
            <content:encoded><![CDATA[<p>I have been an avid Spark enthusiast since 2014 (the early days..). Spark has featured heavily in every project I have been involved with from data warehousing, ETL, feature extraction, advanced analytics to event processing and IoT applications. I like to think of it as a Swiss army knife for distributed processing.</p><p>:::note Spark Training Courses</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/">Data Transformation and Analysis Using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/">Stream and Event Processing using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/">Advanced Analytics Using Apache Spark</a></p><p>:::</p><p>Curiously enough, the first project I had been involved with for some years that did not feature the Apache Spark project was a green field GCP project which got me thinking… where does Spark fit into the GCP landscape?</p><p>Unlike the other major providers who use Spark as the backbone of their managed distributed ETL services with examples such as AWS Glue or the Spark integration runtime option in Azure Data Factory, Google’s managed ETL solution is Cloud DataFlow. Cloud DataFlow which is a managed Apache Beam service does not use a Spark runtime (there is a Spark Runner however this is not an option when using CDF). So where does this leave Spark?</p><p>My summation is that although Spark is not a first-class citizen in GCP (as far as managed ETL), it is not a second-class citizen either. This article will discuss the various ways Spark clusters and applications can be deployed within the GCP ecosystem.</p><h2>Quick Primer on Spark</h2><p>Every Spark application contains several components regardless of deployment mode, the components in the Spark runtime architecture are:</p><ul><li>the Driver</li><li>the Master</li><li>the Cluster Manager</li><li>the Executor(s), which run on worker nodes or Workers</li></ul><p>Each component has a specific role in executing a Spark program and all of the Spark components run in Java virtual machines (JVMs).</p><p><a href="images/spark-runtime.png"><img src="images/spark-runtime.png" alt="Spark Runtime Architecture"/></a></p><p>Cluster Managers schedule and manage distributed resources (compute and memory) across the nodes of the cluster. Cluster Managers available for Spark include:</p><ul><li>Standalone</li><li>YARN (Hadoop)</li><li>Mesos</li><li>Kubernetes</li></ul><h2>Spark on DataProc</h2><p>This is perhaps the simplest and most integrated approach to using Spark in the GCP ecosystem.</p><p>DataProc is GCP’s managed Hadoop Service (akin to AWS EMR or HDInsight on Azure). DataProc uses Hadoop/YARN as the Cluster Manager. DataProc clusters can be deployed on a private network (VPC using RFC1918 address space), supports encryption at Rest using Google Managed or Customer Managed Keys in KMS, supports autoscaling and the use of Preemptible Workers, and can be deployed in a HA config.</p><p>Furthermore, DataProc clusters can enforce strong authentication using Kerberos which can be integrated into other directory services such as Active Directory through the use of cross realm trusts.</p><h3>Deployment</h3><p>DataProc clusters can be deployed using the <code>gcloud dataproc clusters create</code> command or using IaC solutions such as Terraform. For this article I have included an example in the source code using the <code>gcloud</code> command to deploy a DataProc cluster on a private network which was created using Terraform.</p><h3>Integration</h3><p>The beauty of DataProc is its native integration into IAM and the GCP service plane. Having been a long-time user of AWS EMR, I have found that the usability and integration are in many ways superior in GCP DataProc. Let’s look at some examples...</p><h4>IAM and IAP (TCP Forwarding)</h4><p>DataProc is integrated into Cloud IAM using various coarse grained permissions use as <code>dataproc.clusters.use</code> and simplified IAM Roles such as <code>dataproc.editor</code> or <code>dataproc.admin</code>. Members with bindings to the these roles can perform tasks such as submitting jobs and creating workflow templates (which we will discuss shortly), as well as accessing instances such as the master node instance or instances in the cluster using IAP (TCP Forwarding) without requiring a public IP address or a bastion host.</p><h4>DataProc Jobs and Workflows</h4><p>Spark jobs can be submitted using the console or via <code>gcloud dataproc jobs submit</code> as shown here:</p><p><a href="images/dataproc-spark-job.png"><img src="images/dataproc-spark-job.png" alt="Submitting a Spark Job using gcloud dataproc jobs submit"/></a></p><p>Cluster logs are natively available in StackDriver and standard out from the Spark Driver is visible from the console as well as via <code>gcloud</code> commands.</p><p>Complex Workflows can be created by adding Jobs as Steps in Workflow Templates using the following command:</p><pre><code>gcloud dataproc workflow-templates add-job spark
</code></pre><h4>Optional Components and the Component Gateway</h4><p>DataProc provides you with a Hadoop cluster including YARN and HDFS, a Spark runtine – which includes Spark SQL and SparkR. DataProc also supports several optional components including Anaconda, Jupyter, Zeppelin, Druid, Presto, and more.</p><p>Web interfaces to some of these components as well as the management interfaces such as the Resource Manager UI or the Spark History Server UI can be accessed through the Component Gateway.</p><p>This is a Cloud IAM integrated gateway (much like IAP) which can allow access through an authenticated and authorized console session to web UIs in the cluster – without the need for SSH tunnels, additional firewall rules, bastion hosts, or public IPs. Very cool.</p><p>Links to the component UIs as well as built in UIs like the YARN Resource Manager UI are available directly from through the console.</p><h4>Jupyter</h4><p>Jupyter is a popular notebook application in the data science and analytics communities used for reproducible research. DataProc’s Jupyter component provides a ready-made Spark application vector using PySpark. If you have also installed the Anaconda component you will have access to the full complement of scientific and mathematic Python packages such as Pandas and NumPy which can be used in Jupyter notebooks as well. Using the Component Gateway, Jupyer notebooks can be accessed directly from the Google console as shown here:</p><p><a href="images/dataproc-jupyter-notebook.png"><img src="images/dataproc-jupyter-notebook.png" alt="Jupyter Notebooks using DataProc"/></a></p><p>From this example you can see that I accessed source data from a GCS bucket and used HDFS as local scratch space.</p><p>Furthermore, notebooks are automagically saved in your integrated Cloud Storage DataProc staging bucket and can be shared amongst analysts or accessed at a later time. These notebooks also persist beyond the lifespan of the cluster.</p><p>Next up we will look at deploying a Spark Standalone cluster on a GKE cluster, see you then!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Query Cloud SQL through Big Query]]></title>
            <link>https://cloudywithachanceofbigdata.com/query-cloud-sql-through-big-query</link>
            <guid>query-cloud-sql-through-big-query</guid>
            <pubDate>Sat, 08 Feb 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This article demonstrates Cloud SQL federated queries for Big Query, a neat and simple to use feature.]]></description>
            <content:encoded><![CDATA[<p>This article demonstrates Cloud SQL federated queries for Big Query, a neat and simple to use feature.</p><h2>Connecting to Cloud SQL</h2><p>One of the challenges presented when using Cloud SQL on a private network (VPC) is providing access to users. There are several ways to accomplish this which include:</p><ul><li>open the database port on the VPC Firewall (5432 for example for Postgres) and let users access the database using a command line or locally installed GUI tool <em>(may not be allowed in your environment)</em></li><li>provide a web based interface deployed on your VPC such as PGAdmin deployed on a GCE instance or GKE pod <em>(adds security and management overhead)</em></li><li>use the Cloud SQL proxy <em>(requires additional software to be installed and configured)</em></li></ul><p>In additional, all of the above solutions require direct IP connectivity to the instance which may not always be available. Furthermore each of these operations requires the user to present some form of authentication – in many cases the database user and password which then must be managed at an individual level.</p><p>Enter Cloud SQL federated queries for Big Query…</p><h2>Big Query Federated Queries for Cloud SQL</h2><p>Big Query allows you to query tables and views in Cloud SQL (currently MySQL and Postgres) using the Federated Queries feature. The queries could be authorized views in Big Query datasets for example.</p><p>This has the following advantages:</p><ul><li>Allows users to authenticate and use the GCP console to query Cloud SQL</li><li>Does not require direct IP connectivity to the user or additional routes or firewall rules</li><li>Leverages Cloud IAM as the authorization mechanism – rather that unmanaged db user accounts and object level permissions</li><li>External queries can be executed against a read replica of the Cloud SQL instance to offload query IO from the master instance</li></ul><h2>Setting it up</h2><p>Setting up big query federated queries for Cloud SQL is exceptionally straightforward, a summary of the steps are provided below:</p><h3>Step 1. Enable a Public IP on the Cloud SQL instance</h3><p>This sounds bad, but it isn’t really that bad. You need to enable a public interface for Big Query to be able to establish a connection to Cloud SQL, however this is not accessed through the actual public internet – rather it is accessed through the Google network using the back end of the front end if you will.</p><p>Furthermore, you configure an empty list of authorized networks which effectively shields the instance from the public network, this can be configured in Terraform as shown here:</p><div id="81c57a80a7e588b98ea7d294dbaee242"></div><p>This configuration change can be made to a running instance as well as during the initial provisioning of the instance.</p><p>As shown below you will get a warning dialog in the console saying that you have no authorized networks - this is by design.</p><p><a href="images/cloud-sql-publicip-screenshot.png"><img src="images/cloud-sql-publicip-screenshot.png" alt="Cloud SQL Public IP Enabled with No Authorized Networks"/></a></p><h3>Step 2. Create a Big Query dataset which will be used to execute the queries to Cloud SQL</h3><p>Connections to Cloud SQL are defined in a Big Query dataset, this can also be use to control access to Cloud SQL using authorized views controlled by IAM roles.</p><div id="8a4beaab134a1c72613347b5822d1724"></div><h3>Step 3. Create a connection to Cloud SQL</h3><p>To create a connection to Cloud SQL from Big Query you must first enable the BigQuery Connection API, this is done at a project level.</p><p>As this is a fairly recent feature there isn&#x27;t great coverage with either the <strong><code>bq</code></strong> tool or any of the Big Query client libraries to do this so we will need to use the console for now...</p><p>Under the <em><strong>Resources</strong></em> -&gt; <strong><em>Add Data</em></strong> link in the left hand panel of the Big Query console UI, select <strong><em>Create Connection</em></strong>. You will see a side info panel with a form to enter connection details for your Cloud SQL instance.</p><p>In this example I will setup a connection to a Cloud SQL read replica instance I have created:</p><p><a href="images/big-query-add-connection.png"><img src="images/big-query-add-connection.png"/></a></p><p>Creating a Big Query Connection to Cloud SQL</p><p>More information on the Big Query Connections API can be found at: <a href="https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest">https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest</a></p><p>The following permissions are associated with connections in Big Query:</p><pre><code>bigquery.connections.create  
bigquery.connections.get  
bigquery.connections.list  
bigquery.connections.use  
bigquery.connections.update  
bigquery.connections.delete
</code></pre><p>These permissions are conveniently combined into the following predefined roles:</p><pre><code>roles/bigquery.connectionAdmin    (BigQuery Connection Admin)         
roles/bigquery.connectionUser     (BigQuery Connection User)          
</code></pre><h3>Step 4. Query away!</h3><p>Now the connection to Cloud SQL can be accessed using the <strong><code>EXTERNAL_QUERY</code></strong> function in Big Query, as shown here:</p><p><a href="images/cloud-sql-federated-queries-screenshot.png"><img src="images/cloud-sql-federated-queries-screenshot.png" alt="Querying Cloud SQL from Big Query"/></a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud SQL – Availability for PostgreSQL – Part II (Read Replicas)]]></title>
            <link>https://cloudywithachanceofbigdata.com/google-cloud-sql-availability-for-postgresql-read-replicas</link>
            <guid>google-cloud-sql-availability-for-postgresql-read-replicas</guid>
            <pubDate>Fri, 24 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[In this post we will look at read replicas as an additional method to achieve multi zone availability for Cloud SQL, which gives us - in turn - the ability to offload (potentially expensive) IO operations such as user created backups or read operations without adding load to the master instance.]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing Service Mesh Part II]]></title>
            <link>https://cloudywithachanceofbigdata.com/introducing-service-mesh-part-ii</link>
            <guid>introducing-service-mesh-part-ii</guid>
            <pubDate>Tue, 21 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a follow up to the previous post:]]></description>
            <content:encoded><![CDATA[<p>This is a follow up to the previous post:</p><p><a href="https://cloudywithachanceofbigdata.com/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know/"><strong>Sick of hearing about Service Mesh? Here’s what you need to know...</strong></a></p><h2>Refresher</h2><p>A refresher on the data plane, and what the userspace proxy can perform:</p><ul><li><strong>Routing:</strong> Given a REST request for <code>/hello</code> from the local service instance, where should that request be sent?</li><li><strong>Load Balancing:</strong> Once routing has done its job, to which upstream service instance should the request be sent? With what timeout? If the request fails, should it be retried?</li><li><strong>Authorisation and Authentication:</strong> For requests that are incoming, can cryptographic functions determine the authenticity of that requests? Is the called allowed to invoke the requested endpoint?</li><li><strong>Observability:</strong> Detailed logging, statistics, distributed tracing data so that operators can understand the traffic flow and debug problems as they occur</li><li><strong>Service Discovery:</strong> What backend/upstream service instances are available?</li><li><strong>Health Checking:</strong> Are upstream service instances healthy and ready to accept traffic?</li></ul><p>The control plane is slightly less complex. For the data plane to act in a coordinated fashion, the control plane gives you the machinery to make that happen. This is the magical part of the service mesh; the control plane takes a set of isolated sidecar proxies and turns them into a distributed system. The control plane in turn provides an API to allow the user to modify and inspect the behaviour of the data plane.</p><p>You can see from the diagram below the proxies are right next to the service in the same node. We usually call those &#x27;sidecar&#x27; containers.</p><p><a href="images/control-data-plane.png"><img src="images/control-data-plane.png"/></a></p><p>The diagram above gives you a high level indication of what the service mesh would look like. What if I don&#x27;t have many services? Then the service mesh probably isn&#x27;t for you. That&#x27;s a whole lot of machinery to run a single proxy! Having said this, if your solution is running hundreds or thousands of services, then you&#x27;re going to require a whole heap of proxies.</p><p>So there you have it. The service mesh with its control and data plane. To put it simply, the goal of the control plane is to monitor and set a policy that will eventually be enacted by the data plane.</p><h2>Why?</h2><p>You&#x27;ve taken over a project, and the security team have mandated the use of the service mesh. You&#x27;ve never used it yourself before, and the confusion as to why we need another thing is getting you down. An additional thing next to my container that will add latency? And consume resources? And I have to maintain it?! Why would anyone need or want this?</p><p>While there are a few answers to this, the most important answer is something I alluded to in an example in part 1 of this series: this design is a great way to add additional logic into the system. Not only can you add additional logic (to containers possibly outside of your control) but you can do this uniformly across the entire mesh! <em>The service mesh gives you features that are critical for running software that&#x27;s uniform across your whole stack</em></p><p>The set of features that the service mesh can provide include reliability features (Retries, timeouts etc), observability features (latencies, volume etc) and security features (mTLS, access control etc).</p><h2>Let&#x27;s break it down</h2><p><strong>Server-side software relies on these critical features</strong> If you&#x27;re building any type of modern server-side software that&#x27;s predicated on multiple services, think API&#x27;s and web-apps, and if you&#x27;re continually adding features to this in a short timeframe, then all the features listed above become critical for you. Your applications must be reliable, observable and most importantly secure. This is exactly what the service mesh helps you with.</p><p><strong>One view to rule them all</strong> The features mentioned above are language-agnostic, don&#x27;t care about your framework, who wrote it or any part of your development life cycle. They give you, your team and your company a consistent way to deploy changes across your service landscape</p><p><strong>Decoupled from application code</strong> It&#x27;s important to have a single place to include application and business logic, and not have the nightmare of managing that in multiple components of your system. The core stewardship of the functionality that the service mesh provides lies at the <em>platform level</em>. This includes maintenance, deployments, operation etc. The application can be updated and deployed by developers maintaining the application, and the service mesh can change without the application being involved.</p><h2>In short</h2><p>Yes, while the features of the service mesh could be implemented as application code, this solution would not help in driving uniform features sets across the whole system, which is the value proposition for the service mesh.</p><p><em>If you&#x27;re a business-logic developer</em>, you probably don&#x27;t need to worry about the service mesh. Keep pumping out that new fangled business logic that makes the software oh-so-usable</p><p><em>If you&#x27;re in a platform role</em> and most likely using <em>Kubernetes</em>, then you should be right on top of the service mesh! That is unless your architecture dictates a monolith. You&#x27;re going to have a lot of services talking to one another, all tied together with an overarching dependency.</p><p><em>If you&#x27;re in a platform role with no Kubernetes</em> but a bunch of microservices, you should maybe care a little bit about the service mesh, but without the power of Kubernetes and the ease of deployment it brings, you&#x27;ll have to weigh up how you intend to manage all those proxies.</p><p>I hope you enjoyed this article, please feel free to reach out to me at:</p><p>Tom Klimovski<br/>
<!-- -->Principal Consultant, Gamma Data<br/>
<a href="mailto:tom.klimovski@gammadata.io">tom.klimovski@gammadata.io</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud SQL – Availability, Replication, Failover for PostgreSQL – Part I]]></title>
            <link>https://cloudywithachanceofbigdata.com/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql</link>
            <guid>google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql</guid>
            <pubDate>Fri, 17 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[In this multi part blog we will explore the features available in Google Cloud SQL for High Availability, Backup and Recovery, Replication and Failover and Security (at rest and in transit) for the PostgreSQL DBMS engine. Some of these features are relatively hot of the press and in Beta – which still makes them available for general use.]]></description>
            <content:encoded><![CDATA[<p>In this multi part blog we will explore the features available in Google Cloud SQL for High Availability, Backup and Recovery, Replication and Failover and Security (at rest and in transit) for the PostgreSQL DBMS engine. Some of these features are relatively hot of the press and in Beta – which still makes them available for general use.</p><p>We will start by looking at the High Availability (HA) options available to you when using the PostgreSQL engine in Google Cloud SQL.</p><p>Most of you would be familiar with the concepts of High Availability, Redundancy, Fault Tolerance, etc but let’s start with a definition of HA anyway:</p><blockquote><p>High availability (HA) is a characteristic of a system, which aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.</p><p>Wikipedia</p></blockquote><p>Higher than a normal period is quite subjective, typically this is quantified by a percentage represented by a number of “9s”, that is 99.99% (which would be quoted as “four nines”), this would allot you 52.60 minutes of downtime over a one-year period.</p><p>Essentially the number of 9’s required will drive your bias towards the options available to you for Cloud SQL HA.</p><p>We will start with Cloud SQL HA in its simplest form, Regional Availability.</p><h2>Regional Availability</h2><p>Knowing what we know about the Google Cloud Platform, regional availability means that our application or service (in this case Cloud SQL) should be resilient to a failure of any one zone in our region. In fact, as all GCP regions have at least 3 zones – two zones could fail, and our application would still be available.</p><p>Regional availability for Cloud SQL (which Google refers to as High Availability), creates a standby instance in addition to the primary instance and uses a regional Persistent Disk resource to store the database instance data, transaction log and other state files, which is synchronously replicated to a Persistent Disk resource local to the zones that the primary and standby instances are located in.</p><p>A shared IP address (like a Virtual IP) is used to serve traffic to the healthy (normally primary) Cloud SQL instance.</p><p>An overview of Cloud SQL HA is shown here:</p><p><a href="images/cloud-sql-ha.png"><img src="images/cloud-sql-ha.png" alt="Cloud SQL High Availability"/></a></p><h2>Implementing High Availability for Cloud SQL</h2><p>Implementing Regional Availability for Cloud SQL is dead simple, it is one argument:</p><pre><code>availability_type = &quot;REGIONAL&quot;
</code></pre><p>Using the <code>gcloud</code> command line utility, this would be:</p><pre><code>gcloud sql instances create postgresql-instance-1234 \
  --availability-type=REGIONAL \
  --database-version= POSTGRES_9_6
</code></pre><p>Using Terraform (with a complete set of options) it would look like:</p><pre><code>resource &quot;google_sql_database_instance&quot; &quot;postgres_ha&quot; {
  provider = google-beta
  region = var.region
  project = var.project
  name = &quot;postgresql-instance-${random_id.instance_suffix.hex}&quot;
  database_version = &quot;POSTGRES_9_6&quot;
  settings {
   tier = var.tier
   disk_size = var.disk_size
   activation_policy = &quot;ALWAYS&quot;
   disk_autoresize = true
   disk_type = &quot;PD_SSD&quot;
   **availability_type = &quot;REGIONAL&quot;**
   backup_configuration {
     enabled = true
     start_time = &quot;00:00&quot;
   }
   ip_configuration  {
     ipv4_enabled = false
     private_network = google_compute_network.private_network.self_link
   }
   maintenance_window  {
     day = 7
     hour = 0
     update_track = &quot;stable&quot;
   }
  }
 } 
</code></pre><p>Once deployed you will notice a few different items in the console, first from the instance overview page you can see that the High Availability option is <code>ENABLED</code> for your instance.</p><p><a href="images/cloud-sql-ha-1.png"><img src="images/cloud-sql-ha-1.png"/></a></p><p>Second, you will see a Failover button enabled on the detailed management view for this instance.</p><p><a href="images/cloud-sql-ha-2.png"><img src="images/cloud-sql-ha-2.png"/></a></p><h2>Failover</h2><p>Failovers and failbacks can be initiated manually or automatically (should the primary be unresponsive). A manual failover can be invoked by executing the command:</p><pre><code>gcloud sql instances failover postgresql-instance-1234
</code></pre><p>There is an <code>--async</code> option which will return immediately, invoking the failover operation asynchronously.</p><p>Failover can also be invoked from the Cloud Console using the Failover button shown previously. As an example I have created a connection to a regionally available Cloud SQL instance and started a command which runs a loop and prints out a counter:</p><p><a href="images/cloud-sql-ha-3.png"><img src="images/cloud-sql-ha-3.png"/></a></p><p>Now using the <code>gcloud</code> command shown earlier, I have invoked a manual failover of the Cloud SQL instance.</p><p>Once the failover is initiated, the client connection is dropped (as the server is momentarily unavailable):</p><p><a href="images/cloud-sql-ha-4.png"><img src="images/cloud-sql-ha-4.png"/></a></p><p>The connection can be immediately re-established afterwards, the state of the running query is lost - <strong><em>importantly no data is lost</em></strong> however. If your application clients had retry logic in their code and they weren&#x27;t executing a long running query, chances are no one would notice! Once reconnecting normal database activities can be resumed:</p><p><a href="images/cloud-sql-ha-5.png"><img src="images/cloud-sql-ha-5.png"/></a></p><p>A quick check of the instance logs will show that the failover event has occured:</p><p><a href="images/cloud-sql-ha-6.png"><img src="images/cloud-sql-ha-6.png"/></a></p><p>Now when you return to the instance page in the console you will see a Failback button, which indicates that your instance is being served by the standby:</p><p><a href="images/cloud-sql-ha-7.png"><img src="images/cloud-sql-ha-7.png"/></a></p><p>Note that there may be a slight delay in the availability of this option as the replica is still being synched.</p><p>It is worth noting that nothing comes for free! When you run in REGIONAL or High Availability mode - you are effectively paying double the costs as compared to running in ZONAL mode. However availability and cost have always been trade-offs against one another - you get what you pay for...</p><blockquote><p>More information can be found at: <a href="https://cloud.google.com/sql/docs/postgres/high-availability">https://cloud.google.com/sql/docs/postgres/high-availability</a></p></blockquote><p>Next up we will look at read replicas (and their ability to be promoted) as another high availability alternative in Cloud SQL.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sick of hearing about Service Mesh? Here’s what you need to know...]]></title>
            <link>https://cloudywithachanceofbigdata.com/sick-of-hearing-about-service-mesh-heres-what-you-need-to-know</link>
            <guid>sick-of-hearing-about-service-mesh-heres-what-you-need-to-know</guid>
            <pubDate>Thu, 09 Jan 2020 00:00:00 GMT</pubDate>
            <description><![CDATA[So you’ve started delivering a new project and it’s all about this “Cloud Native” or “Microservices” thing. You’re a Delivery Manager or Software Engineer at some type of company and someone has lightly peppered a meeting with a term, ‘Mesh’.]]></description>
            <content:encoded><![CDATA[<p>So you’ve started delivering a new project and it’s all about this “Cloud Native” or “Microservices” thing. You’re a Delivery Manager or Software Engineer at some type of company and someone has lightly peppered a meeting with a term, ‘Mesh’.</p><p>They possibly said event mesh. Or better yet, they mentioned a service mesh. As time went on you kept hearing more and more about the service mesh. You’ve attempted to read up about it, digested a whole bunch of new terms and still didn’t completely understand what the Mesh even does, why you would need it or why the hype train around this technology shows no sign of stopping. This article is an attempt to provide a focused guide to the service mesh, and why it is so interesting.</p><h2>Ok, so what is this thing?</h2><p>Truth be told, the service mesh is actually pretty simple. It’s built around the idea of small, repeatable bits of software, in this case userspace proxies, stuck very close to your services. This is called the <strong><em>data plane</em></strong>. In addition to the userspace proxies, you also get a bunch of management processes, which is referred to as the <strong><em>control plane</em></strong>. Simply put, the data plane (userspace proxies) intercepts all calls between services and the control plane (management processes) coordinates the wholesale behaviour of those proxies. This allows you to perform sweeping changes across your service landscape via the control planes API’s, operators and provides the capability to measure your mesh as a whole.</p><p>Before we get into the engineering of what the proxies are, let’s go with an example.</p><ul><li>The business has bought some software.</li><li>The engineers are tasked with deploying this software in their Kubernetes cluster.</li><li>The engineers first task is to containerise this application, expose its functionality to downstream applications and deploy it to the cluster in a repeatable, continuous fashion.</li><li>There’s a requirement in your organisation that says ‘I need all communications to this vendors software as TLS1.3’. Or, ‘I would like to measure all API latency from this application’.</li></ul><p>The engineer replies ‘I can’t make changes to a third party application! What do I do?’. Service mesh to the rescue.</p><p>Using a service mesh, you can deploy a proxy right next to your vendor container and in effect, abstract away the complexities of measurement and data transport mechanisms, and allow the vendor software to concentrate on it’s business logic.</p><p>This vendor container is now part of the <strong><em>service mesh</em></strong>.</p><h2>Proxies</h2><p>When we talk about proxies, we usually discuss things in OSI model terminology, that is to say Layers 1 through 7. Most of the time when it comes to proxies, you’re comparing Layer 4 to Layer 7. Here’s a quick run-down:</p><p>Layer 4 (L4) -&gt; operates with the delivery of messages with no regard to the content of the messages. They would simply forward network packets to and from the server without inspecting any part of the packets.</p><p>Layer 7 (L7) -&gt; this is a higher level, application layer. This deals with the actual content of the message. If you were routing network traffic, you could do this at L7 in a much more sophisticated way because you can now make decisions based on the packets messages within.</p><p>Why pick between L4 and L7? <em>Speed</em>.</p><p>Back to the service mesh, these userspace proxies are L7-aware TCP proxies. Think <em><strong>NGINX</strong></em> or <em><strong>haproxy</strong></em>. There are different proxies; <a href="https://linkerd.io/">Linkerd</a> is an ultralight service mesh for Kubernetes. The most popular is <a href="https://www.envoyproxy.io/">Envoy</a>, which was created by the ride-share company Lyft. Above, I also mentioned NGINX and haproxy which are also quite popular. So what differentiates NGINX proxies from the service mesh? Their <em>focus</em>. You would implement NGINX as an Ingress proxy (traffic entering your network), but when it comes to proxies that focus on traffic between services, that’s when the service mesh proxy comes in to play.</p><p>Ok, probably time for a diagram now that we’ve explained the Data Plane.</p><p><a href="images/service-mesh.png"><img src="images/service-mesh.png"/></a></p><p>Tune in for part 2 for when we discuss the Control Plane!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Ultimate AWS to GCP Thesaurus]]></title>
            <link>https://cloudywithachanceofbigdata.com/ultimate-aws-to-gcp-thesaurus</link>
            <guid>ultimate-aws-to-gcp-thesaurus</guid>
            <pubDate>Mon, 30 Dec 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[There are many posts available which map analogous services between the different cloud providers, but this post attempts to go a step further and map additional concepts, terms, and configuration options to be the definitive thesaurus for cloud practitioners familiar with AWS looking to fast track their familiarisation with GCP.]]></description>
            <content:encoded><![CDATA[<p>There are many posts available which map analogous services between the different cloud providers, but this post attempts to go a step further and map additional concepts, terms, and configuration options to be the definitive thesaurus for cloud practitioners familiar with AWS looking to fast track their familiarisation with GCP.</p><p>It should be noted that AWS and GCP are fundamentally different platforms, nowhere is this more apparent than in the way networking is implemented between the two providers, see: <a href="https://cloudywithachanceofbigdata.com/gcp-networking-for-aws-professionals/"><strong>GCP Networking for AWS Professionals</strong></a></p><p>This post is focused on the core infrastructure, networking and security services offered by the two major cloud providers, I will do a future post on higher level services such as the ML/AI offerings from the respective providers.</p><p>Furthermore this will be a living post which I will continue to update, I encourage comments from readers on additional mappings which I will incorporate into the post as well.</p><p>I have broken this down into sections based upon the layout of the AWS Console.</p><ul><li><a href="#compute"><img src="images/compute.png"/> <strong>Compute</strong></a></li><li><a href="#storage"><img src="images/storage.png"/> <strong>Storage</strong></a></li><li><a href="#database"><img src="images/database.png"/> <strong>Database</strong></a></li><li><a href="#networking"><img src="images/networking.png"/> <strong>Networking &amp; Content Delivery</strong></a></li><li><a href="#security"><img src="images/security.png"/> <strong>Security, Identity, &amp; Compliance</strong></a></li></ul><a name="compute"></a><h2><img src="images/compute.png"/> Compute</h2><table><thead><tr><th><img src="images/aws.png"/></th><th><img src="images/gcp.png"/></th></tr></thead><tbody><tr><td>EC2 (Elastic Compute Cloud)</td><td>GCE (Google Compute Engine)</td></tr><tr><td>Availability Zone</td><td>Zone</td></tr><tr><td>Instance</td><td>VM Instance</td></tr><tr><td>Instance Family</td><td>Machine Family</td></tr><tr><td>Instance Type</td><td>Machine Type</td></tr><tr><td>Amazon Machine Image (AMI)</td><td>Image</td></tr><tr><td>IAM Role (for an EC2 Instance)</td><td>Service Account</td></tr><tr><td>Security Groups</td><td>VPC Firewall Rules (ALLOW)</td></tr><tr><td>Tag</td><td>Label</td></tr><tr><td>Termination Protection</td><td>Deletion Protection</td></tr><tr><td>Reserved Instances</td><td>Committed Use Discounts</td></tr><tr><td>Capacity Reservation</td><td>Reservation</td></tr><tr><td>User Data</td><td>Startup Script</td></tr><tr><td>Spot Instances</td><td>Preemptible VMs</td></tr><tr><td>Dedicated Instances</td><td>Sole Tenancy</td></tr><tr><td>EBS Volume</td><td>Persistent Disk</td></tr><tr><td>Auto Scaling Group</td><td>Managed Instance Group</td></tr><tr><td>Launch Configuration</td><td>Instance Template</td></tr><tr><td>ELB Listener</td><td>URL Map (Load Balancer)</td></tr><tr><td>ELB Target Group</td><td>Backend/ Instance Group</td></tr><tr><td>Instance Storage (ephemeral)</td><td>Local SSDs</td></tr><tr><td>EBS Snapshots</td><td>Snapshots</td></tr><tr><td>Keypair</td><td>SSH Keys</td></tr><tr><td>Elastic IP</td><td>External IP</td></tr><tr><td>Lambda</td><td>Google Cloud Functions</td></tr><tr><td>Elastic Beanstalk</td><td>Google App Engine</td></tr><tr><td>Elastic Container Registry (ECR)</td><td>Google Container Registry (GCR)</td></tr><tr><td>Elastic Container Service (ECS)</td><td>Google Kubernetes Engine (GKE)</td></tr><tr><td>Elastic Kubernetes Service (EKS)</td><td>Google Kubernetes Engine (GKE)</td></tr><tr><td>AWS Fargate</td><td>Cloud Run</td></tr><tr><td>AWS Service Quotas</td><td>Allocation Quotas</td></tr><tr><td>Account (within an Organisation)<!-- -->†</td><td>Project</td></tr><tr><td>Region</td><td>Region</td></tr><tr><td>AWS Cloud​Formation</td><td>Cloud Deployment Manager</td></tr></tbody></table><a name="storage"></a><h2><img src="images/storage.png"/> Storage</h2><table><thead><tr><th><img src="images/aws.png"/></th><th><img src="images/gcp.png"/></th></tr></thead><tbody><tr><td>Simple Storage Service (S3)</td><td>Google Cloud Storage (GCS)</td></tr><tr><td>Standard Storage Class</td><td>Standard Storage Class</td></tr><tr><td>Infrequent Access Storage Class</td><td>Nearline Storage Class</td></tr><tr><td>Amazon Glacier</td><td>Coldline Storage Class</td></tr><tr><td>Lifecycle Policy</td><td>Retention Policy</td></tr><tr><td>Tags</td><td>Labels</td></tr><tr><td>Snowball</td><td>Transfer Appliance</td></tr><tr><td>Requester Pays</td><td>Requester Pays</td></tr><tr><td>Region</td><td>Location Type/Location</td></tr><tr><td>Object Lock</td><td>Hold</td></tr><tr><td>Vault Lock (Glacier)</td><td>Bucket Lock</td></tr><tr><td>Multi Part Upload</td><td>Parallel Composite Transfer</td></tr><tr><td>Cross-Origin Resource Sharing (CORS)</td><td>Cross-Origin Resource Sharing (CORS)</td></tr><tr><td>Static Website Hosting</td><td>Bucket Website Configuration</td></tr><tr><td>S3 Access Points</td><td>VPC Service Controls</td></tr><tr><td>Object Notifications</td><td>Pub/Sub Notifications for Cloud Storage</td></tr><tr><td>Presigned URL</td><td>Signed URL</td></tr><tr><td>Transfer Acceleration</td><td>Storage Transfer Service</td></tr><tr><td>Elastic File System (EFS)</td><td>Cloud Filestore</td></tr><tr><td>AWS DataSync</td><td>Transfer Service for on-premises data</td></tr><tr><td>ETag</td><td>ETag</td></tr><tr><td>Bucket</td><td>Bucket</td></tr><tr><td><code>aws s3</code></td><td><code>gsutil</code></td></tr></tbody></table><a name="database"></a><h2><img src="images/database.png"/> Database</h2><table><thead><tr><th><img src="images/aws.png"/></th><th><img src="images/gcp.png"/></th></tr></thead><tbody><tr><td>Relational Database Service (RDS)</td><td>Cloud SQL</td></tr><tr><td>DynamoDB</td><td>Cloud Datastore</td></tr><tr><td>ElastiCache</td><td>Cloud Memorystore</td></tr><tr><td>Table (DynamoDB)</td><td>Kind (Cloud Datastore)</td></tr><tr><td>Item (DynamoDB)</td><td>Entity (Cloud Datastore)</td></tr><tr><td>Partition Key (DynamoDB)</td><td>Key (Cloud Datastore)</td></tr><tr><td>Attributes (DynamoDB)</td><td>Properties (Cloud Datastore)</td></tr><tr><td>Local Secondary Index (DynamoDB)</td><td>Composite Index (Cloud Datastore)</td></tr><tr><td>Elastic Map Reduce (EMR)</td><td>Cloud DataProc</td></tr><tr><td>Athena</td><td>Big Query</td></tr><tr><td>AWS Glue</td><td>Cloud DataFlow</td></tr><tr><td>Glue Catalog</td><td>Data Catalog</td></tr><tr><td>Amazon Simple Notification Service (SNS)</td><td>Cloud PubSub (push subscription)</td></tr><tr><td>Amazon Kinesis</td><td>Cloud PubSub</td></tr><tr><td>Amazon Simple Queue Service (SQS)</td><td>Cloud PubSub (poll and pull mode)</td></tr></tbody></table><a name="networking"></a><h2><img src="images/networking.png"/> Networking &amp; Content Delivery</h2><table><thead><tr><th><img src="images/aws.png"/></th><th><img src="images/gcp.png"/></th></tr></thead><tbody><tr><td>Virtual Private Cloud (VPC) (Regional)</td><td>VPC Network (Global or Regional)</td></tr><tr><td>Subnet (Zonal)</td><td>Subnet (Regional)</td></tr><tr><td>Route Tables</td><td>Routes</td></tr><tr><td>Network ACLs (NACLS)</td><td>VPC Firewall Rules (ALLOW or DENY)</td></tr><tr><td>CloudFront</td><td>Cloud CDN</td></tr><tr><td>Route 53</td><td>Cloud DNS/Google Domains</td></tr><tr><td>Direct Connect</td><td>Dedicated (or Partner) Interconnect</td></tr><tr><td>Virtual Private Network (VPN)</td><td>Cloud VPN</td></tr><tr><td>AWS PrivateLink</td><td>Google Private Access</td></tr><tr><td>NAT Gateway</td><td>Cloud NAT</td></tr><tr><td>Elastic Load Balancer</td><td>Load Balancer</td></tr><tr><td>AWS WAF</td><td>Cloud Armour</td></tr><tr><td>VPC Peering Connection</td><td>VPC Network Peering</td></tr><tr><td>Amazon API Gateway</td><td>Apigee API Gateway</td></tr><tr><td>Amazon API Gateway</td><td>Cloud Endpoints</td></tr></tbody></table><a name="security"></a><h2><img src="images/security.png"/> Security, Identity, &amp; Compliance</h2><table><thead><tr><th><img src="images/aws.png"/></th><th><img src="images/gcp.png"/></th></tr></thead><tbody><tr><td>Root Account</td><td>Super Admin</td></tr><tr><td>IAM User</td><td>Member</td></tr><tr><td>IAM Policy</td><td>Role (Collection of Permissions)</td></tr><tr><td>IAM Policy Attachment</td><td>IAM Role Binding (or IAM Binding)</td></tr><tr><td>Key Management Service (KMS)</td><td>Cloud KMS</td></tr><tr><td>CloudHSM</td><td>Cloud HSM</td></tr><tr><td>Amazon Inspector (agent based)</td><td>Cloud Security Scanner (scan based)</td></tr><tr><td>AWS Security Hub</td><td>Cloud Security Command Center (SCC)</td></tr><tr><td>Secrets Manager</td><td>Secret Manager</td></tr><tr><td>Amazon Macie</td><td>Cloud Data Loss Prevention (DLP)</td></tr><tr><td>AWS WAF</td><td>Cloud Armour</td></tr><tr><td>AWS Shield</td><td>Cloud Armour</td></tr></tbody></table><p>† No direct equivalent, this is the closest equivalent</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Storage Object Notifications using Slack]]></title>
            <link>https://cloudywithachanceofbigdata.com/google-cloud-storage-object-notifications-using-slack</link>
            <guid>google-cloud-storage-object-notifications-using-slack</guid>
            <pubDate>Sat, 09 Nov 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[This article describes the steps to integrate Slack with Google Cloud Functions to get notified about object events within a specified Google Cloud Storage bucket.]]></description>
        </item>
        <item>
            <title><![CDATA[Map Reduce is Dead, Long Live Map Reduce]]></title>
            <link>https://cloudywithachanceofbigdata.com/map-reduce-is-dead-long-live-map-reduce</link>
            <guid>map-reduce-is-dead-long-live-map-reduce</guid>
            <pubDate>Sun, 01 Sep 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Firstly, this is not another Hadoop obituary, there are enough of those out there already.]]></description>
            <content:encoded><![CDATA[<p>Firstly, this is not another Hadoop obituary, there are enough of those out there already.</p><p>The generalized title of this article has been used as an expression to convey the idea that something old has been replaced by something new. In the case of the expression “the King is dead, long live the King” the inference is that although one monarch has passed, another monarch instantly succeeds him.</p><p>In the age of instant gratification and hype cycle driven ‘pump and dump’ investment we are very quick to discard technologies that don’t realise overzealous targets for sales or market share. In our continuous attempts to find the next big thing, we are quick to throw out the last big thing and everything associated with it.</p><h2>The Reports of My Death Have Been Greatly Exaggerated</h2><p>A classic example of this is the notion that Map Reduce is dead. Largely proliferated by the Hadoop obituaries which seem to be growing exponentially with each day.</p><p>A common e-myth is that Google invented the Map Reduce pattern, which is completely incorrect. In 2004, Google described a framework distributed systems implementation of the Map Reduce pattern in a white paper named <em>“MapReduce: Simplified Data Processing on Large Clusters.”</em> – this would inspire the first-generation processing framework (MapReduce) in the Hadoop project. But neither Google nor Yahoo! nor contributors to the Hadoop project (which include the pure play vendors) created the Map Reduce algorithm or processing pattern and neither shall any one of these have the rights to kill it.</p><p>The origins of the Map Reduce pattern can be traced all the way back to the early foundations of functional programming beginning with Lambda Calculus in the 1930s to LISP in the 1960s. Map Reduce is an integral pattern in all of today’s functional and distributed systems programming. You only need to look at the support for <code>map()</code> and <code>reduce()</code> operators in some of the most popular languages today including Python, JavaScript, Scala, and many more languages that support functional programming.</p><p>As far as distributed processing frameworks go, the Map Reduce pattern and its <code>map()</code> and <code>reduce()</code> methods are very prominent as higher order functions in APIs such as Spark, Kafka Streams, Apache Samza and Apache Flink to name a few.</p><p>While the initial Hadoop adaptation of Map Reduce has been supplanted by superior approaches, the Map Reduce processing pattern is far from dead.</p><h2>On the fall of Hadoop...</h2><p>There is so much hysteria around the fall of Hadoop, we need to be careful not to toss the baby out with the bath water. Hadoop served a significant role in bringing open source, distributed systems from search engine providers to academia all the way to the mainstream, and still serves an important purpose in many organizations data ecosystems today and will continue to do so for some time.</p><p>OK, it wasn’t the panacea to everything, but who said it was supposed to be? The Hadoop movement was hijacked by hysteria, hype, venture capital, over ambitious sales targets and financial engineering – this does not mean the technology was bad.</p><p>Hadoop spawned many significant related projects such as Spark, Kafka and Presto to name a few. These projects paved the way for cloud integration, which is now the dominant vector for data storage, processing, and analysis.</p><p>While the quest for world domination by the Hadoop pure play vendors may be over, the Hadoop movement (and the impact it has had on the enterprise data landscape) will live on.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ansible Tower for Continuous Infrastructure]]></title>
            <link>https://cloudywithachanceofbigdata.com/ansible-tower-for-continuous-infrastructure</link>
            <guid>ansible-tower-for-continuous-infrastructure</guid>
            <pubDate>Thu, 29 Aug 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[As infrastructure and teams scale, effective and robust configuration management requires growing beyond manual processes and local conventions. Fortunately, Ansible Tower (or the upstream Open Source project Ansible AWX) provides a perfect platform for configuration management at scale.]]></description>
            <content:encoded><![CDATA[<p>As infrastructure and teams scale, effective and robust configuration management requires growing beyond manual processes and local conventions. Fortunately, <a href="https://www.ansible.com/products/tower">Ansible Tower</a> (or the upstream Open Source project <a href="https://github.com/ansible/awx">Ansible AWX</a>) provides a perfect platform for configuration management at scale.</p><p>The <a href="https://docs.ansible.com/ansible-tower/index.html">Ansible Tower/AWX documentation</a> and tutorials provide comprehensive information about the individual components.  However, assembling all the moving pieces into a whole working solution can involve some trial and error and reverse engineering in order to understand how the components relate to one another.  Ansible Tower, like the core Ansible solution, offers flexibility in how features assembled to support different typed of workflows. The types of workflows can include once-off initial configurations, ad-hoc system maintenance, or continuous convergence.</p><p>Continuous convergence, also referred to as desired state, regularly re-applies the defined configuration to infrastructure. This tends to &#x27;correct the drift&#x27; often encountered when only applying the configuration on infrastructure setup. For example, a continuous convergence approach to configuration management could apply the desired configuration on a recurring schedule of every 30 minutes.  </p><p>Some continuous convergence workflow characteristics can include:</p><ul><li>Idempotent Ansible roles. If there are no required configuration deviations, run will report 0 changes.</li><li>A source code repository per Ansible role, similar to the Ansible Galaxy approach,</li><li>A source code repository for Ansible playbooks that include the individual Ansible roles,</li><li>A host configured to provide one unique service function only,</li><li>An Ansible playbook defined for each unique service function that gets applied to the host,</li><li>Playbooks applied to each host on a repeating schedule.</li></ul><p>One way to achieve a continuous convergence workflow combines the Ansible Tower components according to the following conceptual model.</p><p><a href="images/Ansible-AWX-Continuous-Convergence.png"><img src="images/Ansible-AWX-Continuous-Convergence.png"/></a></p><h2>The Workflow Components</h2><h3>Playbook and Role Source Code</h3><p><strong>Ansible roles</strong> contain the individual tasks, handlers, and content with a role responsible for the installation and configuration of a particular software service.</p><p><strong>Ansible playbooks</strong> configure a host for a particular service function in the environment acting as a wrapper for the individual role based configurations.  All the roles expected to be applied to a host must be defined in the playbook.</p><h3>Source Code Repositories</h3><p><strong>Role git repositor</strong>ies contain the versioned definition of a role, e.g. one git repository per individual role.  The roles are pulled into the playbooks using the git reference and tags, which pegs the role version used within a playbook.</p><p><strong>Project git repositories</strong> group the individual playbooks into single collection, e.g. one git repository per set of playbooks.  As with roles, specific versions of project repositories are also identified by version tags. </p><h3>Ansible Tower Server</h3><p>Two foundational concepts in Ansible Tower are projects and inventories. Projects provide access to playbooks and roles. Inventories provide the connection to &quot;real&quot; infrastructure.  Inventories and projects also provide authorisation scope for activities in Ansible Tower. For example, a given group can use the playbooks in Project X and apply jobs to hosts in Inventory Y.</p><p>Each <strong>Ansible Tower Project</strong> is backed by a project git repository.  Each repository contains the playbooks and included roles that can be applied by a given job.  The Project is the glue between the Ansible configuration tasks and the plays that apply the configuration.</p><p><strong>Ansible Tower Inventories</strong> are sets of hosts grouped for administration, similar to inventory sets used when applying playbooks manually.  One option is to group hosts into Inventories by environment.  For example, the hosts for development may be in one Inventory while the hosts for production may be in another Inventory.  User authorisation controls are applied at the Inventory level.</p><p><strong>Ansible Tower Inventory Groups</strong> define sub-sets of hosts within the larger Inventory.  These subsets can then be used to limit the scope of a playbook job.  One option is to group hosts within an Inventory by function.  For example, the hosts for web servers may be in one Inventory Group and the hosts for databases may be in another Inventory Group.  This enables one playbook to target one inventory group.  Inventory groups effectively provide metadata labels for hosts in the Inventory.</p><p>An <strong>Ansible Job Template</strong> determines the configuration to be applied to hosts.  The Job Template links a playbook from a project to an inventory.   The inventory scope can be optionally further limited by specifying inventory group limits.  A Job Template can be invoked either on an ad-hoc basis or via a recurring schedule.</p><p><strong>Ansible Job Schedules</strong> define the time and frequency at which the configuration specified in the Job Template is applied.  Each Job Template can be associated with one or more Job Schedules.  A schedule supports either once-off execution, for example during a defined change window, or regularly recurring execution.  A job schedule that applies the desired state configuration with a frequency of 30 minutes provides an example of a job schedule used for a continuous convergence workflow.</p><h3>&quot;Real&quot; Infrastructure</h3><p>An <strong>Ansible Job Instance</strong> defines a single invocation of an Ansible Job Template, both for scheduled and ad-hoc invocations of the job template.  Outside of Ansible Tower, the Job Instance is the equivalent of executing the <code>ansible-playbook</code> command using an inventory file.</p><p>A <strong>Host</strong> is the actual target infrastructure resources configured by the job instance, applying an ansible playbook of included roles.</p><h2>A note on Ansible Variables</h2><p>As with other features of Ansible and Ansible Tower, variables also offer flexibility in defining parameters and context when applying a configuration.  In addition to declaring and defining variables in roles and playbooks, variable definitions can also be defined in Ansible Tower job templates, inventory and inventory groups, and individual hosts.  Given the plethora of options for variable definition locations, without a set of conventions for managing variable values, debugging runtime issues with roles and playbooks can become difficult.  E.g. which value defined at which location was used when applying the role?</p><p>One example of variable definitions conventions could include:</p><ul><li>Variables shall be given default values in the role, .e.g. in the <code>../defaults/main.yml</code> file.</li><li>If the variable must have a &#x27;real&#x27; value supplied when applying the playbook, the variable shall be defined with an obvious placeholder value which will fail if not overridden.</li><li>Variables shall be described in the role <code>README.md</code> documentation</li><li>Do not apply variables at the host inventory level as host inventory can be transient.</li><li>Variables that select specific capabilities within a role shall be defined at the Ansible Tower Inventory Group.  For example, a role contains the configuration definition for both master and work nodes.  The Inventory Group variables are used to indicate which hosts must have the master configuration and applied and which must have the worker configuration applied.</li><li>Variables that define the environment context for configuration shall be defined in the Ansible Tower Job Template.</li></ul><p>Following these conventions, each of the possible variable definition options serves a particular purpose.  When an issue with variable definition does arise, the source is easily identified.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Managing Secrets in CICD Pipelines]]></title>
            <link>https://cloudywithachanceofbigdata.com/managing-secrets-in-cicd-pipelines</link>
            <guid>managing-secrets-in-cicd-pipelines</guid>
            <pubDate>Tue, 16 Jul 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Overview]]></description>
            <content:encoded><![CDATA[<h2>Overview</h2><p>With the adoption automation for deploying and managing application environments, protecting privileged accounts and credential secrets in a consistent, secure, and scalable manner becomes critical.  Secrets can include account usernames, account passwords and API tokens.  Good credentials management and secrets automation practices reduce the risk of secrets escaping into the wild and being used either intentionally (hacked) or unintentionally (accident).</p><ul><li>Reduce the likelihood of passwords slipping into source code commits and getting pushed to code repositories, especially public repositories such as github.</li><li>Minimise the secrets exposure surface area by reducing the number of people who require knowledge of credentials.  With an automated credentials management process that number can reach zero.</li><li>Limit the useful life of a secret by employing short expiry times and small time-to-live (TTL) values.  Automation enables reliable low-effort secret re-issue and rotation.</li></ul><h2>Objectives</h2><p>The following objectives have been considered in designing a secrets automation solution that can be integrated into an existing CICD environment.</p><ul><li>Integrate into an existing CICD environment without requiring an &quot;all or nothing&quot; implementation.  Allow existing jobs to operate alongside jobs that have been converted to the new secrets automation solution.</li><li>A single design that can be applied across different toolchains and deployment models.  For example, deployment to a Kubernetes environment can use the same secrets management process as an application installation on a virtual machine.  Similarly, the design can be used with different CICD tools, such as <a href="https://about.gitlab.com">GitLab-CI</a>, <a href="https://travis-ci.org">Travis-CI</a>, or other build and deploy automation tool.</li><li>Multi-cloud capable by limiting coupling to a specific hosting environment or cloud services provider.</li><li>The use of secrets (or not) can be decided at any point in time, without requiring changes to the CICD job definition, similar to the use of feature flags in applications.</li><li>Enable changes to secrets, either due to rotation or revocation, to be maintained from a central service point.  Avoid storing the same secret multiple times in different locations.</li><li>Secrets organised in predictable locations in a &quot;rest-ish&quot; fashion by treating secrets and credentials as attributes of entities.</li><li>Use environment variables as the standard interface between deployment orchestration and deployed application, following the 12 Factor App approach.</li></ul><h2>Solution</h2><ul><li>Secrets stored centrally in Hashicorp Vault.</li><li>CICD jobs retrieve secrets from Vault and configure the application deployment environment.</li><li>Deployed applications use the secrets supplied by CICD job to access backend services.</li></ul><p><a href="images/Screen-Shot-2019-07-16-at-17.03.47.png"><img src="images/Screen-Shot-2019-07-16-at-17.03.47.png" alt="CICD Secrets with Vault"/></a></p><h2>Storing Secrets</h2><p>Use <a href="https://www.vaultproject.io/">Vault by Hashicorp</a> as a centralised secrets storage service.  The CICD service retrieves secrets information for integration and deployment jobs.  Vault provides a flexible set of features to support numerous different workflows and available as either Vault Open Source or Vault Enterprise.  The secrets management pattern described uses the Vault Open Source version.  The workflow described here can be explored using Vault in the unsecured development mode, however, a properly configured and managed Vault service is required for production use.</p><p>Vault supports a number of secrets backends and access workflow models.  This solution makes use of the <a href="https://www.vaultproject.io/docs/auth/approle.html">Vault AppRole method</a>, which is designed to support machine-to-machine automated workflows.  With the AppRole workflow model human access to secrets is minimised through the use of access controls and temporary credentials with short TTL&#x27;s.  Within Vault, secrets are organised using an entity centric &quot;rest-ish&quot; style approach ensuring a given secret for a given service is stored in a single predictable location.</p><p>The use of Vault satisfies several of the design objectives:</p><ul><li>enables single point management of secrets. The secrets content is stored in a single location referenced at CICD job runtime.  On the next invocation, the CICD job retrieves the latest version of the secrets content.</li><li>enables storing secrets in predictable locations with file system directory style path location.  The &quot;rest-ish&quot; approach to organising secret locations enables storing a given secret only once.  Access policies provide the mechanism to limit CICD  visibility to only those secrets required for the CICD job.</li></ul><h2>Passing Secrets to Applications</h2><p>Use environment variables to pass secrets from the CICD service to the application environment.  </p><p>There are existing utilities available for populating a child process environment with Vault sourced secrets, such as <a href="https://github.com/channable/vaultenv">vaultenv</a> or <a href="https://github.com/hashicorp/envconsul">envconsul</a>.  This approach works well for running an application service.  However, with CICD, often there are often sets of tasks that require access to secrets information as opposed to a single command.  Using the child environment approach would require wrapping each command in a CICD job step with the env utility.  This works against the objective of introducing a secrets automation solution into existing CICD jobs without requiring substantial refactoring.  Similarly, some CICD solutions such as <a href="https://jenkins.io/">Jenkins</a> provide Vault integration plugins which pre-populate the environment with secrets content.  This meets the objective of minimal CICD job refactoring, but closely couples the solution to a particular CICD service stack, reducing portability.  </p><p>With a job script oriented CICD automation stack like GitLab-CI or Travis-CI, an alternative is to insert a job step at the beginning of a series of CICD tasks that will populated the required secret values into expected environment variables.  Subsequent tasks in the job can then execute without requiring refactoring.  The decision on whether to source a particular environment variable&#x27;s content directly from the CICD job setup or from the Vault secrets store can be made by adding an optional prefix to environment variables to be sourced from the Vault secrets store.  The prefixed instance of the environment variable contains the location or path to the required secret.  Secret locations are identified using the convention <code>/&lt;vault-secret-path&gt;/&lt;secret-key&gt;</code></p><ul><li>enables progressive implementation due to transparency of secret sourcing. Subsequent steps continue to rely on expected environment vars</li><li>enables use in any toolchain that supports use of environment variables to pass information to application environment. </li><li>CICD job steps not tied to a specific secrets store. An alternative secrets storage service could be supported by only requiring modification of the secret getter utility.</li><li>control of whether to source application environment variables from the CICD job directly or from the secrets engine is managed at the CICD job setup level as opposed to requiring CICD job refactoring to switch the content source.</li><li>continues the 12 Factor App approach of using environment variables to pass context to application environments.</li></ul><h2>Example Workflow</h2><p>An example workflow for a CICD job designed to use environment variables for configuring an application.</p><h3>Assumptions</h3><p>The following are available in the CICD environment.</p><ul><li>A job script oriented CICD automation stack that executes job tasks as a series of shell commands, such as <a href="https://about.gitlab.com">GitLab-CI</a> or <a href="https://jenkins.io/doc/book/pipeline/">Jenkins Pipelines</a>.</li><li>A secrets storage engine with a python API, such as Hashicorp Vault.</li><li>CICD execution environment includes the <code>[get-vault-secrets-by-approle](https://github.com/datwiz/cicd-secrets-in-vault/blob/master/scripts/get-vault-secrets-by-approle)</code> utility script.</li></ul><h3>Workflow Steps</h3><h3>Add a Vault secret</h3><p>Add a secret to Vault at the location <code>secret/fake-app/users/fake-users</code> with a key/value entry of <code>password=fake-password</code></p><h3>Add a Vault access policy</h3><p>Add a Vault policy for the CICD job (or set of CICD jobs) that includes &#x27;read&#x27; access to the secret.</p><pre><code># cicd-fake-app-policy 
path &quot;secret/data/fake-app/users/fake-user&quot; {
    capabilities = [&quot;read&quot;]
}

path &quot;secret/metadata/fake-app/users/fake-user&quot; {
    capabilities = [&quot;list&quot;]
}
</code></pre><h3>Add a Vault appRole</h3><p>Add a Vault appRole linked to the new policy.  This example specifies a new appRole with an secret-id TTL of 60 days and non-renewable access tokens with a TTL of 5 minutes.  The CICD job uses the access token to read secrets.</p><pre><code>vault write auth/approle/role/fake-role \
    secret_id_ttl=1440h \
    token_ttl=5m \
    token_max_ttl=5m \
    policies=cicd-fake-app-policy
</code></pre><h3>Read the Vault approle-id</h3><p>Retrieve the approle-id of the new appRole taking note of the returned approle-id.</p><pre><code>vault read auth/approle/role/fake-role
</code></pre><h3>Add a Vault appRole secret-id</h3><p>Add a secret-id for the appRole, taking note of the returned secret-id</p><pre><code>vault write -f auth/approle/role/fake-role/secret-id
</code></pre><h3>Add CICD Job Steps</h3><p>In the CICD job definition insert job steps to retrieve secrets values a set variables in the job execution environment. These are the steps to add in a gitlab-ci.yml CICD job.</p><pre><code>...
script:
- get-vault-secrets-by-approle &gt; ${VAULT_VAR_FILE}
- source ${VAULT_VAR_FILE} &amp;&amp; rm ${VAULT_VAR_FILE}
...
</code></pre><p>The helper script <code>get-vault-secrets-by-approle</code> could be executed and sourced in a single step, e.g. <code>source $(get-vault-secrets-by-approle)</code>.  However, when executed in a single statement all script output is processed by the <code>source</code> command and script error messages don&#x27;t get printed and captured in the job logs.  Splitting the read and environment var sourcing into 2 steps aids in troubleshooting.</p><h3>Add CICD job vars for Vault access</h3><p>In the CICD job configuration add Vault access environment variables.</p><pre><code>VAULT_ADDR=https://vault.example.com:8200
VAULT_ROLE_ID=db02de05-fa39-4855-059b-67221c5c2f63
VAULT_SECRET_ID=6a174c20-f6de-a53c-74d2-6018fcceff64
VAULT_VAR_FILE=/var/tmp/vault-vars.sh
</code></pre><h3>Add CICD job vars for Vault secrets</h3><p>In the CICD job configuration add environment variables for the items to be sourced from vault secrets.  The secret path follows the convention of <code>&lt;secret-mount-path&gt;/&lt;secret-path&gt;/&lt;secret-key&gt;</code></p><pre><code>V_FAKE_PASSWORD=secret/fake-app/users/fake-user/password
</code></pre><h3>Remove CICD job vars</h3><p>In the CICD job configuration remove the previously used <code>FAKE_APP_PASSWORD</code> variable.</p><h3>Execute the CICD job</h3><p>Kick off the CICD job.  Any CICD job configuration variables prefixed with &quot;<code>V_</code>&quot; results in the addition of a corresponding environment variable in the job execution environment with content sourced from Vault.</p><blockquote><p>Full source code can be found at:</p><p><a href="https://github.com/datwiz/cicd-secrets-in-vault">https://github.com/datwiz/cicd-secrets-in-vault</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Change Data Capture at Scale using Spark]]></title>
            <link>https://cloudywithachanceofbigdata.com/change-data-capture-at-scale-using-spark</link>
            <guid>change-data-capture-at-scale-using-spark</guid>
            <pubDate>Fri, 28 Jun 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Change Data Capture (CDC) is one of the most challenging processing patterns to implement at scale. I personally have had several cracks at this using various different frameworks and approaches, the most recent of which was implemented using Spark – and I think I have finally found the best approach. Even though the code examples referenced use Spark, the pattern is language agnostic – the focus is on the approach not the specific implementation (as this could be applied to any framework or runtime).]]></description>
            <content:encoded><![CDATA[<p>Change Data Capture (CDC) is one of the most challenging processing patterns to implement at scale. I personally have had several cracks at this using various different frameworks and approaches, the most recent of which was implemented using Spark – and I think I have finally found the best approach. Even though the code examples referenced use Spark, the pattern is language agnostic – the focus is on the approach not the specific implementation (as this could be applied to any framework or runtime).</p><p>:::note Spark Training Courses</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/">Data Transformation and Analysis Using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/">Stream and Event Processing using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/">Advanced Analytics Using Apache Spark</a></p><p>:::</p><p>The first challenge you are faced with, is to compare a very large dataset (representing the current state of an object) with another potentially very large dataset (representing new or incoming data). Ideally, you would like the process to be configuration driven and accommodate such things as composite primary keys, or operational columns which you would like to restrict from change detection. You may also want to implement a pattern to segregate sensitive attributes from non-sensitive attributes.</p><h2>Overview</h2><p>This pattern (and all my other recent attempts) is fundamentally based upon calculating a deterministic hash of the key and non-key attribute(s), and then using this hash as the basis for comparison. The difference between this pattern and my other attempts is in the distillation and reconstitution of data during the process, as well as breaking the pattern into discrete stages (designed to minimize the impact to other applications). This pattern can be used to process delta or full datasets.</p><p>A high-level flowchart representing the basic pattern is shown here:</p><p><a href="images/CDC.png"><img src="images/CDC.png" alt="CDC Flowchart"/></a></p><h2>The Example</h2><p>The example provided uses the <a href="https://github.com/avensolutions/synthetic-cdc-data-generator">Synthetic CDC Data Generator application</a>, configuring an incoming set with 5 uuid columns acting as a composite key, and 10 random number columns acting as non key values. The initial days payload consists of 10,000 records, the subsequent days payload consists of another 10,000 records. From the initial dataset, a <code>DELETE</code> operation was performed at the source system for 20% of records, an <code>UPDATE</code> was performed on 40% of the records and the remaining 40% of records were unchanged. In this case the 20% of records that were deleted at the source, were replaced by new <code>INSERT</code> operations creating new keys.</p><p>After creating the synthesized day 1 and day 2 datasets, the files are processed as follows:</p><p>$ spark-submit cdc.py config.yaml data/day1 2019-06-18<br/>
<!-- -->$ spark-submit cdc.py config.yaml data/day2 2019-06-19</p><p>Where <code>config.yaml</code> is the configuration for the dataset, data/day1 and data/day2 represent the different data files, and 2019-06-18 and 2019-06-19 represent a business effective date.</p><h2>The Results</h2><p>You should see the following output from running the preceding commands for day 1 and day 2 respectively:</p><h3>Day 1:</h3><div id="b75edc7825b46c12b328d78d47b4b902"></div><h3>Day 2:</h3><div id="ca92e132105fb5bb381bf9dfca562bf4"></div><p>A summary analysis of the resultant dataset should show:</p><div id="ded1f98dc4fce13c9bb3d12a51a46b94"></div><h2>Pattern Details</h2><p>Details about the pattern and its implementation follow.</p><h3>Current and Historical Datasets</h3><p>The output of each operation will yield a current dataset (that is the current stateful representation of a give object) and a historical dataset partition (capturing the net changes from the previous state in an appended partition).</p><p>This is useful, because often consumers will primarily query the latest state of an object. The change sets (or historical dataset partitions) can be used for more advanced analysis by sophisticated users.</p><h3>Type 2 SCDs (sort of)</h3><p>Two operational columns are added to each current and historical object:</p><ul><li><code>OPERATION</code> : Represents the last known operation to the record, valid values include :<ul><li><code>I</code> (<code>INSERT</code>)</li><li><code>U</code> (<code>UPDATE</code>)</li><li><code>D</code> (<code>DELETE</code> – hard <code>DELETE</code>s, applies to full datasets only)</li><li><code>X</code> (Not supplied, applies to delta processing only)</li><li><code>N</code> (No change)</li></ul></li><li><code>EFF_START_DATE</code></li></ul><p>Since data structures on most big data or cloud storage platforms are immutable, we only store the effective start date for each record, this is changed as needed with each coarse-grained operation on the current object. The effective end date is inferred by the presence of a new effective start date (or change in the <code>EFF_START_DATE</code> value for a given record).</p><h3>The Configuration</h3><p>I am using a YAML document to store the configuration for the pattern. Important attributes to include in your configuration are a list of keys and non keys and their datatype (this implementation does type casting as well). Other important attributes include the table names and file paths for the current and historical data structures.</p><p>The configuration is read at the beginning of a routine as an input along with the path of an incoming data file (a CSV file in this case) and a business effective date (which will be used as the <code>EFF_START_DATE</code> for new or updated records).</p><p>Processing is performed using the specified key and non key attributes and the output datasets (current and historical) are written to columnar storage files (parquet in this case). This is designed to make subsequent access and processing more efficient.</p><h3>The Algorithm</h3><p>I have broken the process into stages as follows:</p><h4>Stage 1 – Type Cast and Hash Incoming Data</h4><p>The first step is to create deterministic hashes of the configured key and non key values for incoming data. The hashes are calculated based upon a list of elements representing the key and non key values using the MD5 algorithm. The hashes for each record are then stored with the respective record. Furthermore, the fields are casted their target datatype as specified in the configuration. Both of these operations can be performed in a single pass of each row using a <code>map()</code> operation.</p><p>Importantly we only calculate hashes once upon arrival of new data, as the hashes are persisted for the life of the data – and the data structures are immutable – the hashes should never change or be invalidated.</p><h4>Stage 2 – Determine INSERTs</h4><p>We now compare Incoming Hashes with previously calculated hash values for the (previous day’s) current object. If no current object exists for the dataset, then it can be assumed this is a first run. In this case every record is considered as an <code>INSERT</code> with an <code>EFF_START_DATE</code> of the business effective date supplied.</p><p>If there is a current object, then the key and non key hash values (only the hash values) are read from the current object. These are then compared to the respective hashes of the incoming data (which should still be in memory).</p><p>Given the full outer join:</p><p>incoming<!-- -->_<!-- -->data(keyhash, nonkeyhash)
FULL OUTER JOIN<br/>
<!-- -->current<!-- -->_<!-- -->data(keyhash, nonkeyhash)
ON keyhash</p><p>Keys which exist in the left entity which do not exist in the right entity must be the results of an INSERT operation.</p><p>Tag these records with an operation of <code>I</code> with an <code>EFF_START_DATE</code> of the business effective date, then rejoin only these records with their full attribute payload from the incoming dataset. Finally, write out these records to the current and historical partition in <code>overwrite</code> mode.</p><h4>Stage 3 - Determine DELETEs or Missing Records</h4><p>Referring the previous full outer join operation, keys which exist in the right entity (current object) which do not appear in the left entity (incoming data) will be the result of a (hard) <code>DELETE</code> operation if you are processing full snapshots, otherwise if you are processing deltas these would be missing records (possibly because there were no changes at the source).</p><p>Tag these records as <code>D</code> or <code>X</code> respectively with an <code>EFF_START_DATE</code> of the business effective date, rejoin these records with their full attribute payload from the current dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h4>Stage 4 - Determine UPDATEs or Unchanged Records</h4><p>Again, referring to the previous full outer join, keys which exist in both the incoming and current datasets must be either the result of an <code>UPDATE</code> or they could be unchanged. To determine which case they fall under, compare the non key hashes. If the non key hashes differ, it must have been a result of an <code>UPDATE</code> operation at the source, otherwise the record would be unchanged.</p><p>Tag these records as <code>U</code> or <code>N</code> respectively with an <code>EFF_START_DATE</code> of the business effective date (in the case of an update - otherwise maintain the current <code>EFF_START_DATE</code>), rejoin these records with their full attribute payload from the incoming dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h3>Key Callouts</h3><p>A summary of the key callouts from this pattern are:</p><ul><li>Use the RDD API for iterative record operations (such as type casting and hashing)</li><li>Persist hashes with the records</li><li>Use Dataframes for <code>JOIN</code> operations</li><li>Only perform <code>JOIN</code>s with the <code>keyhash</code> and <code>nonkeyhash</code> columns – this minimizes the amount of data shuffled across the network</li><li>Write output data in columnar (Parquet) format</li><li>Break the routine into stages, covering each operation, culminating with a <code>saveAsParquet()</code> action – this may seem expensive but for large datsets it is more efficient to break down DAGs for each operation</li><li>Use caching for objects which will be reused between actions</li></ul><h4>Metastore Integration</h4><p>Although I did not include this in my example, you could easily integrate this pattern with a metastore (such as a Hive metastore or AWS Glue Catalog), by using table objects and <code>ALTER TABLE</code> statements to add historical partitions.</p><h4>Further optimisations</h4><p>If the incoming data is known to be relatively small (in the case of delta processing for instance), you could consider a broadcast join where the smaller incoming data is distributed to all of the different Executors hosting partitions from the current dataset.</p><p>Also you could add a key to the column config to configure a column to be nullable or not.</p><p>Happy CDCing!</p><blockquote><p>Full source code for this article can be found at: <a href="https://github.com/avensolutions/cdc-at-scale-using-spark">https://github.com/avensolutions/cdc-at-scale-using-spark</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Synthetic CDC Data Generator]]></title>
            <link>https://cloudywithachanceofbigdata.com/synthetic-cdc-data-generator</link>
            <guid>synthetic-cdc-data-generator</guid>
            <pubDate>Fri, 28 Jun 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[This is a simple routine to generate random data with a configurable number or records, key fields and non key fields to be used to create synthetic data for source change data capture (CDC) processing. The output includes an initial directory containing CSV files representing an initial data load, and an incremental directory containing CSV files representing incremental data.]]></description>
            <content:encoded><![CDATA[<p>This is a simple routine to generate random data with a configurable number or records, key fields and non key fields to be used to create synthetic data for source change data capture (CDC) processing. The output includes an initial directory containing CSV files representing an initial data load, and an incremental directory containing CSV files representing incremental data.</p><p>Spark Training Courses from the AlphaZetta Academy</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/">Data Transformation and Analysis Using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/">Stream and Event Processing using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/">Advanced Analytics Using Apache Spark</a></p><p>Arguments (by position) include:</p><ul><li><code>no_init_recs</code> : the number of initial records to generate</li><li><code>no_incr_recs</code> : the number of incremental records on the second run - should be <code>&gt;= no_init_recs</code></li><li><code>no_keys</code> : number of key columns in the dataset – keys are generated as UUIDs</li><li><code>no_nonkeys</code> : number of non-key columns in the dataset – nonkey values are generated as random numbers</li><li><code>pct_del</code> : percentage of initial records deleted on the second run - between 0.0 and 1.0</li><li><code>pct_upd</code> : percentage of initial records updated on the second run - between 0.0 and 1.0</li><li><code>pct_unchanged</code> : percentage of records unchanged on the second run - between 0.0 and 1.0</li><li><code>initial_output</code> : folder for initial output in CSV format</li><li><code>incremental_output</code> : folder for incremental output in CSV format</li></ul><p>NOTE : <code>pct_del</code> + <code>pct_upd</code> + <code>pct_unchanged</code> must equal 1.0</p><p>Example usage:</p><pre><code>$ spark-submit synthetic-cdc-data-generator.py 100000 100000 2 3 0.2 0.4 0.4 data/day1 data/day2
</code></pre><p>Example output from the <strong><em>day1</em></strong> run for the above configuration would look like this:</p><div id="befb034da2b4f25a1dbbc0e9b4b8eef6"></div><p>Note that this routine can be run subsequent times producing different key and non key values each time, as the keys are UUIDs and the values are random numbers.</p><p>We will use this application to generate random input data to demonstrate CDC using Spark in a subsequent post, see you soon!</p><blockquote><p>Full source code can be found at: <a href="https://github.com/avensolutions/synthetic-cdc-data-generator">https://github.com/avensolutions/synthetic-cdc-data-generator</a></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Scalable, Secure Application Load Balancing with VPC Native GKE and Istio]]></title>
            <link>https://cloudywithachanceofbigdata.com/scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio</link>
            <guid>scalable-secure-application-load-balancing-with-vpc-native-gke-and-istio</guid>
            <pubDate>Sat, 18 May 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[At the time of this writing, GCP does not have a generally available non-public facing Layer 7 load balancer. While this is sure to change in the future, this article outlines a design pattern which has been proven to provide scalable and extensible application load balancing services for multiple applications running in Kubernetes pods on GKE.]]></description>
            <content:encoded><![CDATA[<p>At the time of this writing, GCP does not have a generally available non-public facing Layer 7 load balancer. While this is sure to change in the future, this article outlines a design pattern which has been proven to provide scalable and extensible application load balancing services for multiple applications running in Kubernetes pods on GKE.</p><p>When you create a service of type LoadBalancer in GKE, Kubernetes hooks into the provider (GCP in this case) on your behalf to create a Google Load Balancer, while this may be specified as INTERNAL, there are two issues:</p><h3>Issue #1:</h3><p>The GCP load balancer created for you is a Layer 4 TCP load balancer.</p><h3>Issue #2:</h3><p>The normal behaviour is for Google to enumerate all of the node pools in your GKE cluster and “automagically” create mapping GCE instance groups for each node pool for each zone the instances are deployed in. This means the entire surface area of your cluster is exposed to the external network – which may not be optimal for internal applications on a multi tenanted cluster.</p><h3>The Solution:</h3><p>Using <a href="https://istio.io/">Istio</a> deployed on GKE along with the <a href="https://istio.io/docs/concepts/traffic-management/#ingress-and-egress">Istio Ingress Gateway</a> along with an externally created load balancer, it is possible to get scalable HTTP load balancing along with all the normal ALB goodness (stickiness, path-based routing, host-based routing, health checks, TLS offload, etc.).</p><p>An abstract depiction of this architecture is shown here:</p><p><a href="images/istio-ingress-blog.png"><img src="images/istio-ingress-blog.png" alt="Istio Ingress Design Pattern for VPC Native GKE Clusters"/></a></p><p>This can be deployed with a combination of <a href="https://www.terraform.io/">Terraform</a> and kubectl. The steps to deploy at a high level are:</p><ol><li>Create a GKE cluster with at least two node pools: ingress-nodepool and service-nodepool. Ideally create these node pools as multi-zonal for availability. You could create additional node pools for your Egress Gateway or an operations-nodepool to host Istio, etc as well.</li><li>Deploy Istio.</li><li>Deploy the Istio Ingress Gateway service on the ingress-nodepool using Service type NodePort.</li><li>Create an associated Certificate Gateway using server certificates and private keys for TLS offload.</li><li>Create a service in the service-nodepool.</li><li>Reserve an unallocated static IP address from the node network range.</li><li><a href="https://cloud.google.com/load-balancing/docs/internal/setting-up-internal">Create an internal TCP load balancer</a>:<ol><li>Specify the frontend as the IP address reserved in step 6.</li><li>Specify the backend as the managed instance groups created during the node pool creation for the ingress-nodepool (ingress-nodepool-ig-a, ingress-nodepool-ig-b, ingress-nodepool-ig-c).</li><li>Specify ports 80 and 443.</li></ol></li><li>Create a GCP Firewall Rule to allow traffic from authorized sources (network tags or CIDR ranges) to a target of the ingress-nodepool network tag.</li><li>Create a Cloud DNS A Record for your managed zone as <!-- -->*<!-- -->.namespace.zone pointing to the IP Address assigned to the load balancer frontend in step 7.1.</li><li><a href="https://cloud.google.com/load-balancing/docs/health-checks#firewall_rules">Enable Health Checks through the GCP firewall</a> to reach the ingress-nodepool network tag at a minimum – however there is no harm in allowing these to all node pools.</li></ol><p>The service should then be resolvable and routable from authorized internal networks (peered private VPCs or internal networks connected via VPN or Dedicated Interconnect) as:</p><blockquote><p>https://_service<strong>.</strong>namespace<strong>.</strong>zone<strong>/</strong>endpoint__</p></blockquote><h3>The advantages of this design pattern are...</h3><ol><li>The Ingress Gateway provides fully functional application load balancing services.</li><li>Istio provides service discovery and routing using names and namespaces.</li><li>The Ingress Gateway service and ingress gateway node pool can be scaled as required to meet demand.</li><li>The Ingress Gateway is multi zonal for greater availability</li></ol>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AWS Professional and Speciality Exam Tips]]></title>
            <link>https://cloudywithachanceofbigdata.com/aws-professional-and-speciality-exam-tips</link>
            <guid>aws-professional-and-speciality-exam-tips</guid>
            <pubDate>Thu, 04 Apr 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[One you get beyond the Associate level AWS certification exams into the Professional or Speciality track exams the degree of difficulty rises significantly. As a veteran of the Certified Solutions Architect Professional and Big Data Specialty exams, I thought I would share my experiences which I believe are applicable to all the certification streams and tracks in the AWS certification program.]]></description>
            <content:encoded><![CDATA[<p>One you get beyond the Associate level AWS certification exams into the Professional or Speciality track exams the degree of difficulty rises significantly. As a veteran of the Certified Solutions Architect Professional and Big Data Specialty exams, I thought I would share my experiences which I believe are applicable to all the certification streams and tracks in the AWS certification program.</p><p>First off let me say that I am a self-professed certification addict, having sat more than thirty technical certification exams over my thirty plus year career in technology including certification and re-certification exams. I would put the AWS professional and specialty exams right up there in terms of their level of difficulty.</p><p>The AWS Professional and Specialty exams are specifically designed to be challenging. Although they have removed the pre-requisites for these exams (much to my dismay…), you really need to be prepared for these exams otherwise you are throwing your hard-earned money away.  </p><p>There are very few - if any - “easy” questions. All of the questions are scenario based and require you to design a solution to meet multiple requirements. The question and/or the correct answer will invariably involve the use of multiple AWS services (not just one). You will be tested on your reading comprehension, time management and ability to cope under pressure as well as being tested on your knowledge of the AWS platform.  </p><p>The following sections provide some general tips which will help you approach the exam and give you the best chance of success on the day. This is not a brain dump or a substitute for the hard work and dedication required to ensure success on your exam day.</p><h2>Time Management</h2><p>Needless to say, your ability to manage time is critical, on average you will have approximately 2-3 minutes to answer each question. Reading the questions and answers carefully may take up 1-2 minutes on its own. If the answer is not apparent to you, you are best to mark the question and come back to it at the end of the exam.  </p><p>In many cases there may be subsequent questions and answer sets which jog your memory or help you deduce the correct answers to the questions you initial passed on. For instance, you may see references in future questions which put context around services you may not be completely familiar with, this may enable you to answer flagged questions with more confidence.  </p><p>Of course, you must answer all questions before completing the exam, there are no points for incomplete or unattempted answers.</p><h2>Recommended Approach to each Question</h2><p>Most of the questions on the Professional or Specialty certification exams fall into one of three categories:</p><ul><li>Short-ish question, multiple long detailed answer options</li><li>Long-ish scenario question, multiple relatively short answer options</li><li>Long-ish question with multiple relatively long, detailed answers</li></ul><p>The latter scenario is thankfully less common. However, in all cases it is important to read the last sentence in the question first, this will provide indicators to help you read through the question in its entirety and all of the possible answers with a clear understanding of what is <em>“really”</em> being asked. For instance, the operative phrase may be <em>“highly available”</em> or <em>“most cost effective”</em>.</p><p>Try to eliminate answers based on what you know, for instance answers with erroneous instance families can be eliminated immediately. This will give you a much better statistical chance of success, even if you have to venture an educated guess in the end.</p><h2>The Most Complicated Solution is Probably Not the Correct One</h2><p>In many answer sets to questions on the Professional or Specialty exams you will see some ridiculously complicated solution approaches, these are most often incorrect answers. Although there may be enough loosely relevant terminology or services to appear reasonable.</p><p>Note the following statement direct from the AWS Certified Solutions Architect Professional Exam Blueprint:</p><blockquote><p>“Distractors, or incorrect answers, are response options that an examinee with incomplete knowledge or skill would likely choose. However, they are generally plausible responses that fit in the content area defined by the test objective.”</p></blockquote><p>AWS wants professionals who design and implement solutions which are simple, sustainable, highly available, scalable and cost effective. One of the key Amazon Leadership Principles is <em>“Invent and Simplify”</em>, simplify is often the operative word.</p><h2>Don’t spend time on dumps or practice exams (other than those from AWS)</h2><p>The question pools for AWS exams are enormous, the chances of you getting the same questions and answer sets as someone else are slim. Furthermore, non-AWS sources may not be trustworthy. There is no substitute to AWS white papers, how to’s, and real-life application of your learnings.</p><h2>Don’t focus on Service Limits or Calculations</h2><p>In my experiences with AWS exams, they are not overly concerned with service limits, default values, formulas (e.g. the formula to calculate required partitions for a DynamoDB table) or syntax - so don’t waste time remembering them. You should however understand the 7 layer OSI model and be able to read and interpret CIDR notation.</p><p>Mainly, however, they want you to understand how services work together in an AWS solution to achieve an outcome for a customer.</p><h2>Some Final Words of Advice</h2><p><strong>Always do what you think AWS would want you to do!</strong> </p><p>It is worthwhile having a quick look at the <a href="https://blog.aboutamazon.com.au/amazon-in-australia/our-leadership-principles">AWS Leadership Principles</a> (I have already referenced one of these in this article) as these are applied religiously in every aspect of the AWS business.  In particular, you should pay specific attention to the principals around simplicity and frugality.</p><p><strong>Good luck!</strong></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GCP Networking for AWS Professionals]]></title>
            <link>https://cloudywithachanceofbigdata.com/gcp-networking-for-aws-professionals</link>
            <guid>gcp-networking-for-aws-professionals</guid>
            <pubDate>Thu, 21 Feb 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[A primer on GCP networking for AWS engineers and architects]]></description>
            <content:encoded><![CDATA[<p>GCP and AWS share many similarities, they both provide similar services and both leverage containerization, virtualization and software defined networking.</p><p>There are some significant differences when it comes to their respective implementations, networking is a key example of this.</p><p>Before we compare and contrast the two different approaches to networking, it is worthwhile noting the genesis of the two major cloud providers.</p><h4><em>Google was born to be global, Amazon became global</em></h4><p>By no means am I suggesting that Amazon didn&#x27;t have designs on going global from it&#x27;s beginnings, but AWS was driven (entirely at the beginning) by the needs of the Amazon eCommerce business. Amazon started in the US before expanding into other regions (such as Europe and Australia). In some cases the expansion took decades (Amazon only entered Australia as a retailer in 2018).</p><p>Google, by contrast, was providing application, search and marketing services worldwide from its very beginning. GCP which was used as the vector to deliver these services and applications was architected around this global model, even though their actual data centre expansion may not have been as rapid as AWS’s (for example GCP opened its Australia region 5 years after AWS).</p><p>Their respective networking implementations reflect how their respective companies evolved.</p><h4><em>AWS is a leader in IaaS, GCP is a leader in PaaS</em></h4><p>This is only an opinion and may be argued, however if you look at the chronology of the two platforms, consider this:</p><ul><li>The first services released by AWS (simultaneously for all intents and purposes) were S3, SQS and EC2</li><li>The first service released by Google was AppEngine (a pure PaaS offering)</li></ul><p>Google has launched and matured their IaaS offerings since as AWS has done similarly with their PaaS offerings, but they started from much different places.</p><p>With all of that said, here are the key differences when it comes to networking between the two major cloud providers:</p><h3>GCP VPCs are Global by default, AWS VPCs are Regional only</h3><p>This is the first fundamental difference between the two providers. Each GCP project is allocated one VPC network with Subnets in each of the 18 GCP Regions. Whereas each AWS Account is allocated one Default VPC in each AWS Region with a Subnet in each AWS Availability Zone for that Region, that is each account has 17 VPCs in each of the 17 Regions (excluding GovCloud regions).</p><p><a href="images/gcp-default-network.png"><img src="images/gcp-default-network.png" alt="Default Global VPC Network in GCP"/></a></p><p>It is entirely possible to create VPCs in GCP which are Regional, but they are Global by default.</p><p>This global tenancy can be advantageous in many cases, but can be limiting in others, for instance there is a limit of 25 peering connections to any one VPC, the limit in AWS is 125.</p><h3>GCP Subnets are Regional, AWS Subnets are Zonal</h3><p>Subnets in GCP automatically span all Zones in a Region, whereas AWS VPC Subnets are assigned to Availability Zones in a Region. This means you are abstracted from some of the networking and zonal complexity, but you have less control over specific network placement of instances and endpoints. You can infer from this design that Zones are replicated or synchronised within a Region, making them less of a direct consideration for High Availability (or at least as much or your concern as they otherwise would be).</p><h3>All GCP Firewall Rules are Stateful</h3><p>AWS Security Groups are stateful firewall rules – meaning they maintain connection state for inbound connections, AWS also has Network ACLs (NACLs) which are stateless firewall rules. GCP has no direct equivalent of NACLs, however GCP Firewall Rules are more configurable than their AWS counterparts. For instance, GCP Firewall Rules can include Deny actions which is not an option with AWS Security Group Rules.</p><h3>Load Balancers in GCP are layer 4 (TCP/UDP) unless they are public facing</h3><p>AWS Application Load Balancers can be deployed in private VPCs with no external IPs attached to them. GCP has Application Load Balancers (Layer 7 load balancers) but only for public facing applications, internal facing load balancers in GCP are Network Load Balancers. This presents some challenges with application level load balancing functionality such as stickiness. There are potential workarounds however such as NGINX in GKE behind</p><h3>Firewall rules are at the Network Level not at the Instance or Service Level</h3><p>There are simple firewall settings available at the instance level, these are limited to allowing HTTP and HTTPS traffic to the instance only and don’t allow you to specify sources. Detailed Firewall Rules are set at the GCP VPC Network level and are not attached or associated with instances as they are in AWS.</p><p><em>Hopefully this is helpful for AWS engineers and architects being exposed to GCP for the first time!</em></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Streaming Data Warehouse]]></title>
            <link>https://cloudywithachanceofbigdata.com/the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined</link>
            <guid>the-streaming-data-warehouse-kappa-architecture-and-data-warehousing-re-imagined</guid>
            <pubDate>Thu, 14 Feb 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Kappa Architecture and Data Warehousing re-imagined]]></description>
            <content:encoded><![CDATA[<h3>Kappa Architecture and Data Warehousing re-imagined</h3><p><img src="images/sdw.png" alt="Streaming Data Warehouse"/></p><p>The aspiration to extend data analysis (predictive, descriptive or otherwise) to streaming event data has been common across every enterprise scale program I have been involved with. Often, however, this aspiration goes unrealised as it tends to slide down the priority scale as we still grapple with legacy batch oriented integration patterns and processes.</p><p>Event processing is not a new concept, real time event and transaction processing has been a standard feature for security, digital and operations functions for some time, however in the Data Warehousing, BI and Advanced Analytics worlds it is often spoken about but rarely implemented, except for tech companies of course. In many cases personalization is still a batch oriented process, e.g. train a model from a feature set built from historical data, generate recommendations in batch, serve these recommendations upon the next visit - wash, rinse, and repeat.</p><p>Lambda has existed for several years now as a data-processing architecture pattern designed to incorporate both batch and stream-processing capabilities. Moreover, messaging platforms have existed for decades, from point-to-point messaging systems, to message-oriented-middleware systems, to distributed pub-sub messaging systems such as Apache Kafka.</p><p>Additionally, open source streaming data processing frameworks and tools have proliferated in recent years with projects such as Storm, Samza, Flink and Spark Streaming becoming established solutions.</p><p>Kafka in particular, with its focus on durability, resiliency, availability and consistency, has graduated into fully fledged data platform <strong>not simply a transient messaging system</strong>. In many cases Kafka is serving as a back end for operational processes, such as applications implementing the CQRS (Command Query Responsibility Segregation) design pattern.  </p><p>In other words, it is not the technology that holds us back, it&#x27;s our lack of imagination.</p><p>Enter <a href="http://milinda.pathirage.org/kappa-architecture.com/">Kappa Architecture</a> where we no longer have to attempt to integrate streaming data with batch processes…<strong>everything is a stream</strong>. The ultimate embodiment of Kappa Architecture is the <strong><em>Streaming Data Warehouse</em></strong>.</p><p>In the Streaming Data Warehouse, tables are represented by topics. Topics represent either:</p><ul><li>unbounded event or change streams; or</li><li>stateful representations of data (such as master, reference or summary data sets).</li></ul><p>This approach makes possible the enrichment and/or summarisation of transaction or event data with master or reference data. Furthermore many of the patterns used in data warehousing and master data management are inherent in Kafka as you can represent the current state of an object as well as the complete change history of that object (in other words change data capture and associated slowly changing dimensions from one inbound stream).</p><p>Data is acquired from source systems either in real time or as a scheduled extract process, <strong>in either case the data is presented to Kafka as a stream</strong>.</p><p>The Kafka Avro Schema Registry provides a systematic contract with source systems which also serves as a data dictionary for consumers supporting schema evolution with backward and forward compatibility. Data is retained on the Kafka platform for a designated period of time (days or weeks) where it is available for applications and processes to consume - these processes can include data summarisation or sliding window operations for reporting or notification, or data integration or datamart building processes which sink data to other systems - these could include relational or non-relational data stores.</p><p>Real time applications can be built using the KStreams API and emerging tools such as KSQL can be used to provide a well-known interface for sampling streaming data or performing windowed processing operations on streams. Structured Streaming in Spark or Spark Streaming in its original RDD/DStream implementation can be used to prepare and enrich data for machine learning operations using Spark ML or Spark MLlib.  </p><p>In addition, data sinks can operate concurrently to sink datasets to S3 or Google Cloud Storage or both (multi cloud - like real time analytics - is something which is talked about more than it’s implemented…).</p><p>In the Streaming Data Warehouse architecture Kafka is much more than a messaging platform it is a distributed data platform, which could easily replace major components of a legacy (or even a modern) data architecture.  </p><p>It just takes a little imagination…</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Test Driven Infrastructure and Test Automation with Ansible, Molecule and Azure]]></title>
            <link>https://cloudywithachanceofbigdata.com/test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure</link>
            <guid>test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure</guid>
            <pubDate>Thu, 31 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[A few years back, before the rise of the hyper-scalers, I had my first infracode 'aha moment' with OpenStack. The second came with Kitchen.]]></description>
            <content:encoded><![CDATA[<p>A few years back, before the rise of the hyper-scalers, I had my first infracode &#x27;aha moment&#x27; with OpenStack. The second came with <a href="https://kitchen.ci/">Kitchen</a>.</p><p>I had already been using test driven development for application code and configuration automation for infrastructure but Kitchen brought the two together. Kitchen made it possible to write tests, spin up infrastructure, and then tear everything down again - the Red/Green/Refactor cycle for infrastructure. What made this even better was that it wasn&#x27;t a facsimile of a target environment, it was the same - same VM&#x27;s, same OS, same network.</p><p>Coming from a Chef background for configuration automation, Kitchen is a great fit to the Ruby ecosystem. Kitchen works with Ansible and Azure, but a Ruby environment and at least a smattering of Ruby coding skills are required.</p><p><a href="https://molecule.readthedocs.io/">Molecule</a> provides a similar red-green development cycle to Kitchen, but without the need to step outside of the familiar Python environment.</p><p>Out of the box, Molecule supports development of Ansible roles using either a Docker or Virtual Box infrastructure provider. Molecule also leverages the Ansible drivers for private and public cloud platforms.</p><p>Molecule can be configured to test an individual role or collections of roles in Ansible playbooks.</p><p>This tutorial demonstrates how to use Molecule with Azure to develop and test an individual Ansible role following the red/green/refactor infracode workflow, which can be generalised as:</p><ul><li><strong>Red</strong>-<!-- --> write a failing infrastructure test</li><li><strong>Green</strong> - write the Ansible tasks needed to pass the test</li><li>Refactor - repeat the process</li></ul><p>The steps required for this tutorial are as follows:</p><h2>Azure setup</h2><p>Ensure there is an existing Azure Resource Group that will be used for infracode development and testing. Within the resource group, ensure there is a single virtual network (vnet) with a single subnet. Ansible will use these for the default network setup.</p><h2>Setup a working environment</h2><p>There are a number of options for setting up a Python environment for Ansible and Molecule, including Python virtualenv or a Docker container environment.</p><h2>Create a Docker image for Ansible+Molecule+Azure</h2><p>This tutorial uses a Docker container environment. A <code>Dockerfile</code> for the image can be found in <code>./molecule-azure-image/Dockerfile</code>. The image sets up a sane Python3 environment with Ansible, Ansible<!-- -->[<!-- -->azure<!-- -->]<!-- -->, and Molecule <code>pip</code> modules installed.</p><div id="4bd0c2ccae06dcaedffc2d91e594145f"></div><h2>Create a Docker workspace</h2><p>Setup a working environment using the Docker image with Ansible, Molecule, and the <code>azure-cli</code> installed.</p><div id="f80ef20a720914cfd4e02cf9783fec06"></div><p>This example assumes the following:</p><ul><li>a resource group already exists with access rights to create virtual machines; and</li><li>the resource group contains a single vnet with a single subnet</li></ul><h2>Log into an Azure subcription</h2><p>Ansible supports a number of different methods for authenticating with Azure. This example uses the <code>azure-cli</code> to login interactively.</p><div id="fd8987e7f724de5393a411c24c74978b"></div><h2>Create an empty Ansible role with Molecule</h2><p>Molecule provides an <code>init</code> function with defaults for various providers. The molecule-azure-role-template creates an empty role with scaffolding for Azure.</p><div id="f9b301d950a2254ab9af4806f2110544"></div><p>Check that the environment is working by running the following code:</p><div id="d56c3cd1e25b51acc634e5adb8a0a256"></div><p>The output should look be similar to…</p><div id="a3f8aed99a7c910588a5651d8cabf0e8"></div><h2>Spin up an Azure VM</h2><p>Spin up a fresh VM to be used for infra-code development.</p><div id="14a621ee65f9c2db583ed5ef94274c71"></div><p>Molecule provides a handy option for logging into the new VM:</p><div id="456aa8a8860bf785b382e18ede204d33"></div><p>There is now a fresh Ubuntu 18.04 virtual machine ready for infra-code development. For this example, a basic Nginx server will be installed and verified.</p><h2>Write a failing test</h2><p><a href="https://testinfra.readthedocs.io/en/latest/">Testinfra</a> provides a <code>pytest</code> based framework for verifying server and infrastructure configuration. Molecule then manages the execution of those <code>testinfra</code> tests. The Molecule template provides a starting point for crafting tests of your own. For this tutorial, installation of the <code>nginx</code> service is verified. Modify the tests file using <code>vi molecule/default/tests/test_default.py</code></p><div id="5b22b20a192aecbecb8cc229cb5f2a69"></div><h2>Execute the failing test</h2><p>The Ansible task needed to install and enable <code>nginx</code> has not yet been written, so the test should fail:</p><div id="38eb4bb776a41db7aa68f5962a97af62"></div><p>If the initial sample tests in <code>test_default.py</code> are kept, then 3 tests should fail and 2 tests should pass.</p><h2>Write a task to install <code>nginx</code></h2><p>Add a task to install the <code>nginx</code> service using <code>vi tasks/main.yml</code>:</p><div id="40d884f0c3a39fc4b3e921d451d60358"></div><h2>Apply the role</h2><p>Apply the role to the instance created using Molecule.</p><div id="5787aee41e2e3e9373f656677567ae41"></div><p>The <code>nginx</code> package should now be installed, both enabled and started, and listening on port 80. Note that the <code>nginx</code> instance will not be accessible from the Internet due to the Azure network security rules. The <code>nginx</code> instance can be confirmed manually by logging into the instance and using <code>curl</code> to make a request to the <code>nginx</code> service.</p><div id="fb02518e7129bf28e27822c42221f706"></div><h2>Execute the passing test</h2><p>After applying the Ansible task to the instance, the <code>testinfra</code> tests should now pass.</p><div id="b6359519ca6068615f8f1473636f90ea"></div><h2>Cleanup</h2><p>Now that the Ansible role works as defined in the test specification, the development environment can be cleaned up.</p><div id="150971a02b3f4b2c65d551cb09a203d0"></div><p>Molecule removes the Azure resources created to develop and test the configuration role. Note that deletion may take a few minutes.</p><p>Finally, once you are done, exit the container environment. If the container was started with the <code>--rm</code> switch, the container will also be removed, leaving you with a clean workspace and newly minted Ansible role with automated test cases.</p><div id="4fbb00b116b1a389b0343f6424b19a1b"></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[When Life Gives you Undrinkable Wine – Make Cognac…]]></title>
            <link>https://cloudywithachanceofbigdata.com/when-life-gives-you-undrinkable-wine-make-cognac</link>
            <guid>when-life-gives-you-undrinkable-wine-make-cognac</guid>
            <pubDate>Wed, 30 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Make Cognac]]></description>
            <content:encoded><![CDATA[<p><img src="images/shutterstock_425848342.jpg" alt="Make Cognac"/></p><p>Firstly, this is not another motivational talk or motherhood statement (there are plenty of those out there already), but a metaphor about resourcefulness.</p><p>We have all heard the adage “when life gives you lemons, make lemonade”. Allow me to present a slight twist (pardon the pun…) on this statement.</p><p>Cognac is a variety of brandy named after the town of Cognac, France. It is distilled from a white wine grown from grapes in the area. The wine used to make Cognac is characterised as &quot;virtually undrinkable&quot;. However in its distilled form, Cognac is considered to be the world&#x27;s most refined spirit. With bottles of high end Cognac fetching as much as $200,000.</p><p>The area surrounding the town of Cognac was a recognised wine-growing area dating back to the third century, however when it was evident that the grapes produced in the town of Cognac itself were unsuitable for wine making, local producers developed the practice of double distillation in copper pot stills and ageing in oak barrels for at least two years. The product yielded was the spirit we now know as Cognac.</p><p>Fast forward 15 centuries to Scotland in the 1800’s, where John Walker was a humble shopkeeper. Local whiskey producers would bring John different varieties of single malt whiskeys, many of them below an acceptable standard. Instead of on selling these varieties – he began blending them as made-to-order whiskies, blended to meet specific customer requirements. From this idea a product (and subsequently one of the most globally recognised brands) was created which would last over 200 years.</p><p><strong>Interesting, but what does this have to do with technology you might ask?</strong></p><p>Ingenuity and resourcefulness have existed for as long as man has, but in the realm of technology and in particular cloud computing and open source software it has never been more imperative. We are continually faced with challenges on each assignment or project, often technology not working as planned or as advertised, or finding dead ends when trying to solve problems. This is particularly evident now as software and technology are moving at such an accelerated rate.</p><p>To be successful in this era you need creativity and lateral thinking. You need an ability not just to solve problems to which there are no documented solutions, but to create new products from existing ones which are not necessarily suitable for your specific objective. In many cases, looking to add value in the process. That is not just making lemonade from lemons (which anyone could think of) but making Cognac from sub-standard grapes or premium blended whiskey from sub-standard single malt whiskies.</p><p>One of my favourite Amazon Leadership Principles is <strong><em>“Invent and Simplify”</em></strong>:</p><blockquote><p><em>Leaders expect and require innovation and invention from their teams and always find ways to simplify. They are externally aware, look for new ideas from everywhere, and are not limited by “not invented here&quot;. As we do new things, we accept that we may be misunderstood for long periods of time.</em></p><p><a href="https://www.amazon.jobs/en/principles"><em><strong>https:</strong></em><strong>//www.amazon.jobs/en/principles</strong></a>  </p></blockquote><p>The intent in this statement is not simply to “lift and shift” current on premise systems and processes to the cloud, but to look for ways to streamline, rationalise and simplify processes in doing so. This mandates a combination of practicality and creativity.</p><p>The take away is when you are presented with a challenge, be creative and inventive and not just solve the problem but look for ways to add value in the process.</p><p>Cheers!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[S3 Object Notifications using Lambda and SES]]></title>
            <link>https://cloudywithachanceofbigdata.com/s3-object-notifications-using-lambda-and-ses</link>
            <guid>s3-object-notifications-using-lambda-and-ses</guid>
            <pubDate>Fri, 18 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Simple pattern for formatted emails from S3 object notifications using AWS Lambda and SES, built with Terraform and Python]]></description>
            <content:encoded><![CDATA[<p>Following on from the previous post in the Really Simple Terraform series <a href="https://cloudywithachanceofbigdata.com/really-simple-terraform-infrastructure-automation-using-aws-lambda/">simple-lambda-ec2-scheduler</a>, where we used Terraform to deploy a Lambda function including the packaging of the Python function into a ZIP archive and creation of all supporting objects (roles, policies, permissions, etc) – in this post we will take things a step further by using templating to update parameters in the Lambda function code before the packaging and creation of the Lambda function.</p><p>S3 event notifications can be published directly to an SNS topic which you could create an email subscription, this is quite straightforward. However the email notifications you get look something like this:</p><p><img src="images/sns-object-notification-email.png" alt="Email Notification sent via an SNS Topic Subscription"/></p><p>There is very little you can do about this.</p><p>However if you take a slightly different approach by triggering a Lambda function to send an email via SES you have much more control over content and formatting. Using this approach you could get an email notification that looks like this:</p><p><img src="images/ses-object-notification-email.png" alt="Email Notification sent using Lambda and SES"/></p><p>Much easier on the eye!</p><h2>Prerequisites</h2><p>You will need verified AWS SES (Simple Email Service) email addresses for the sender and recipient’s addresses used for your object notification emails. This can be done via the console as shown here:</p><p><img src="images/ses-verify.png" alt="SES Email Address Verification"/></p><p><em>Note that SES is not available in every AWS region, pick one that is generally closest to your particular reason (but it really doesn&#x27;t matter for this purpose).</em></p><h2>Deployment</h2><p>The Terraform module creates an IAM Role and associated policy for the Lambda function as shown here:</p><div id="023fab404c0df759d6d1d4bdb02ab4e8"></div><p>Variables in the module are substituted into the function code template, the rendered template file is then packaged as a ZIP archive to be uploaded as the Lambda function source as shown here:</p><div id="7d72d8c67114a9df0af1528a3b754d9e"></div><p><em>As in the previous post, I will reiterate that although Terraform is technically not a build tool, it can be used for simple build operations such as this.</em></p><p>The Lambda function is deployed using the following code:</p><div id="5e7f2a238e8e0270cd55def40a389903"></div><p>Finally the S3 object notification events are configured as shown here:</p><div id="e7de65f20c79e0efb115024597864a75"></div><p>Use the following commands to run this example (I have created a default credentials profile, but you could supply your API credentials directly, use STS, etc):</p><pre><code>cd simple-notifications-with-lambda-and-ses
terraform init
terraform apply
</code></pre><blockquote><p><em>Full source code can be found at: <a href="https://github.com/avensolutions/simple-notifications-with-lambda-and-ses"><strong>https://github.com/avensolutions/simple-notifications-with-lambda-and-ses</strong></a></em></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Infrastructure Automation using AWS Lambda]]></title>
            <link>https://cloudywithachanceofbigdata.com/infrastructure-automation-using-aws-lambda</link>
            <guid>infrastructure-automation-using-aws-lambda</guid>
            <pubDate>Tue, 15 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[Simple pattern for automating EC2 tasks using AWS Lambda and Terraform]]></description>
            <content:encoded><![CDATA[<p>There are many other blog posts and examples available for either scheduling infrastructure tasks such as the starting or stopping of EC2 instances; or deploying a Lambda function using Terraform. However, I have found many of the other examples to be unnecessarily complicated, so I have put together a very simple example doing both.</p><p>The function itself could be easily adapted to take other actions including interacting with other AWS services using the boto3 library (the Python AWS SDK). The data payload could be modified to pass different data to the function as well.</p><p>The script only requires input variables for <strong><em>schedule<!-- -->_<!-- -->expression</em></strong> (cron schedule based upon GMT for triggering the function – could also be expressed as a rate, e.g. <strong><em>rate(5 minutes))</em></strong> and <strong><em>environment</em></strong> (value passed to the function on each invocation). In this example the Input data is the value for the “Environment” key for an EC2 instance tag – a user defined tag to associate the instance to a particular environment (e.g. Dev, Test. Prod). The key could be changed as required, for instance if you wanted to stop instances based upon their given name or part thereof you could change the tag key to be “Name”.</p><p>When triggered, the function will stop all running EC2 instances with the given Environment tag.</p><p>The Terraform script creates:</p><ul><li>an IAM Role and associated policy for the Lambda Function</li><li>the Lambda function</li><li>a Cloudwatch event rule and trigger</li></ul><p>The IAM role and policies required for the Lambda function are deployed as shown here:</p><div id="6b8ed7c149a60e823361ee282615b826"></div><p>The function source code is packaged into a ZIP archive and deployed using Terraform as follows:</p><div id="ca6a26a62302ff809eae028bbfb28b41"></div><p>Admittedly Terraform is an infrastructure automation tool and not a build/packaging tool (such as Jenkins, etc), but in this case the packaging only involves zipping up the function source code, so Terraform can be used as a ‘one stop shop’ to keep things simple.</p><p>The Cloudwatch schedule trigger is deployed as follows:</p><div id="7920fda821eb4f03d8ba942da572180c"></div><p>Use the following commands to run this example (I have created a default credentials profile, but you could supply your API credentials directly, use STS, etc):</p><pre><code>cd simple-lambda-ec2-scheduler
terraform init
terraform apply
</code></pre><p><img src="images/terraform-screenshot.png" alt="Terraform output"/></p><blockquote><p><em>Full source code can be found at: <a href="https://github.com/avensolutions/simple-lambda-ec2-scheduler"><strong>https://github.com/avensolutions/simple-lambda-ec2-scheduler</strong></a></em></p></blockquote><p>Stay tuned for more simple Terraform deployment recipes in coming posts…</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Multi Stage ETL Framework using Spark SQL]]></title>
            <link>https://cloudywithachanceofbigdata.com/multi-stage-etl-framework-using-spark-sql</link>
            <guid>multi-stage-etl-framework-using-spark-sql</guid>
            <pubDate>Wed, 09 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[A simple configuration driven Spark SQL ETL framework]]></description>
            <content:encoded><![CDATA[<p>Most traditional data warehouse or datamart ETL routines consist of multi stage SQL transformations, often a series of CTAS (<code>CREATE TABLE AS SELECT</code>) statements usually creating transient or temporary tables – such as volatile tables in Teradata or Common Table Expressions (CTE’s).</p><p>:::note Spark Training Courses</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/">Data Transformation and Analysis Using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/">Stream and Event Processing using Apache Spark</a><br/>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/">Advanced Analytics Using Apache Spark</a></p><p>:::</p><p>The initial challenge when moving from a SQL/MPP based ETL framework platformed on Oracle, Teradata, SQL Server, etc to a Spark based ETL framework is what to do with this…</p><p><img src="images/multi-stage-sql.png" alt="Multi Stage SQL Based ETL"/></p><p>One approach is to use the lightweight, configuration driven, multi stage Spark SQL based ETL framework described in this post.</p><p>This framework is driven from a YAML configuration document. YAML was preferred over JSON as a document format as it allows for multi-line statements (SQL statements), as well as comments - which are very useful as SQL can sometimes be undecipherable even for the person that wrote it.</p><p>The YAML config document has three main sections: <strong><code>sources</code></strong>, <strong><code>transforms</code></strong> and <strong><code>targets</code></strong>.</p><h3>Sources</h3><p>The <strong><code>sources</code></strong> section is used to configure the input data source(s) including optional column and row filters. In this case the data sources are tables available in the Spark catalog (for instance the AWS Glue Catalog or a Hive Metastore), this could easily be extended to read from other datasources using the Spark DataFrameReader API.</p><div id="eaf03229466718ee125e0a6d23370f1b"></div><h3>Transforms</h3><p>The <strong><code>transforms</code></strong> section contains the multiple SQL statements to be run in sequence where each statement creates a temporary view using objects created by preceding statements.</p><div id="89ad7ac6b036e5f22b2d3dec43b1fe44"></div><h3>Targets</h3><p>Finally the <strong><code>targets</code></strong> section writes out the final object or objects to a specified destination (S3, HDFS, etc).</p><div id="5af780dd6b6e5ddd79a4cac8a59e6a69"></div><h3>Process SQL Statements</h3><p>The <strong><code>process_sql_statements.py</code></strong> script that is used to execute the framework is very simple (30 lines of code not including comments, etc). It loads the sources into Spark Dataframes and then creates temporary views to reference these datasets in the <strong><code>transforms</code></strong> section, then sequentially executes the SQL statements in the list of transforms. Lastly the script writes out the final view or views to the desired destination – in this case parquet files stored in S3 were used as the target.</p><p>You could implement an object naming convention such as prefixing object names with <code>sv_</code>, <code>iv_</code>, <code>fv_</code> (for source view, intermediate view and final view respectively) if this helps you differentiate between the different objects.</p><p>To use this framework you would simply use <strong><code>spark-submit</code></strong> as follows:</p><pre><code>spark-submit process_sql_statements.py config.yml
</code></pre><blockquote><p><em>Full source code can be found at: <a href="https://github.com/avensolutions/spark-sql-etl-framework"><strong>https://github.com/avensolutions/spark-sql-etl-framework</strong></a></em></p></blockquote>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Cost of Future Change]]></title>
            <link>https://cloudywithachanceofbigdata.com/cost-of-future-change</link>
            <guid>cost-of-future-change</guid>
            <pubDate>Tue, 01 Jan 2019 00:00:00 GMT</pubDate>
            <description><![CDATA[what we should really be focused on (but no one is…)]]></description>
            <content:encoded><![CDATA[<h3>what we should really be focused on (but no one is…)</h3><p><img src="images/changes-ahead.jpg" alt="Cost of Future Change"/></p><p>In my thirty year career in Technology I can’t think of a more profound period of change. It is not as if change never existed previously, it did. Innovation and Moore&#x27;s law have been constant features in technology. However, for decades we had experienced a long period of relativity stability. That is, the rate of change was somewhat fixed. This was largely due to the fact that change, for the most part, was dictated almost entirely by a handful of large companies (such as Oracle, Microsoft, IBM, SAP, etc.). We, the technology community, were exclusively at the mercy of the product management teams at these companies dreaming up the next new feature, and we were subject to the tech giant’s product release cycle as to when we could use this feature. The rate of change was therefore artificially suppressed to a degree.</p><p>Then along came the open source software revolution. I can recall being fortunate enough to be in a meeting with Steve Ballmer along with a small group of Microsoft partners in Sydney, Australia around 2002. I can remember him as a larger than life figure with a booming voice and an intense presence. I can vividly remember him going on a long diatribe about Open Source Software, primarily focused around Linux. To put some historical context around it, Linux was a fringe technology at best at that point in time – nobody was taking it seriously in the enterprise. The Apache Software Foundation wasn’t a blip on the radar at the time, Software-as-a-Service was in its infancy, and Platform-as-a-Service and cloud were non-factors – so what was he worried about?</p><p>From the fear instilled in his rant, you would have thought this was going to be the end of humanity as we knew it. Well it was the beginning of the end, the end of the halcyon days of the software industry as we knew it. The early beginnings of the end of the monopoly on change held by a select few software vendors. The beginning of the democratisation of change.</p><p>Fast forward fifteen years, and now everyone is a (potential) software publisher. Real world problems are solved in real companies and in many cases software products are created, published and made (freely) available to other companies. We have seen countless examples of this pattern, Hadoop at Yahoo!, Kafka at LinkedIn, Airflow at Airbnb, to name a few. Then there are companies created with scant capital or investment, driven by a small group of smart people focused on solving a problem or streamlining a process that they have encountered in the real world. Many of these companies growing to be globally recognised names, such as Atlassian or Hashicorp.</p><p>The rate of change is no longer suppressed to the privileged few – in fact we have an explosion of change. No longer do we have a handful of technology options to achieve a desired outcome or solve a particular problem, we have a constellation of options.</p><p>Now the bad news, the force multiplier in change created by open source communities cuts both ways. When the community is behind a project it gets hyper-charged, conversely when the community drops off, functionality and stability drop off just as quickly.</p><p>This means there are components, projects, modules, services we are using today that won’t be around in the same format in 2-3 years’ time. There are products that don’t exist today that will be integral (and potentially critical) to our operations in 1-2 years’ time.</p><p>This brings me to my final point – the cost of future change. We as leaders and custodians of technology in this period should not only factor in current day run costs or the cost of change we know about (the cost of current change), but the hidden cost of future change. We need to think that whatever we are doing now is not what we will be doing in a years’ time – we will need to accommodate future change.</p><p>We need to be thinking about the cost of future change and what we can do to minimise this. This means not just swapping out one system for another, but thinking about how you will change this system or component again in the near future. We need to take the opportunity that is in front of us to replace monoliths with modular systems, move from tightly coupled to loosely coupled components, from proprietary systems to open systems, from rigid architectures to extensible architectures.</p><p>When you’re planning a change today, consider the cost of future change…</p>]]></content:encoded>
        </item>
    </channel>
</rss>