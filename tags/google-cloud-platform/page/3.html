<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.18">
<link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Full Stack Chronicles Blog Feed RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Full Stack Chronicles Blog Feed Atom Feed">
<link rel="alternate" type="application/json" href="/feed.json" title="Full Stack Chronicles Blog Feed JSON Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0FVDC1E8G6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0FVDC1E8G6",{})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Full Stack Chronicles" href="/opensearch.xml"><title data-rh="true">17 posts tagged with &quot;google-cloud-platform&quot; | Full Stack Chronicles</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" name="twitter:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" property="og:url" content="https://fullstackchronicles.io/tags/google-cloud-platform/page/3"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="wot-verification" content="e7522390be4370727fac"><meta data-rh="true" property="og:title" content="17 posts tagged with &quot;google-cloud-platform&quot; | Full Stack Chronicles"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://fullstackchronicles.io/tags/google-cloud-platform/page/3"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/tags/google-cloud-platform/page/3" hreflang="en"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/tags/google-cloud-platform/page/3" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://MZCGVO503N-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.77ea11e8.css">
<link rel="preload" href="/assets/js/runtime~main.edf57fa8.js" as="script">
<link rel="preload" href="/assets/js/main.6d1825e7.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><div class="announcementBar_IbjG" style="background-color:#A9BCD0;color:#1A4E82" role="banner"><div class="announcementBarPlaceholder_NC_W"></div><div class="announcementBarContent_KsVm"><b>If you find our content useful, give it a ⭐️ on <a target="_blank" rel="noopener noreferrer" href="https://github.com/stackql/fullstackchronicles.io">GitHub</a>, if you want to contribute and become an author, submit a PR</b></div><button type="button" class="clean-btn close announcementBarClose_FG1z" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav class="navbar navbar--fixed-top navbarHideable_ObN2 navbar--primary"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Full Stack Chronicles</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-current="page" class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/tags">Topics</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/tags">All Topics</a></li><li><a class="dropdown__link" href="/tags/gcp">Google Cloud Platform</a></li><li><a class="dropdown__link" href="/tags/aws">AWS</a></li><li><a class="dropdown__link" href="/tags/azure">Azure</a></li><li><a class="dropdown__link" href="/tags/snowflake">Snowflake</a></li><li><a class="dropdown__link" href="/tags/okta">Okta</a></li><li><a class="dropdown__link" href="/tags/openapi">OpenAPI</a></li><li><a class="dropdown__link" href="/tags/spark">Spark</a></li><li><a class="dropdown__link" href="/tags/kafka">Kafka</a></li><li><a class="dropdown__link" href="/tags/ci-cd">CI/CD</a></li></ul></div><a class="navbar__item navbar__link" href="/archive">Archive</a><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_S7eR colorModeToggle_vKtC"><button class="clean-btn toggleButton_rCf9 toggleButtonDisabled_Pu9x" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_qEbK"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent Posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way">Recurse JavaScript Object to Get Values for a Given Key the Easy Way</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/dataops-with-container-images-and-multi-stage-builds">DataOps with Container Images and Multi-Stage Builds</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/using-the-snowflake-sql-api-with-typescript">Using the Snowflake SQL API with TypeScript</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/split-a-large-swagger-openapi-specification-into-smaller-documents">Split a large Open API or Swagger Specification into smaller documents</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/stream-processing-with-spark-structured-streaming-kafka-and-snowflake-using-python">Stream Processing with Spark Structured Streaming, Kafka and Snowflake using Python</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/simple-cli-pkce-auth-using-okta">Simple CLI Application to Login to Okta using PKCE</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/cloudy-with-a-chance-of-big-data-has-moved">Cloudy with a Chance of Big Data has Moved</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/scaling-up-prefect-with-gitstorage">Scaling up Prefect with GitStorage</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/implementing-a-serverless-sftp-gateway-using-the-aws-transfer-family">Implementing a Serverless SFTP Gateway using the AWS Transfer Family</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/simple-sso-with-an-external-idp-using-active-directory-and-okta">Simple SSO with an external IdP using Active Directory and Okta</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>17 posts tagged with &quot;google-cloud-platform&quot;</h1><a href="/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/spark-in-the-google-cloud-platform-part-2">Spark in the Google Cloud Platform Part 2</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-02-29T00:00:00.000Z" itemprop="datePublished">February 29, 2020</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/spark-gcp-featured-image.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Apache Spark in GCP" src="/assets/images/spark-gcp-featured-image-86e0bfd36db759253fff66591b594d8b.png" width="213" height="155" class="img_E7b_"></p><p>In the previous post in this series <a href="https://cloudywithachanceofbigdata.com/spark-in-the-google-cloud-platform-part-1/" target="_blank" rel="noopener noreferrer"><strong>Spark in the Google Cloud Platform Part 1</strong></a>, we started to explore the various ways in which we could deploy Apache Spark applications in GCP. The first option we looked at was deploying Spark using Cloud DataProc, a managed Hadoop cluster with various ecosystem components included.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</h5></div><div class="admonition-content"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>In this post, we will look at another option for deploying Spark in GCP – <em>a Spark Standalone cluster running on GKE</em>.</p><p>Spark Standalone refers to the in-built cluster manager provided with each Spark release. Standalone can be a bit of a misnomer as it sounds like a single instance – which it is not, standalone simply refers to the fact that it is not dependent upon any other projects or components – such as Apache YARN, Mesos, etc.</p><p>A Spark Standalone cluster consists of a Master node or instance and one of more Worker nodes. The Master node serves as both a master and a cluster manager in the Spark runtime architecture.</p><p>The Master process is responsible for marshalling resource requests on behalf of applications and monitoring cluster resources.</p><p>The Worker nodes host one or many Executor instances which are responsible for carrying out tasks.</p><p>Deploying a Spark Standalone cluster on GKE is reasonably straightforward. In the example provided in this post we will set up a private network (VPC), create a GKE cluster, and deploy a Spark Master pod and two Spark Worker pods (in a real scenario you would typically have many Worker pods).</p><p>Once the network and GKE cluster have been deployed, the first step is to create Docker images for both the Master and Workers.</p><p>The <code>Dockerfile</code> below can be used to create an image capable or running either the Worker or Master daemons:</p><iframe width="100%" frameborder="0" id="gist-a2828409021205b3f6587c824c59928d"></iframe><p>Note the shell scripts included in the <code>Dockerfile</code>: <code>spark-master</code> and <code>spark-worker</code>. These will be used later on by K8S deployments to start the relative Master and Worker daemon processes in each of the pods.</p><p>Next, we will use Cloud Build to build an image using the <code>Dockerfile</code> are store this in GCR (Google Container Registry), from the Cloud Build directory in our project we will run:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud builds submit --tag gcr.io/spark-demo-266309/spark-standalone</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Next, we will create Kubernetes deployments for our Master and Worker pods.</p><p>Firstly, we need to get cluster credentials for our GKE cluster named ‘spark-cluster’:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud container clusters get-credentials spark-cluster --zone australia-southeast1-a --project spark-demo-266309</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Now from within the <code>k8s-deployments\deploy</code> folder of our project we will use the <code>kubectl</code> command to deploy the Master pod, service and the Worker pods</p><p>Starting with the Master deployment, this will deploy our Spark Standalone image into a container running the Master daemon process:</p><iframe width="100%" frameborder="0" id="gist-31bca11627167e0cd963103e4c7f11d2"></iframe><p>To deploy the Master, run the following:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-master-deployment.yaml</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>The Master will expose a web UI on port 8080 and an RPC service on port 7077, we will need to deploy a K8S service for this, the YAML required to do this is shown here:</p><iframe width="100%" frameborder="0" id="gist-a72d3c38d7a3f94e88c7affd28a3034b"></iframe><p>To deploy the Master service, run the following:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-master-service.yaml</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Now that we have a Master pod and service up and running, we need to deploy our Workers which are preconfigured to communicate with the Master service.</p><p>The YAML required to deploy the two Worker pods is shown here:</p><iframe width="100%" frameborder="0" id="gist-97ceb93ed35959c41d80fb8c025a7ba1"></iframe><p>To deploy the Worker pods, run the following:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-worker-deployment.yaml</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>You can now inspect the Spark processes running on your GKE cluster.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get deployments</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Shows...</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-master   1/1     1            1           7m45s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-worker   2/2     2            2           9s</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl get pods</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Shows...</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">NAME                            READY   STATUS    RESTARTS   AGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-master-f69d7d9bc-7jgmj    1/1     Running   0          8m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-worker-55965f669c-rm59p   1/1     Running   0          24s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> spark-worker-55965f669c-wsb2f   1/1     Running   0          24s</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Next, as we need to expose the Web UI for the Master process we will create a <em>LoadBalancer</em> resource. The YAML used to do this is provided here:</p><iframe width="100%" frameborder="0" id="gist-56ee86f50f329f99679ff243bb00fb07"></iframe><p>To deploy the LB, you would run the following:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl create -f spark-ui-lb.yaml</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p><strong>NOTE</strong> This is just an example, for simplicity we are creating an external <em>LoadBalancer</em> with a public IP, this configuration is likely not be appropriate in most real scenarios, alternatives would include an internal <em>LoadBalancer</em>, retraction of Authorized Networks, a jump host, SSH tunnelling or IAP.</p><p>Now you’re up and running!</p><p>You can access the Master web UI from the Google Console link shown here:</p><p><a target="_blank" href="/assets/files/master-ui-link-c9d78a7032459c01291494b1c26e34ff.png"><img loading="lazy" alt="Accessing the Spark Master UI from the Google Cloud Console" src="/assets/images/master-ui-link-c9d78a7032459c01291494b1c26e34ff.png" width="1070" height="611" class="img_E7b_"></a></p><p>The Spark Master UI should look like this:</p><p><a target="_blank" href="/assets/files/spark-master-ui-fa6eecc91847995dea15601166516004.png"><img loading="lazy" alt="Spark Master UI" src="/assets/images/spark-master-ui-fa6eecc91847995dea15601166516004.png" width="1070" height="668" class="img_E7b_"></a></p><p>Next we will exec into a Worker pod, get a shell:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">kubectl exec -it spark-worker-55965f669c-rm59p -- sh</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Now from within the shell environment of a Worker – which includes all of the Spark client libraries, we will submit a simple Spark application:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">spark-submit --class org.apache.spark.examples.SparkPi \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> --master spark://10.11.250.98:7077 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">/opt/spark/examples/jars/spark-examples*.jar 10000</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>You can see the results in the shell, as shown here:</p><p><a target="_blank" href="/assets/files/spark-application-example-0f11e071394077ac8c678ed7d3e93791.png"><img loading="lazy" alt="Spark Pi Estimator Example" src="/assets/images/spark-application-example-0f11e071394077ac8c678ed7d3e93791.png" width="1022" height="932" class="img_E7b_"></a></p><p>Additionally, as all of the container logs go to Stackdriver, you can view the application logs there as well:</p><p><a target="_blank" href="/assets/files/container-logs-in-stackdriver-350797a4dd5cceab0b4aa12445adeed4.png"><img loading="lazy" alt="Container Logs in StackDriver" src="/assets/images/container-logs-in-stackdriver-350797a4dd5cceab0b4aa12445adeed4.png" width="1070" height="668" class="img_E7b_"></a></p><p>This is a simple way to get a Spark cluster running, it is not without its downsides and shortcomings however, which include the limited security mechanisms available (SASL, network security, shared secrets).</p><p>In the final post in this series we will look at Spark on Kubernetes, using Kubernetes as the Spark cluster manager and interacting with Spark using the Kubernetes API and control plane, see you then.</p><blockquote><p>Full source code for this article is available at: <a href="https://github.com/gamma-data/spark-on-gcp" target="_blank" rel="noopener noreferrer">https://github.com/gamma-data/spark-on-gcp</a></p></blockquote><p>The infrastructure coding for this example uses Powershell and Terraform, and is deployed as follows:</p><div class="codeBlockContainer_I0IT language-powershell theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-powershell codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">PS &gt; .\run.ps1 private-network apply &lt;gcp-project&gt; &lt;region&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">PS &gt; .\run.ps1 gke apply &lt;gcp-project&gt; &lt;region&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/apache-spark">apache-spark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/cloud-dataproc">cloud-dataproc</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/dataproc">dataproc</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gcp">gcp</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gke">gke</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/kubernetes">kubernetes</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/spark">spark</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Spark in the Google Cloud Platform Part 2" href="/spark-in-the-google-cloud-platform-part-2"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/spark-in-the-google-cloud-platform-part-1">Spark in the Google Cloud Platform Part 1</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-02-14T00:00:00.000Z" itemprop="datePublished">February 14, 2020</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/spark-gcp-featured-image.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Apache Spark in GCP" src="/assets/images/spark-gcp-featured-image-86e0bfd36db759253fff66591b594d8b.png" width="213" height="155" class="img_E7b_"></p><p>I have been an avid Spark enthusiast since 2014 (the early days..). Spark has featured heavily in every project I have been involved with from data warehousing, ETL, feature extraction, advanced analytics to event processing and IoT applications. I like to think of it as a Swiss army knife for distributed processing.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</h5></div><div class="admonition-content"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>Curiously enough, the first project I had been involved with for some years that did not feature the Apache Spark project was a green field GCP project which got me thinking… where does Spark fit into the GCP landscape?</p><p>Unlike the other major providers who use Spark as the backbone of their managed distributed ETL services with examples such as AWS Glue or the Spark integration runtime option in Azure Data Factory, Google’s managed ETL solution is Cloud DataFlow. Cloud DataFlow which is a managed Apache Beam service does not use a Spark runtime (there is a Spark Runner however this is not an option when using CDF). So where does this leave Spark?</p><p>My summation is that although Spark is not a first-class citizen in GCP (as far as managed ETL), it is not a second-class citizen either. This article will discuss the various ways Spark clusters and applications can be deployed within the GCP ecosystem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="quick-primer-on-spark">Quick Primer on Spark<a class="hash-link" href="#quick-primer-on-spark" title="Direct link to heading">​</a></h2><p>Every Spark application contains several components regardless of deployment mode, the components in the Spark runtime architecture are:</p><ul><li>the Driver</li><li>the Master</li><li>the Cluster Manager</li><li>the Executor(s), which run on worker nodes or Workers</li></ul><p>Each component has a specific role in executing a Spark program and all of the Spark components run in Java virtual machines (JVMs).</p><p><a target="_blank" href="/assets/files/spark-runtime-0b55b3d8793bab097b517603866abe16.png"><img loading="lazy" alt="Spark Runtime Architecture" src="/assets/images/spark-runtime-0b55b3d8793bab097b517603866abe16.png" width="752" height="420" class="img_E7b_"></a></p><p>Cluster Managers schedule and manage distributed resources (compute and memory) across the nodes of the cluster. Cluster Managers available for Spark include:</p><ul><li>Standalone</li><li>YARN (Hadoop)</li><li>Mesos</li><li>Kubernetes</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="spark-on-dataproc">Spark on DataProc<a class="hash-link" href="#spark-on-dataproc" title="Direct link to heading">​</a></h2><p>This is perhaps the simplest and most integrated approach to using Spark in the GCP ecosystem.</p><p>DataProc is GCP’s managed Hadoop Service (akin to AWS EMR or HDInsight on Azure). DataProc uses Hadoop/YARN as the Cluster Manager. DataProc clusters can be deployed on a private network (VPC using RFC1918 address space), supports encryption at Rest using Google Managed or Customer Managed Keys in KMS, supports autoscaling and the use of Preemptible Workers, and can be deployed in a HA config.</p><p>Furthermore, DataProc clusters can enforce strong authentication using Kerberos which can be integrated into other directory services such as Active Directory through the use of cross realm trusts.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="deployment">Deployment<a class="hash-link" href="#deployment" title="Direct link to heading">​</a></h3><p>DataProc clusters can be deployed using the <code>gcloud dataproc clusters create</code> command or using IaC solutions such as Terraform. For this article I have included an example in the source code using the <code>gcloud</code> command to deploy a DataProc cluster on a private network which was created using Terraform.</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="integration">Integration<a class="hash-link" href="#integration" title="Direct link to heading">​</a></h3><p>The beauty of DataProc is its native integration into IAM and the GCP service plane. Having been a long-time user of AWS EMR, I have found that the usability and integration are in many ways superior in GCP DataProc. Let’s look at some examples...</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="iam-and-iap-tcp-forwarding">IAM and IAP (TCP Forwarding)<a class="hash-link" href="#iam-and-iap-tcp-forwarding" title="Direct link to heading">​</a></h4><p>DataProc is integrated into Cloud IAM using various coarse grained permissions use as <code>dataproc.clusters.use</code> and simplified IAM Roles such as <code>dataproc.editor</code> or <code>dataproc.admin</code>. Members with bindings to the these roles can perform tasks such as submitting jobs and creating workflow templates (which we will discuss shortly), as well as accessing instances such as the master node instance or instances in the cluster using IAP (TCP Forwarding) without requiring a public IP address or a bastion host.</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="dataproc-jobs-and-workflows">DataProc Jobs and Workflows<a class="hash-link" href="#dataproc-jobs-and-workflows" title="Direct link to heading">​</a></h4><p>Spark jobs can be submitted using the console or via <code>gcloud dataproc jobs submit</code> as shown here:</p><p><a target="_blank" href="/assets/files/dataproc-spark-job-953a699ceef00b9e3b0f05068a844a56.png"><img loading="lazy" alt="Submitting a Spark Job using gcloud dataproc jobs submit" src="/assets/images/dataproc-spark-job-953a699ceef00b9e3b0f05068a844a56.png" width="1097" height="499" class="img_E7b_"></a></p><p>Cluster logs are natively available in StackDriver and standard out from the Spark Driver is visible from the console as well as via <code>gcloud</code> commands.</p><p>Complex Workflows can be created by adding Jobs as Steps in Workflow Templates using the following command:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud dataproc workflow-templates add-job spark</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="optional-components-and-the-component-gateway">Optional Components and the Component Gateway<a class="hash-link" href="#optional-components-and-the-component-gateway" title="Direct link to heading">​</a></h4><p>DataProc provides you with a Hadoop cluster including YARN and HDFS, a Spark runtine – which includes Spark SQL and SparkR. DataProc also supports several optional components including Anaconda, Jupyter, Zeppelin, Druid, Presto, and more.</p><p>Web interfaces to some of these components as well as the management interfaces such as the Resource Manager UI or the Spark History Server UI can be accessed through the Component Gateway.</p><p>This is a Cloud IAM integrated gateway (much like IAP) which can allow access through an authenticated and authorized console session to web UIs in the cluster – without the need for SSH tunnels, additional firewall rules, bastion hosts, or public IPs. Very cool.</p><p>Links to the component UIs as well as built in UIs like the YARN Resource Manager UI are available directly from through the console.</p><h4 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="jupyter">Jupyter<a class="hash-link" href="#jupyter" title="Direct link to heading">​</a></h4><p>Jupyter is a popular notebook application in the data science and analytics communities used for reproducible research. DataProc’s Jupyter component provides a ready-made Spark application vector using PySpark. If you have also installed the Anaconda component you will have access to the full complement of scientific and mathematic Python packages such as Pandas and NumPy which can be used in Jupyter notebooks as well. Using the Component Gateway, Jupyer notebooks can be accessed directly from the Google console as shown here:</p><p><a target="_blank" href="/assets/files/dataproc-jupyter-notebook-14d165b5563c903961beb68bef757a82.png"><img loading="lazy" alt="Jupyter Notebooks using DataProc" src="/assets/images/dataproc-jupyter-notebook-14d165b5563c903961beb68bef757a82.png" width="1359" height="924" class="img_E7b_"></a></p><p>From this example you can see that I accessed source data from a GCS bucket and used HDFS as local scratch space.</p><p>Furthermore, notebooks are automagically saved in your integrated Cloud Storage DataProc staging bucket and can be shared amongst analysts or accessed at a later time. These notebooks also persist beyond the lifespan of the cluster.</p><p>Next up we will look at deploying a Spark Standalone cluster on a GKE cluster, see you then!</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/apache-spark">apache-spark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gcp">gcp</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/spark">spark</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Spark in the Google Cloud Platform Part 1" href="/spark-in-the-google-cloud-platform-part-1"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/query-cloud-sql-through-big-query">Query Cloud SQL through Big Query</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-02-08T00:00:00.000Z" itemprop="datePublished">February 8, 2020</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/cloud-sql-federated-queries.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="cloudsql federated queries" src="/assets/images/cloud-sql-federated-queries-8af6a8b8a2536a9c78de7d33b3a5c626.png" width="150" height="157" class="img_E7b_"></p><p>This article demonstrates Cloud SQL federated queries for Big Query, a neat and simple to use feature.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="connecting-to-cloud-sql">Connecting to Cloud SQL<a class="hash-link" href="#connecting-to-cloud-sql" title="Direct link to heading">​</a></h2><p>One of the challenges presented when using Cloud SQL on a private network (VPC) is providing access to users. There are several ways to accomplish this which include:</p><ul><li>open the database port on the VPC Firewall (5432 for example for Postgres) and let users access the database using a command line or locally installed GUI tool <em>(may not be allowed in your environment)</em></li><li>provide a web based interface deployed on your VPC such as PGAdmin deployed on a GCE instance or GKE pod <em>(adds security and management overhead)</em></li><li>use the Cloud SQL proxy <em>(requires additional software to be installed and configured)</em></li></ul><p>In additional, all of the above solutions require direct IP connectivity to the instance which may not always be available. Furthermore each of these operations requires the user to present some form of authentication – in many cases the database user and password which then must be managed at an individual level.</p><p>Enter Cloud SQL federated queries for Big Query…</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="big-query-federated-queries-for-cloud-sql">Big Query Federated Queries for Cloud SQL<a class="hash-link" href="#big-query-federated-queries-for-cloud-sql" title="Direct link to heading">​</a></h2><p>Big Query allows you to query tables and views in Cloud SQL (currently MySQL and Postgres) using the Federated Queries feature. The queries could be authorized views in Big Query datasets for example.</p><p>This has the following advantages:</p><ul><li>Allows users to authenticate and use the GCP console to query Cloud SQL</li><li>Does not require direct IP connectivity to the user or additional routes or firewall rules</li><li>Leverages Cloud IAM as the authorization mechanism – rather that unmanaged db user accounts and object level permissions</li><li>External queries can be executed against a read replica of the Cloud SQL instance to offload query IO from the master instance</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="setting-it-up">Setting it up<a class="hash-link" href="#setting-it-up" title="Direct link to heading">​</a></h2><p>Setting up big query federated queries for Cloud SQL is exceptionally straightforward, a summary of the steps are provided below:</p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="step-1-enable-a-public-ip-on-the-cloud-sql-instance">Step 1. Enable a Public IP on the Cloud SQL instance<a class="hash-link" href="#step-1-enable-a-public-ip-on-the-cloud-sql-instance" title="Direct link to heading">​</a></h3><p>This sounds bad, but it isn’t really that bad. You need to enable a public interface for Big Query to be able to establish a connection to Cloud SQL, however this is not accessed through the actual public internet – rather it is accessed through the Google network using the back end of the front end if you will.</p><p>Furthermore, you configure an empty list of authorized networks which effectively shields the instance from the public network, this can be configured in Terraform as shown here:</p><iframe width="100%" frameborder="0" id="gist-81c57a80a7e588b98ea7d294dbaee242"></iframe><p>This configuration change can be made to a running instance as well as during the initial provisioning of the instance.</p><p>As shown below you will get a warning dialog in the console saying that you have no authorized networks - this is by design.</p><p><a target="_blank" href="/assets/files/cloud-sql-publicip-screenshot-53f21feadbfaf3be93f69de3ce771c2f.png"><img loading="lazy" alt="Cloud SQL Public IP Enabled with No Authorized Networks" src="/assets/images/cloud-sql-publicip-screenshot-53f21feadbfaf3be93f69de3ce771c2f.png" width="1139" height="775" class="img_E7b_"></a></p><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="step-2-create-a-big-query-dataset-which-will-be-used-to-execute-the-queries-to-cloud-sql">Step 2. Create a Big Query dataset which will be used to execute the queries to Cloud SQL<a class="hash-link" href="#step-2-create-a-big-query-dataset-which-will-be-used-to-execute-the-queries-to-cloud-sql" title="Direct link to heading">​</a></h3><p>Connections to Cloud SQL are defined in a Big Query dataset, this can also be use to control access to Cloud SQL using authorized views controlled by IAM roles.</p><iframe width="100%" frameborder="0" id="gist-8a4beaab134a1c72613347b5822d1724"></iframe><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="step-3-create-a-connection-to-cloud-sql">Step 3. Create a connection to Cloud SQL<a class="hash-link" href="#step-3-create-a-connection-to-cloud-sql" title="Direct link to heading">​</a></h3><p>To create a connection to Cloud SQL from Big Query you must first enable the BigQuery Connection API, this is done at a project level.</p><p>As this is a fairly recent feature there isn&#x27;t great coverage with either the <strong><code>bq</code></strong> tool or any of the Big Query client libraries to do this so we will need to use the console for now...</p><p>Under the <em><strong>Resources</strong></em> -&gt; <strong><em>Add Data</em></strong> link in the left hand panel of the Big Query console UI, select <strong><em>Create Connection</em></strong>. You will see a side info panel with a form to enter connection details for your Cloud SQL instance.</p><p>In this example I will setup a connection to a Cloud SQL read replica instance I have created:</p><p><a target="_blank" href="/assets/files/big-query-add-connection-67a0236996204c84c7608a8e6b8f4875.png"><img loading="lazy" src="/assets/images/big-query-add-connection-67a0236996204c84c7608a8e6b8f4875.png" width="959" height="775" class="img_E7b_"></a></p><p>Creating a Big Query Connection to Cloud SQL</p><p>More information on the Big Query Connections API can be found at: <a href="https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest" target="_blank" rel="noopener noreferrer">https://cloud.google.com/bigquery/docs/reference/bigqueryconnection/rest</a></p><p>The following permissions are associated with connections in Big Query:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.create  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.get  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.list  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.use  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.update  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">bigquery.connections.delete</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>These permissions are conveniently combined into the following predefined roles:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">roles/bigquery.connectionAdmin    (BigQuery Connection Admin)         </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">roles/bigquery.connectionUser     (BigQuery Connection User)          </span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><h3 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="step-4-query-away">Step 4. Query away!<a class="hash-link" href="#step-4-query-away" title="Direct link to heading">​</a></h3><p>Now the connection to Cloud SQL can be accessed using the <strong><code>EXTERNAL_QUERY</code></strong> function in Big Query, as shown here:</p><p><a target="_blank" href="/assets/files/cloud-sql-federated-queries-screenshot-34e49ac369753d8551095e92b7fc6264.png"><img loading="lazy" alt="Querying Cloud SQL from Big Query" src="/assets/images/cloud-sql-federated-queries-screenshot-34e49ac369753d8551095e92b7fc6264.png" width="1373" height="730" class="img_E7b_"></a></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/big-query">big-query</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/bigquery">bigquery</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/cloudsql">cloudsql</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gcp">gcp</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/googlecloudplatform">googlecloudplatform</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Query Cloud SQL through Big Query" href="/query-cloud-sql-through-big-query"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/google-cloud-sql-availability-for-postgresql-read-replicas">Google Cloud SQL – Availability for PostgreSQL – Part II (Read Replicas)</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-01-24T00:00:00.000Z" itemprop="datePublished">January 24, 2020</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/cloudsql-featured-image.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="CloudSQL HA" src="/assets/images/cloudsql-featured-image-896f0c764d7310d88c5cc9461f3feb6c.png" width="151" height="151" class="img_E7b_"></p><p>In this post we will look at read replicas as an additional method to achieve multi zone availability for Cloud SQL, which gives us - in turn - the ability to offload (potentially expensive) IO operations such as user created backups or read operations without adding load to the master instance.</p><p>In the previous post in this series we looked at Regional availability for PostgreSQL HA using Cloud SQL:</p><p><a href="https://cloudywithachanceofbigdata.com/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql-part-i/" target="_blank" rel="noopener noreferrer"><strong>Google Cloud SQL – Availability, Replication, Failover for PostgreSQL – Part I</strong></a></p><p>Recall that this option was simple to implement and worked relatively seamlessly and transparently with respect to zonal failover.</p><p>Now let&#x27;s look at read replicas in Cloud SQL as an additional measure for availability.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="deploying-read-replicas">Deploying Read Replica(s)<a class="hash-link" href="#deploying-read-replicas" title="Direct link to heading">​</a></h2><p>Deploying read replicas is slightly more involved than simple regional (high) availability, as you will need to define each replica or replicas as a separate Cloud SQL instance which is a slave to the primary instance (the master instance).</p><p>An example using Terraform is provided here, starting by creating the master instance:</p><iframe width="100%" frameborder="0" id="gist-34371a3c7edab140e70208cd7710c25a"></iframe><p>Next you would specify one or more read replicas (typically in a zone other than the zone the master is in):</p><iframe width="100%" frameborder="0" id="gist-980f2d6461db0613b4090413041b5ec5"></iframe><p>Note that several of the options supplied are omitted when creating a read replica database instance, such as the backup and maintenance options - as these operations cannot be performed on a read replica as we will see later.</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-1-212da608bf5ba2aa73198627a0cfe3e1.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-1-212da608bf5ba2aa73198627a0cfe3e1.png" alt="Cloud SQL Instances - showing master and replica"></a><figcaption class="figure-caption">Cloud SQL Instances - showing master and replica</figcaption></figure><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-2-ec13f6b5f93493f369db3f4c11987507.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-2-ec13f6b5f93493f369db3f4c11987507.png" alt="Cloud SQL Master Instance"></a><figcaption class="figure-caption">Cloud SQL Master Instance</figcaption></figure><p>Voila! You have just set up a master instance (the primary instance your application and/or users will connect to) along with a read replica in a different zone which will be asynchronously updated as changes occur on the master instance.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="read-replicas-in-action">Read Replicas in Action<a class="hash-link" href="#read-replicas-in-action" title="Direct link to heading">​</a></h2><p>Now that we have created a read replica, lets see it in action. After connecting to the read replica (like you would any other instance), attempt to access a table that has <strong><em>not</em></strong> been created on the master as shown here:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-3-8d16fdd58297948424c733b59cb58ff5.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-3-8d16fdd58297948424c733b59cb58ff5.png" alt="SELECT operation from the replica instance"></a><figcaption class="figure-caption">SELECT operation from the replica instance</figcaption></figure><p>Now create the table and insert some data on the <strong><em>master</em></strong> instance:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-4-c207e904d4880b5f799bcfdabf7de36a.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-4-c207e904d4880b5f799bcfdabf7de36a.png" alt="Create a table and insert a record on the master instance"></a><figcaption class="figure-caption">Create a table and insert a record on the master instance</figcaption></figure><p>Now try the select operation on the <strong><em>replica</em></strong> instance:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-5-a9412ef5afba9855221998e5551cb19e.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-5-a9412ef5afba9855221998e5551cb19e.png" alt="SELECT operation from the replica instance (after changes have been made on the master)"></a><figcaption class="figure-caption">SELECT operation from the replica instance (after changes have been made on the master)</figcaption></figure><p>It works!</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="some-points-to-note-about-cloud-sql-read-replicas">Some Points to Note about Cloud SQL Read Replicas<a class="hash-link" href="#some-points-to-note-about-cloud-sql-read-replicas" title="Direct link to heading">​</a></h2><ul><li>Users connect to a read replica as a normal database connection (as shown above)</li><li>Google managed backups (using the console or <code>gcloud sql backups create ..</code> ) can <strong><em>NOT</em></strong> be performed against replica instances</li><li>Read replicas can be used to offload IO intensive operations from the the master instance - such as user managed backup operations (e.g. <code>pg_dump</code>)</li></ul><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-6-c367a62c5d3e480fa25ea4d0ea2669cf.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-6-c367a62c5d3e480fa25ea4d0ea2669cf.png" alt="pg_dump operation against a replica instance"></a><figcaption class="figure-caption">pg_dump operation against a replica instance</figcaption></figure><ul><li><strong>BE CAREFUL</strong> Despite their name, read replicas are <strong>NOT</strong> read only, updates can be made which will NOT propagate back to the master instance - you could get yourself in an awful mess if you allow users to perform <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>CREATE</code> or <code>DROP</code> operations against replica instances.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="promoting-a-read-replica">Promoting a Read Replica<a class="hash-link" href="#promoting-a-read-replica" title="Direct link to heading">​</a></h2><p>If required a read replica can be promoted to a standalone Cloud SQL instance, which is another DR option. Keep in mind however as the read replica is updated in an asynchronous manner, promotion of a read replica may result in a loss of data (hopefully not much but a loss nonetheless). Your application RPO will dictate if this is acceptable or not.</p><p>Promotion of a read replica is reasonably straightforward as demonstrated here using the console:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-7-a4ec8a31ed6601a9e7253ce408893a90.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-7-a4ec8a31ed6601a9e7253ce408893a90.png" alt="Promoting a read replica using the console"></a><figcaption class="figure-caption">Promoting a read replica using the console</figcaption></figure><p>You can also use the following <code>gcloud</code> command:</p><p> gcloud sql instances promote-replica  &lt;replica<!-- -->_<!-- -->instance<!-- -->_<!-- -->name&gt;</p><p>Once you click on the <em>Promote Replica</em> button you will see the following warning:</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-8-310006339cb5a3d3c491dfb00fc86c88.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-8-310006339cb5a3d3c491dfb00fc86c88.png" alt=""></a><figcaption class="figure-caption"></figcaption></figure><p><em>Promoting a read replica using the console</em></p><p>This simply states that once you promote the replica instance your instance will become an independent instance with no further relationship with the master instance. Once accepted and the promotion process is complete, you can see that you now have two independent Cloud SQL instances (as advertised!):</p><figure><a href="/assets/images/cloud-sql-postgres-read-replicas-9-5cbdcea2c489b064bbbc1bad3617fbce.png"><img src="/assets/images/cloud-sql-postgres-read-replicas-9-5cbdcea2c489b064bbbc1bad3617fbce.png" alt="Promoted Cloud SQL instance"></a><figcaption class="figure-caption">Promoted Cloud SQL instance</figcaption></figure><p>Some of the options you would normally configure with a master instance would need to be configured on the promoted replica instance - such as high availability, maintenance and scheduled backups - but in the event of a zonal failure you would be back up and running with virtually no data loss!</p><blockquote><p>Full source code for this article is available at: <a href="https://github.com/gamma-data/cloud-sql-postgres-availability-tutorial" target="_blank" rel="noopener noreferrer">https://github.com/gamma-data/cloud-sql-postgres-availability-tutorial</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/cloudsql">cloudsql</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gcp">gcp</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/ha">ha</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/highavailability">highavailability</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/postgresql">postgresql</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Google Cloud SQL – Availability for PostgreSQL – Part II (Read Replicas)" href="/google-cloud-sql-availability-for-postgresql-read-replicas"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql">Google Cloud SQL – Availability, Replication, Failover for PostgreSQL – Part I</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2020-01-17T00:00:00.000Z" itemprop="datePublished">January 17, 2020</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://fullstackchronicles.io/images/cloudsql-featured-image.png"><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="CloudSQL HA" src="/assets/images/cloudsql-featured-image-896f0c764d7310d88c5cc9461f3feb6c.png" width="151" height="151" class="img_E7b_"></p><p>In this multi part blog we will explore the features available in Google Cloud SQL for High Availability, Backup and Recovery, Replication and Failover and Security (at rest and in transit) for the PostgreSQL DBMS engine. Some of these features are relatively hot of the press and in Beta – which still makes them available for general use.</p><p>We will start by looking at the High Availability (HA) options available to you when using the PostgreSQL engine in Google Cloud SQL.</p><p>Most of you would be familiar with the concepts of High Availability, Redundancy, Fault Tolerance, etc but let’s start with a definition of HA anyway:</p><blockquote><p>High availability (HA) is a characteristic of a system, which aims to ensure an agreed level of operational performance, usually uptime, for a higher than normal period.</p><p>Wikipedia</p></blockquote><p>Higher than a normal period is quite subjective, typically this is quantified by a percentage represented by a number of “9s”, that is 99.99% (which would be quoted as “four nines”), this would allot you 52.60 minutes of downtime over a one-year period.</p><p>Essentially the number of 9’s required will drive your bias towards the options available to you for Cloud SQL HA.</p><p>We will start with Cloud SQL HA in its simplest form, Regional Availability.</p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="regional-availability">Regional Availability<a class="hash-link" href="#regional-availability" title="Direct link to heading">​</a></h2><p>Knowing what we know about the Google Cloud Platform, regional availability means that our application or service (in this case Cloud SQL) should be resilient to a failure of any one zone in our region. In fact, as all GCP regions have at least 3 zones – two zones could fail, and our application would still be available.</p><p>Regional availability for Cloud SQL (which Google refers to as High Availability), creates a standby instance in addition to the primary instance and uses a regional Persistent Disk resource to store the database instance data, transaction log and other state files, which is synchronously replicated to a Persistent Disk resource local to the zones that the primary and standby instances are located in.</p><p>A shared IP address (like a Virtual IP) is used to serve traffic to the healthy (normally primary) Cloud SQL instance.</p><p>An overview of Cloud SQL HA is shown here:</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-9f997d42db7d02fc6391cb507e81bc6c.png"><img loading="lazy" alt="Cloud SQL High Availability" src="/assets/images/cloud-sql-ha-9f997d42db7d02fc6391cb507e81bc6c.png" width="716" height="541" class="img_E7b_"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="implementing-high-availability-for-cloud-sql">Implementing High Availability for Cloud SQL<a class="hash-link" href="#implementing-high-availability-for-cloud-sql" title="Direct link to heading">​</a></h2><p>Implementing Regional Availability for Cloud SQL is dead simple, it is one argument:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">availability_type = &quot;REGIONAL&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Using the <code>gcloud</code> command line utility, this would be:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud sql instances create postgresql-instance-1234 \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --availability-type=REGIONAL \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  --database-version= POSTGRES_9_6</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Using Terraform (with a complete set of options) it would look like:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">resource &quot;google_sql_database_instance&quot; &quot;postgres_ha&quot; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  provider = google-beta</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  region = var.region</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  project = var.project</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  name = &quot;postgresql-instance-${random_id.instance_suffix.hex}&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  database_version = &quot;POSTGRES_9_6&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  settings {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   tier = var.tier</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   disk_size = var.disk_size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   activation_policy = &quot;ALWAYS&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   disk_autoresize = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   disk_type = &quot;PD_SSD&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   **availability_type = &quot;REGIONAL&quot;**</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   backup_configuration {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     enabled = true</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     start_time = &quot;00:00&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ip_configuration  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     ipv4_enabled = false</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     private_network = google_compute_network.private_network.self_link</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   maintenance_window  {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     day = 7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     hour = 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     update_track = &quot;stable&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> } </span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>Once deployed you will notice a few different items in the console, first from the instance overview page you can see that the High Availability option is <code>ENABLED</code> for your instance.</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-1-12d875328c3d6743bab7c41038d7e382.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-1-12d875328c3d6743bab7c41038d7e382.png" width="1310" height="446" class="img_E7b_"></a></p><p>Second, you will see a Failover button enabled on the detailed management view for this instance.</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-2-39a03f8c41a267780563cbb20bafd2d4.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-2-39a03f8c41a267780563cbb20bafd2d4.png" width="1310" height="612" class="img_E7b_"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_R0VQ" id="failover">Failover<a class="hash-link" href="#failover" title="Direct link to heading">​</a></h2><p>Failovers and failbacks can be initiated manually or automatically (should the primary be unresponsive). A manual failover can be invoked by executing the command:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx" style="color:#393A34;background-color:#f6f8fa"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">gcloud sql instances failover postgresql-instance-1234</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" title="Copy" class="copyButton_eDfN clean-btn"><span class="copyButtonIcons_W9eQ" aria-hidden="true"><svg class="copyButtonIcon_XEyF" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_i9w9" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div><p>There is an <code>--async</code> option which will return immediately, invoking the failover operation asynchronously.</p><p>Failover can also be invoked from the Cloud Console using the Failover button shown previously. As an example I have created a connection to a regionally available Cloud SQL instance and started a command which runs a loop and prints out a counter:</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-3-6710a724106c4048213b24a2082b64cb.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-3-6710a724106c4048213b24a2082b64cb.png" width="1920" height="1040" class="img_E7b_"></a></p><p>Now using the <code>gcloud</code> command shown earlier, I have invoked a manual failover of the Cloud SQL instance.</p><p>Once the failover is initiated, the client connection is dropped (as the server is momentarily unavailable):</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-4-bb4bcf4f70179371b1b26d96a92a4dff.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-4-bb4bcf4f70179371b1b26d96a92a4dff.png" width="1920" height="1040" class="img_E7b_"></a></p><p>The connection can be immediately re-established afterwards, the state of the running query is lost - <strong><em>importantly no data is lost</em></strong> however. If your application clients had retry logic in their code and they weren&#x27;t executing a long running query, chances are no one would notice! Once reconnecting normal database activities can be resumed:</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-5-6f8c7134a54b7a38444f5ddbdbcf960e.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-5-6f8c7134a54b7a38444f5ddbdbcf960e.png" width="1920" height="1040" class="img_E7b_"></a></p><p>A quick check of the instance logs will show that the failover event has occured:</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-6-a4ab88ef121dbc8842347478cf0811d0.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-6-a4ab88ef121dbc8842347478cf0811d0.png" width="1310" height="568" class="img_E7b_"></a></p><p>Now when you return to the instance page in the console you will see a Failback button, which indicates that your instance is being served by the standby:</p><p><a target="_blank" href="/assets/files/cloud-sql-ha-7-25ae1613611767eea9c101c3a24f0632.png"><img loading="lazy" src="/assets/images/cloud-sql-ha-7-25ae1613611767eea9c101c3a24f0632.png" width="1239" height="641" class="img_E7b_"></a></p><p>Note that there may be a slight delay in the availability of this option as the replica is still being synched.</p><p>It is worth noting that nothing comes for free! When you run in REGIONAL or High Availability mode - you are effectively paying double the costs as compared to running in ZONAL mode. However availability and cost have always been trade-offs against one another - you get what you pay for...</p><blockquote><p>More information can be found at: <a href="https://cloud.google.com/sql/docs/postgres/high-availability" target="_blank" rel="noopener noreferrer">https://cloud.google.com/sql/docs/postgres/high-availability</a></p></blockquote><p>Next up we will look at read replicas (and their ability to be promoted) as another high availability alternative in Cloud SQL.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/cloudsql">cloudsql</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/gcp">gcp</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/google-cloud-platform">google-cloud-platform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/googlecloudplatform">googlecloudplatform</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/ha">ha</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/highavailability">highavailability</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/tags/postgresql">postgresql</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Google Cloud SQL – Availability, Replication, Failover for PostgreSQL – Part I" href="/google-cloud-sql-ha-backup-and-recovery-replication-failover-and-security-for-postgresql"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/tags/google-cloud-platform/page/2"><div class="pagination-nav__label">Newer Entries</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/tags/google-cloud-platform/page/4"><div class="pagination-nav__label">Older Entries</div></a></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Blog</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/">Home</a></li><li class="footer__item"><a class="footer__link-item" href="/archive">Archive</a></li><li class="footer__item"><a class="footer__link-item" href="/tags">Tags</a></li></ul></div><div class="col footer__col"><div class="footer__title">Sponsors</div><ul class="footer__items"><li class="footer__item"><a href="https://stackql.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">StackQL<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://gammadata.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gamma Data<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.windrate.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">WindRate<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.edf57fa8.js"></script>
<script src="/assets/js/main.6d1825e7.js"></script>
</body>
</html>