<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.9">
<link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Cloudy with a chance of Big Data Blog Feed RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Cloudy with a chance of Big Data Blog Feed Atom Feed">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-4N6KSJ2G0P"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-4N6KSJ2G0P",{})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Cloudy with a chance of Big Data" href="/opensearch.xml"><title data-react-helmet="true">2 posts tagged with &quot;big-data&quot; | Cloudy with a chance of Big Data</title><meta data-react-helmet="true" property="og:title" content="2 posts tagged with &quot;big-data&quot; | Cloudy with a chance of Big Data"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://cloudywithachanceofbigdata.com/tags/big-data"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://cloudywithachanceofbigdata.com/tags/big-data"><link data-react-helmet="true" rel="alternate" href="https://cloudywithachanceofbigdata.com/tags/big-data" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://cloudywithachanceofbigdata.com/tags/big-data" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d8b73325.css">
<link rel="preload" href="/assets/js/runtime~main.be9d500d.js" as="script">
<link rel="preload" href="/assets/js/main.2a4c0142.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_2qcr"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title">Cloudy with a chance of Big Data</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-current="page" class="navbar__link" href="/tags">Topics</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/tags">All Topics</a></li><li><a class="dropdown__link" href="/tags/gcp">Google Cloud Platform</a></li><li><a class="dropdown__link" href="/tags/aws">AWS</a></li><li><a class="dropdown__link" href="/tags/azure">Azure</a></li><li><a class="dropdown__link" href="/tags/ci-cd">CI/CD</a></li></ul></div><a class="navbar__item navbar__link" href="/archive">Archive</a><a href="https://github.com/cloudywithachanceofbigdata/cloudywithachanceofbigdata.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_71bT toggle_3Zt9 toggleDisabled_3cF-"><div class="toggleTrack_32Fl" role="button" tabindex="-1"><div class="toggleTrackCheck_3lV7"><span class="toggleIcon_O4iE">üåú</span></div><div class="toggleTrackX_S2yS"><span class="toggleIcon_O4iE">üåû</span></div><div class="toggleTrackThumb_xI_Z"></div></div><input type="checkbox" class="toggleScreenReader_28Tw" aria-label="Switch between dark and light mode"></div><div class="searchBox_1Doo"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_2ahu thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_2hhb margin-bottom--md">Recent Posts</div><ul class="sidebarItemList_2xAf"><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/aws-deployments-with-cloudformation-and-gitlab-ci">Simplified AWS Deployments with CloudFormation and GitLab CI</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/from-wordpress-to-jamstack">From Wordpress to Jamstack</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/using-jsonnet-to-configure-multiple-environments">Using Jsonnet to Configure Multiple Environments</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/use-bigquery-to-trigger-cloud-run">Use BigQuery to trigger Cloud Run</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/azure-static-web-app-review">Azure Static Web App Review</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/introducing-the-metadata-hub-mdh">Introducing the Metadata Hub (MDH)</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/masking-private-keys-in-ci-cd-pipelines-in-gitlab">Masking Private Keys in CI/CD Pipelines in GitLab</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/simple-tasker-configuration-driven-orchestration">Simple Tasker: Configuration driven orchestration</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/okta-admin-command-line-interface">Okta Admin Command Line Interface</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/enumerating-all-roles-for-a-user-in-snowflake">Enumerating all roles for a user in Snowflake</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;big-data&quot;</h1><a href="/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/map-reduce-is-dead-long-live-map-reduce">Map Reduce is Dead, Long Live Map Reduce</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2019-09-01T00:00:00.000Z" itemprop="datePublished">September 1, 2019</time> ¬∑ <!-- -->3 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_1yU8" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://cloudywithachanceofbigdata.com/images/image.png"><div class="markdown" itemprop="articleBody"><p>Firstly, this is not another Hadoop obituary, there are enough of those out there already.</p><p>The generalized title of this article has been used as an expression to convey the idea that something old has been replaced by something new. In the case of the expression ‚Äúthe King is dead, long live the King‚Äù the inference is that although one monarch has passed, another monarch instantly succeeds him.</p><p>In the age of instant gratification and hype cycle driven ‚Äòpump and dump‚Äô investment we are very quick to discard technologies that don‚Äôt realise overzealous targets for sales or market share. In our continuous attempts to find the next big thing, we are quick to throw out the last big thing and everything associated with it.</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="the-reports-of-my-death-have-been-greatly-exaggerated">The Reports of My Death Have Been Greatly Exaggerated<a aria-hidden="true" class="hash-link" href="#the-reports-of-my-death-have-been-greatly-exaggerated" title="Direct link to heading">‚Äã</a></h2><p>A classic example of this is the notion that Map Reduce is dead. Largely proliferated by the Hadoop obituaries which seem to be growing exponentially with each day.</p><p>A common e-myth is that Google invented the Map Reduce pattern, which is completely incorrect. In 2004, Google described a framework distributed systems implementation of the Map Reduce pattern in a white paper named <em>‚ÄúMapReduce: Simplified Data Processing on Large Clusters.‚Äù</em> ‚Äì this would inspire the first-generation processing framework (MapReduce) in the Hadoop project. But neither Google nor Yahoo! nor contributors to the Hadoop project (which include the pure play vendors) created the Map Reduce algorithm or processing pattern and neither shall any one of these have the rights to kill it.</p><p>The origins of the Map Reduce pattern can be traced all the way back to the early foundations of functional programming beginning with Lambda Calculus in the 1930s to LISP in the 1960s. Map Reduce is an integral pattern in all of today‚Äôs functional and distributed systems programming. You only need to look at the support for <code>map()</code> and <code>reduce()</code> operators in some of the most popular languages today including Python, JavaScript, Scala, and many more languages that support functional programming.</p><p>As far as distributed processing frameworks go, the Map Reduce pattern and its <code>map()</code> and <code>reduce()</code> methods are very prominent as higher order functions in APIs such as Spark, Kafka Streams, Apache Samza and Apache Flink to name a few.</p><p>While the initial Hadoop adaptation of Map Reduce has been supplanted by superior approaches, the Map Reduce processing pattern is far from dead.</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="on-the-fall-of-hadoop">On the fall of Hadoop...<a aria-hidden="true" class="hash-link" href="#on-the-fall-of-hadoop" title="Direct link to heading">‚Äã</a></h2><p>There is so much hysteria around the fall of Hadoop, we need to be careful not to toss the baby out with the bath water. Hadoop served a significant role in bringing open source, distributed systems from search engine providers to academia all the way to the mainstream, and still serves an important purpose in many organizations data ecosystems today and will continue to do so for some time.</p><p>OK, it wasn‚Äôt the panacea to everything, but who said it was supposed to be? The Hadoop movement was hijacked by hysteria, hype, venture capital, over ambitious sales targets and financial engineering ‚Äì this does not mean the technology was bad.</p><p>Hadoop spawned many significant related projects such as Spark, Kafka and Presto to name a few. These projects paved the way for cloud integration, which is now the dominant vector for data storage, processing, and analysis.</p><p>While the quest for world domination by the Hadoop pure play vendors may be over, the Hadoop movement (and the impact it has had on the enterprise data landscape) will live on.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/big-data">big-data</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/hadoop">hadoop</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/map-reduce">map-reduce</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Map Reduce is Dead, Long Live Map Reduce" href="/map-reduce-is-dead-long-live-map-reduce"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_GeHD" itemprop="headline"><a itemprop="url" href="/change-data-capture-at-scale-using-spark">Change Data Capture at Scale using Spark</a></h2><div class="blogPostData_291c margin-vert--md"><time datetime="2019-06-28T00:00:00.000Z" itemprop="datePublished">June 28, 2019</time> ¬∑ <!-- -->9 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_1R69"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_1yU8" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><meta itemprop="image" content="https://cloudywithachanceofbigdata.com/images/CDC-using-Spark.png"><div class="markdown" itemprop="articleBody"><p>Change Data Capture (CDC) is one of the most challenging processing patterns to implement at scale. I personally have had several cracks at this using various different frameworks and approaches, the most recent of which was implemented using Spark ‚Äì and I think I have finally found the best approach. Even though the code examples referenced use Spark, the pattern is language agnostic ‚Äì the focus is on the approach not the specific implementation (as this could be applied to any framework or runtime).</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</h5></div><div class="admonition-content"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>The first challenge you are faced with, is to compare a very large dataset (representing the current state of an object) with another potentially very large dataset (representing new or incoming data). Ideally, you would like the process to be configuration driven and accommodate such things as composite primary keys, or operational columns which you would like to restrict from change detection. You may also want to implement a pattern to segregate sensitive attributes from non-sensitive attributes.</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="overview">Overview<a aria-hidden="true" class="hash-link" href="#overview" title="Direct link to heading">‚Äã</a></h2><p>This pattern (and all my other recent attempts) is fundamentally based upon calculating a deterministic hash of the key and non-key attribute(s), and then using this hash as the basis for comparison. The difference between this pattern and my other attempts is in the distillation and reconstitution of data during the process, as well as breaking the pattern into discrete stages (designed to minimize the impact to other applications). This pattern can be used to process delta or full datasets.</p><p>A high-level flowchart representing the basic pattern is shown here:</p><p><a target="_blank" href="/assets/files/CDC-59f029c756f942661da9a4744801d227.png"><img alt="CDC Flowchart" src="/assets/images/CDC-59f029c756f942661da9a4744801d227.png"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="the-example">The Example<a aria-hidden="true" class="hash-link" href="#the-example" title="Direct link to heading">‚Äã</a></h2><p>The example provided uses the <a href="https://github.com/avensolutions/synthetic-cdc-data-generator" target="_blank" rel="noopener noreferrer">Synthetic CDC Data Generator application</a>, configuring an incoming set with 5 uuid columns acting as a composite key, and 10 random number columns acting as non key values. The initial days payload consists of 10,000 records, the subsequent days payload consists of another 10,000 records. From the initial dataset, a <code>DELETE</code> operation was performed at the source system for 20% of records, an <code>UPDATE</code> was performed on 40% of the records and the remaining 40% of records were unchanged. In this case the 20% of records that were deleted at the source, were replaced by new <code>INSERT</code> operations creating new keys.</p><p>After creating the synthesized day 1 and day 2 datasets, the files are processed as follows:</p><p>$ spark-submit cdc.py config.yaml data/day1 2019-06-18<br>
<!-- -->$ spark-submit cdc.py config.yaml data/day2 2019-06-19</p><p>Where <code>config.yaml</code> is the configuration for the dataset, data/day1 and data/day2 represent the different data files, and 2019-06-18 and 2019-06-19 represent a business effective date.</p><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="the-results">The Results<a aria-hidden="true" class="hash-link" href="#the-results" title="Direct link to heading">‚Äã</a></h2><p>You should see the following output from running the preceding commands for day 1 and day 2 respectively:</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="day-1">Day 1:<a aria-hidden="true" class="hash-link" href="#day-1" title="Direct link to heading">‚Äã</a></h3><iframe width="100%" frameborder="0" id="gist-b75edc7825b46c12b328d78d47b4b902"></iframe><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="day-2">Day 2:<a aria-hidden="true" class="hash-link" href="#day-2" title="Direct link to heading">‚Äã</a></h3><iframe width="100%" frameborder="0" id="gist-ca92e132105fb5bb381bf9dfca562bf4"></iframe><p>A summary analysis of the resultant dataset should show:</p><iframe width="100%" frameborder="0" id="gist-ded1f98dc4fce13c9bb3d12a51a46b94"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="pattern-details">Pattern Details<a aria-hidden="true" class="hash-link" href="#pattern-details" title="Direct link to heading">‚Äã</a></h2><p>Details about the pattern and its implementation follow.</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="current-and-historical-datasets">Current and Historical Datasets<a aria-hidden="true" class="hash-link" href="#current-and-historical-datasets" title="Direct link to heading">‚Äã</a></h3><p>The output of each operation will yield a current dataset (that is the current stateful representation of a give object) and a historical dataset partition (capturing the net changes from the previous state in an appended partition).</p><p>This is useful, because often consumers will primarily query the latest state of an object. The change sets (or historical dataset partitions) can be used for more advanced analysis by sophisticated users.</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="type-2-scds-sort-of">Type 2 SCDs (sort of)<a aria-hidden="true" class="hash-link" href="#type-2-scds-sort-of" title="Direct link to heading">‚Äã</a></h3><p>Two operational columns are added to each current and historical object:</p><ul><li><code>OPERATION</code> : Represents the last known operation to the record, valid values include :<ul><li><code>I</code> (<code>INSERT</code>)</li><li><code>U</code> (<code>UPDATE</code>)</li><li><code>D</code> (<code>DELETE</code> ‚Äì hard <code>DELETE</code>s, applies to full datasets only)</li><li><code>X</code> (Not supplied, applies to delta processing only)</li><li><code>N</code> (No change)</li></ul></li><li><code>EFF_START_DATE</code></li></ul><p>Since data structures on most big data or cloud storage platforms are immutable, we only store the effective start date for each record, this is changed as needed with each coarse-grained operation on the current object. The effective end date is inferred by the presence of a new effective start date (or change in the <code>EFF_START_DATE</code> value for a given record).</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="the-configuration">The Configuration<a aria-hidden="true" class="hash-link" href="#the-configuration" title="Direct link to heading">‚Äã</a></h3><p>I am using a YAML document to store the configuration for the pattern. Important attributes to include in your configuration are a list of keys and non keys and their datatype (this implementation does type casting as well). Other important attributes include the table names and file paths for the current and historical data structures.</p><p>The configuration is read at the beginning of a routine as an input along with the path of an incoming data file (a CSV file in this case) and a business effective date (which will be used as the <code>EFF_START_DATE</code> for new or updated records).</p><p>Processing is performed using the specified key and non key attributes and the output datasets (current and historical) are written to columnar storage files (parquet in this case). This is designed to make subsequent access and processing more efficient.</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="the-algorithm">The Algorithm<a aria-hidden="true" class="hash-link" href="#the-algorithm" title="Direct link to heading">‚Äã</a></h3><p>I have broken the process into stages as follows:</p><h4 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="stage-1--type-cast-and-hash-incoming-data">Stage 1 ‚Äì Type Cast and Hash Incoming Data<a aria-hidden="true" class="hash-link" href="#stage-1--type-cast-and-hash-incoming-data" title="Direct link to heading">‚Äã</a></h4><p>The first step is to create deterministic hashes of the configured key and non key values for incoming data. The hashes are calculated based upon a list of elements representing the key and non key values using the MD5 algorithm. The hashes for each record are then stored with the respective record. Furthermore, the fields are casted their target datatype as specified in the configuration. Both of these operations can be performed in a single pass of each row using a <code>map()</code> operation.</p><p>Importantly we only calculate hashes once upon arrival of new data, as the hashes are persisted for the life of the data ‚Äì and the data structures are immutable ‚Äì the hashes should never change or be invalidated.</p><h4 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="stage-2--determine-inserts">Stage 2 ‚Äì Determine INSERTs<a aria-hidden="true" class="hash-link" href="#stage-2--determine-inserts" title="Direct link to heading">‚Äã</a></h4><p>We now compare Incoming Hashes with previously calculated hash values for the (previous day‚Äôs) current object. If no current object exists for the dataset, then it can be assumed this is a first run. In this case every record is considered as an <code>INSERT</code> with an <code>EFF_START_DATE</code> of the business effective date supplied.</p><p>If there is a current object, then the key and non key hash values (only the hash values) are read from the current object. These are then compared to the respective hashes of the incoming data (which should still be in memory).</p><p>Given the full outer join:</p><p>incoming<!-- -->_<!-- -->data(keyhash, nonkeyhash)
FULL OUTER JOIN<br>
<!-- -->current<!-- -->_<!-- -->data(keyhash, nonkeyhash)
ON keyhash</p><p>Keys which exist in the left entity which do not exist in the right entity must be the results of an INSERT operation.</p><p>Tag these records with an operation of <code>I</code> with an <code>EFF_START_DATE</code> of the business effective date, then rejoin only these records with their full attribute payload from the incoming dataset. Finally, write out these records to the current and historical partition in <code>overwrite</code> mode.</p><h4 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="stage-3---determine-deletes-or-missing-records">Stage 3 - Determine DELETEs or Missing Records<a aria-hidden="true" class="hash-link" href="#stage-3---determine-deletes-or-missing-records" title="Direct link to heading">‚Äã</a></h4><p>Referring the previous full outer join operation, keys which exist in the right entity (current object) which do not appear in the left entity (incoming data) will be the result of a (hard) <code>DELETE</code> operation if you are processing full snapshots, otherwise if you are processing deltas these would be missing records (possibly because there were no changes at the source).</p><p>Tag these records as <code>D</code> or <code>X</code> respectively with an <code>EFF_START_DATE</code> of the business effective date, rejoin these records with their full attribute payload from the current dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h4 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="stage-4---determine-updates-or-unchanged-records">Stage 4 - Determine UPDATEs or Unchanged Records<a aria-hidden="true" class="hash-link" href="#stage-4---determine-updates-or-unchanged-records" title="Direct link to heading">‚Äã</a></h4><p>Again, referring to the previous full outer join, keys which exist in both the incoming and current datasets must be either the result of an <code>UPDATE</code> or they could be unchanged. To determine which case they fall under, compare the non key hashes. If the non key hashes differ, it must have been a result of an <code>UPDATE</code> operation at the source, otherwise the record would be unchanged.</p><p>Tag these records as <code>U</code> or <code>N</code> respectively with an <code>EFF_START_DATE</code> of the business effective date (in the case of an update - otherwise maintain the current <code>EFF_START_DATE</code>), rejoin these records with their full attribute payload from the incoming dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h3 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="key-callouts">Key Callouts<a aria-hidden="true" class="hash-link" href="#key-callouts" title="Direct link to heading">‚Äã</a></h3><p>A summary of the key callouts from this pattern are:</p><ul><li>Use the RDD API for iterative record operations (such as type casting and hashing)</li><li>Persist hashes with the records</li><li>Use Dataframes for <code>JOIN</code> operations</li><li>Only perform <code>JOIN</code>s with the <code>keyhash</code> and <code>nonkeyhash</code> columns ‚Äì this minimizes the amount of data shuffled across the network</li><li>Write output data in columnar (Parquet) format</li><li>Break the routine into stages, covering each operation, culminating with a <code>saveAsParquet()</code> action ‚Äì this may seem expensive but for large datsets it is more efficient to break down DAGs for each operation</li><li>Use caching for objects which will be reused between actions</li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="metastore-integration">Metastore Integration<a aria-hidden="true" class="hash-link" href="#metastore-integration" title="Direct link to heading">‚Äã</a></h4><p>Although I did not include this in my example, you could easily integrate this pattern with a metastore (such as a Hive metastore or AWS Glue Catalog), by using table objects and <code>ALTER TABLE</code> statements to add historical partitions.</p><h4 class="anchor anchorWithHideOnScrollNavbar_3R7-" id="further-optimisations">Further optimisations<a aria-hidden="true" class="hash-link" href="#further-optimisations" title="Direct link to heading">‚Äã</a></h4><p>If the incoming data is known to be relatively small (in the case of delta processing for instance), you could consider a broadcast join where the smaller incoming data is distributed to all of the different Executors hosting partitions from the current dataset.</p><p>Also you could add a key to the column config to configure a column to be nullable or not.</p><p>Happy CDCing!</p><blockquote><p>Full source code for this article can be found at: <a href="https://github.com/avensolutions/cdc-at-scale-using-spark" target="_blank" rel="noopener noreferrer">https://github.com/avensolutions/cdc-at-scale-using-spark</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_2ga9 padding--none margin-left--sm"><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/big-data">big-data</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/cdc">cdc</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/pyspark">pyspark</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/python">python</a></li><li class="tag_11ep"><a class="tag_1Okp tagRegular_3MiF" href="/tags/spark">spark</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Change Data Capture at Scale using Spark" href="/change-data-capture-at-scale-using-spark"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Blog</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/">Home</a></li><li class="footer__item"><a class="footer__link-item" href="/archive">Archive</a></li><li class="footer__item"><a class="footer__link-item" href="/tags">Tags</a></li></ul></div><div class="col footer__col"><div class="footer__title">Sponsors</div><ul class="footer__items"><li class="footer__item"><a href="https://infraql.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>InfraQL<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://gammadata.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Gamma Data<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/cloudywithachanceofbigdata/cloudywithachanceofbigdata.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/assets/js/runtime~main.be9d500d.js"></script>
<script src="/assets/js/main.2a4c0142.js"></script>
</body>
</html>