<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">10 posts tagged with &quot;python&quot; | Full Stack Chronicles</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" name="twitter:image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><meta data-rh="true" property="og:url" content="https://fullstackchronicles.io/tags/python/page/2"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="wot-verification" content="e7522390be4370727fac"><meta data-rh="true" property="og:title" content="10 posts tagged with &quot;python&quot; | Full Stack Chronicles"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://fullstackchronicles.io/tags/python/page/2"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/tags/python/page/2" hreflang="en"><link data-rh="true" rel="alternate" href="https://fullstackchronicles.io/tags/python/page/2" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://MZCGVO503N-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Full Stack Chronicles Blog Feed RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Full Stack Chronicles Blog Feed Atom Feed">
<link rel="alternate" type="application/json" href="/feed.json" title="Full Stack Chronicles Blog Feed JSON Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0FVDC1E8G6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0FVDC1E8G6",{})</script>


<link rel="search" type="application/opensearchdescription+xml" title="Full Stack Chronicles" href="/opensearch.xml"><link rel="stylesheet" href="/assets/css/styles.e97cb5a8.css">
<link rel="preload" href="/assets/js/runtime~main.5afc7ff5.js" as="script">
<link rel="preload" href="/assets/js/main.26aa8672.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top navbarHideable_m1mJ navbar--primary"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/full-stack-logo-transparent.svg" alt="Full Stack Chronicles" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Full Stack Chronicles</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-current="page" class="navbar__link active" aria-haspopup="true" aria-expanded="false" role="button" href="/tags">Topics</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/tags">All Topics</a></li><li><a class="dropdown__link" href="/tags/gcp">Google Cloud Platform</a></li><li><a class="dropdown__link" href="/tags/aws">AWS</a></li><li><a class="dropdown__link" href="/tags/azure">Azure</a></li><li><a class="dropdown__link" href="/tags/snowflake">Snowflake</a></li><li><a class="dropdown__link" href="/tags/okta">Okta</a></li><li><a class="dropdown__link" href="/tags/openapi">OpenAPI</a></li><li><a class="dropdown__link" href="/tags/spark">Spark</a></li><li><a class="dropdown__link" href="/tags/kafka">Kafka</a></li><li><a class="dropdown__link" href="/tags/ci-cd">CI/CD</a></li></ul></div><a class="navbar__item navbar__link" href="/archive">Archive</a><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent Posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/create-and-use-custom-magic-commands-in-jupyter">Create and use Custom Magic Commands in Jupyter</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/deno-in-five-minutes">Deno in 5 Minutes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/dbt-in-five-minutes">DBT in 5 Minutes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/loading-parquet-files-into-snowflake">Loading Parquet Files into Snowflake</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/analyze-developer-activity-with-stackql-jupyter-bigquery">Analyze Developer Activity with StackQL, Jupyter and BigQuery</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/converting-google-discovery-docs-to-openapi3-specs">Converting Google Discovery Docs to OpenAPI3 Specs</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/recurse-javascript-object-to-get-values-for-a-given-key-the-easy-way">Recurse JavaScript Object to Get Values for a Given Key the Easy Way</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/dataops-with-container-images-and-multi-stage-builds">DataOps with Container Images and Multi-Stage Builds</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/using-the-snowflake-sql-api-with-typescript">Using the Snowflake SQL API with TypeScript</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/split-a-large-swagger-openapi-specification-into-smaller-documents">Split a large Open API or Swagger Specification into smaller documents</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>10 posts tagged with &quot;python&quot;</h1><a href="/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/CDC-using-Spark.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/change-data-capture-at-scale-using-spark">Change Data Capture at Scale using Spark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2019-06-28T00:00:00.000Z" itemprop="datePublished">June 28, 2019</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="CDC using Spark" src="/assets/images/CDC-using-Spark-38cb48e3545c719f89f4380b50711cf3.png" width="256" height="251" class="img_ev3q"></p><p>Change Data Capture (CDC) is one of the most challenging processing patterns to implement at scale. I personally have had several cracks at this using various different frameworks and approaches, the most recent of which was implemented using Spark – and I think I have finally found the best approach. Even though the code examples referenced use Spark, the pattern is language agnostic – the focus is on the approach not the specific implementation (as this could be applied to any framework or runtime).</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>Spark Training Courses</div><div class="admonitionContent_S0QG"><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p></div></div><p>The first challenge you are faced with, is to compare a very large dataset (representing the current state of an object) with another potentially very large dataset (representing new or incoming data). Ideally, you would like the process to be configuration driven and accommodate such things as composite primary keys, or operational columns which you would like to restrict from change detection. You may also want to implement a pattern to segregate sensitive attributes from non-sensitive attributes.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="overview">Overview<a class="hash-link" href="#overview" title="Direct link to heading">​</a></h2><p>This pattern (and all my other recent attempts) is fundamentally based upon calculating a deterministic hash of the key and non-key attribute(s), and then using this hash as the basis for comparison. The difference between this pattern and my other attempts is in the distillation and reconstitution of data during the process, as well as breaking the pattern into discrete stages (designed to minimize the impact to other applications). This pattern can be used to process delta or full datasets.</p><p>A high-level flowchart representing the basic pattern is shown here:</p><p><a target="_blank" href="/assets/files/CDC-59f029c756f942661da9a4744801d227.png"><img loading="lazy" alt="CDC Flowchart" src="/assets/images/CDC-59f029c756f942661da9a4744801d227.png" width="382" height="1118" class="img_ev3q"></a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-example">The Example<a class="hash-link" href="#the-example" title="Direct link to heading">​</a></h2><p>The example provided uses the <a href="https://github.com/avensolutions/synthetic-cdc-data-generator" target="_blank" rel="noopener noreferrer">Synthetic CDC Data Generator application</a>, configuring an incoming set with 5 uuid columns acting as a composite key, and 10 random number columns acting as non key values. The initial days payload consists of 10,000 records, the subsequent days payload consists of another 10,000 records. From the initial dataset, a <code>DELETE</code> operation was performed at the source system for 20% of records, an <code>UPDATE</code> was performed on 40% of the records and the remaining 40% of records were unchanged. In this case the 20% of records that were deleted at the source, were replaced by new <code>INSERT</code> operations creating new keys.</p><p>After creating the synthesized day 1 and day 2 datasets, the files are processed as follows:</p><p>$ spark-submit cdc.py config.yaml data/day1 2019-06-18<br>
<!-- -->$ spark-submit cdc.py config.yaml data/day2 2019-06-19</p><p>Where <code>config.yaml</code> is the configuration for the dataset, data/day1 and data/day2 represent the different data files, and 2019-06-18 and 2019-06-19 represent a business effective date.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-results">The Results<a class="hash-link" href="#the-results" title="Direct link to heading">​</a></h2><p>You should see the following output from running the preceding commands for day 1 and day 2 respectively:</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="day-1">Day 1:<a class="hash-link" href="#day-1" title="Direct link to heading">​</a></h3><iframe width="100%" frameborder="0" id="gist-b75edc7825b46c12b328d78d47b4b902"></iframe><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="day-2">Day 2:<a class="hash-link" href="#day-2" title="Direct link to heading">​</a></h3><iframe width="100%" frameborder="0" id="gist-ca92e132105fb5bb381bf9dfca562bf4"></iframe><p>A summary analysis of the resultant dataset should show:</p><iframe width="100%" frameborder="0" id="gist-ded1f98dc4fce13c9bb3d12a51a46b94"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="pattern-details">Pattern Details<a class="hash-link" href="#pattern-details" title="Direct link to heading">​</a></h2><p>Details about the pattern and its implementation follow.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="current-and-historical-datasets">Current and Historical Datasets<a class="hash-link" href="#current-and-historical-datasets" title="Direct link to heading">​</a></h3><p>The output of each operation will yield a current dataset (that is the current stateful representation of a give object) and a historical dataset partition (capturing the net changes from the previous state in an appended partition).</p><p>This is useful, because often consumers will primarily query the latest state of an object. The change sets (or historical dataset partitions) can be used for more advanced analysis by sophisticated users.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="type-2-scds-sort-of">Type 2 SCDs (sort of)<a class="hash-link" href="#type-2-scds-sort-of" title="Direct link to heading">​</a></h3><p>Two operational columns are added to each current and historical object:</p><ul><li><code>OPERATION</code> : Represents the last known operation to the record, valid values include :<ul><li><code>I</code> (<code>INSERT</code>)</li><li><code>U</code> (<code>UPDATE</code>)</li><li><code>D</code> (<code>DELETE</code> – hard <code>DELETE</code>s, applies to full datasets only)</li><li><code>X</code> (Not supplied, applies to delta processing only)</li><li><code>N</code> (No change)</li></ul></li><li><code>EFF_START_DATE</code></li></ul><p>Since data structures on most big data or cloud storage platforms are immutable, we only store the effective start date for each record, this is changed as needed with each coarse-grained operation on the current object. The effective end date is inferred by the presence of a new effective start date (or change in the <code>EFF_START_DATE</code> value for a given record).</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-configuration">The Configuration<a class="hash-link" href="#the-configuration" title="Direct link to heading">​</a></h3><p>I am using a YAML document to store the configuration for the pattern. Important attributes to include in your configuration are a list of keys and non keys and their datatype (this implementation does type casting as well). Other important attributes include the table names and file paths for the current and historical data structures.</p><p>The configuration is read at the beginning of a routine as an input along with the path of an incoming data file (a CSV file in this case) and a business effective date (which will be used as the <code>EFF_START_DATE</code> for new or updated records).</p><p>Processing is performed using the specified key and non key attributes and the output datasets (current and historical) are written to columnar storage files (parquet in this case). This is designed to make subsequent access and processing more efficient.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-algorithm">The Algorithm<a class="hash-link" href="#the-algorithm" title="Direct link to heading">​</a></h3><p>I have broken the process into stages as follows:</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="stage-1--type-cast-and-hash-incoming-data">Stage 1 – Type Cast and Hash Incoming Data<a class="hash-link" href="#stage-1--type-cast-and-hash-incoming-data" title="Direct link to heading">​</a></h4><p>The first step is to create deterministic hashes of the configured key and non key values for incoming data. The hashes are calculated based upon a list of elements representing the key and non key values using the MD5 algorithm. The hashes for each record are then stored with the respective record. Furthermore, the fields are casted their target datatype as specified in the configuration. Both of these operations can be performed in a single pass of each row using a <code>map()</code> operation.</p><p>Importantly we only calculate hashes once upon arrival of new data, as the hashes are persisted for the life of the data – and the data structures are immutable – the hashes should never change or be invalidated.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="stage-2--determine-inserts">Stage 2 – Determine INSERTs<a class="hash-link" href="#stage-2--determine-inserts" title="Direct link to heading">​</a></h4><p>We now compare Incoming Hashes with previously calculated hash values for the (previous day’s) current object. If no current object exists for the dataset, then it can be assumed this is a first run. In this case every record is considered as an <code>INSERT</code> with an <code>EFF_START_DATE</code> of the business effective date supplied.</p><p>If there is a current object, then the key and non key hash values (only the hash values) are read from the current object. These are then compared to the respective hashes of the incoming data (which should still be in memory).</p><p>Given the full outer join:</p><p>incoming<!-- -->_<!-- -->data(keyhash, nonkeyhash)
FULL OUTER JOIN<br>
<!-- -->current<!-- -->_<!-- -->data(keyhash, nonkeyhash)
ON keyhash</p><p>Keys which exist in the left entity which do not exist in the right entity must be the results of an INSERT operation.</p><p>Tag these records with an operation of <code>I</code> with an <code>EFF_START_DATE</code> of the business effective date, then rejoin only these records with their full attribute payload from the incoming dataset. Finally, write out these records to the current and historical partition in <code>overwrite</code> mode.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="stage-3---determine-deletes-or-missing-records">Stage 3 - Determine DELETEs or Missing Records<a class="hash-link" href="#stage-3---determine-deletes-or-missing-records" title="Direct link to heading">​</a></h4><p>Referring the previous full outer join operation, keys which exist in the right entity (current object) which do not appear in the left entity (incoming data) will be the result of a (hard) <code>DELETE</code> operation if you are processing full snapshots, otherwise if you are processing deltas these would be missing records (possibly because there were no changes at the source).</p><p>Tag these records as <code>D</code> or <code>X</code> respectively with an <code>EFF_START_DATE</code> of the business effective date, rejoin these records with their full attribute payload from the current dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="stage-4---determine-updates-or-unchanged-records">Stage 4 - Determine UPDATEs or Unchanged Records<a class="hash-link" href="#stage-4---determine-updates-or-unchanged-records" title="Direct link to heading">​</a></h4><p>Again, referring to the previous full outer join, keys which exist in both the incoming and current datasets must be either the result of an <code>UPDATE</code> or they could be unchanged. To determine which case they fall under, compare the non key hashes. If the non key hashes differ, it must have been a result of an <code>UPDATE</code> operation at the source, otherwise the record would be unchanged.</p><p>Tag these records as <code>U</code> or <code>N</code> respectively with an <code>EFF_START_DATE</code> of the business effective date (in the case of an update - otherwise maintain the current <code>EFF_START_DATE</code>), rejoin these records with their full attribute payload from the incoming dataset, then write out these records to the current and historical partition in <code>append</code> mode.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="key-callouts">Key Callouts<a class="hash-link" href="#key-callouts" title="Direct link to heading">​</a></h3><p>A summary of the key callouts from this pattern are:</p><ul><li>Use the RDD API for iterative record operations (such as type casting and hashing)</li><li>Persist hashes with the records</li><li>Use Dataframes for <code>JOIN</code> operations</li><li>Only perform <code>JOIN</code>s with the <code>keyhash</code> and <code>nonkeyhash</code> columns – this minimizes the amount of data shuffled across the network</li><li>Write output data in columnar (Parquet) format</li><li>Break the routine into stages, covering each operation, culminating with a <code>saveAsParquet()</code> action – this may seem expensive but for large datsets it is more efficient to break down DAGs for each operation</li><li>Use caching for objects which will be reused between actions</li></ul><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="metastore-integration">Metastore Integration<a class="hash-link" href="#metastore-integration" title="Direct link to heading">​</a></h4><p>Although I did not include this in my example, you could easily integrate this pattern with a metastore (such as a Hive metastore or AWS Glue Catalog), by using table objects and <code>ALTER TABLE</code> statements to add historical partitions.</p><h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="further-optimisations">Further optimisations<a class="hash-link" href="#further-optimisations" title="Direct link to heading">​</a></h4><p>If the incoming data is known to be relatively small (in the case of delta processing for instance), you could consider a broadcast join where the smaller incoming data is distributed to all of the different Executors hosting partitions from the current dataset.</p><p>Also you could add a key to the column config to configure a column to be nullable or not.</p><p>Happy CDCing!</p><blockquote><p>Full source code for this article can be found at: <a href="https://github.com/avensolutions/cdc-at-scale-using-spark" target="_blank" rel="noopener noreferrer">https://github.com/avensolutions/cdc-at-scale-using-spark</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/big-data">big-data</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cdc">cdc</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/pyspark">pyspark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/python">python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/spark">spark</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/img/fullstackchronicles-cover-image.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/synthetic-cdc-data-generator">Synthetic CDC Data Generator</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2019-06-28T00:00:00.000Z" itemprop="datePublished">June 28, 2019</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>This is a simple routine to generate random data with a configurable number or records, key fields and non key fields to be used to create synthetic data for source change data capture (CDC) processing. The output includes an initial directory containing CSV files representing an initial data load, and an incremental directory containing CSV files representing incremental data.</p><p>Spark Training Courses from the AlphaZetta Academy</p><p><a href="https://academy.alphazetta.ai/data-transformation-and-analysis-using-apache-spark/" target="_blank" rel="noopener noreferrer">Data Transformation and Analysis Using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/stream-and-event-processing-using-apache-spark/" target="_blank" rel="noopener noreferrer">Stream and Event Processing using Apache Spark</a><br>
<a href="https://academy.alphazetta.ai/advanced-analytics-using-apache-spark/" target="_blank" rel="noopener noreferrer">Advanced Analytics Using Apache Spark</a></p><p>Arguments (by position) include:</p><ul><li><code>no_init_recs</code> : the number of initial records to generate</li><li><code>no_incr_recs</code> : the number of incremental records on the second run - should be <code>&gt;= no_init_recs</code></li><li><code>no_keys</code> : number of key columns in the dataset – keys are generated as UUIDs</li><li><code>no_nonkeys</code> : number of non-key columns in the dataset – nonkey values are generated as random numbers</li><li><code>pct_del</code> : percentage of initial records deleted on the second run - between 0.0 and 1.0</li><li><code>pct_upd</code> : percentage of initial records updated on the second run - between 0.0 and 1.0</li><li><code>pct_unchanged</code> : percentage of records unchanged on the second run - between 0.0 and 1.0</li><li><code>initial_output</code> : folder for initial output in CSV format</li><li><code>incremental_output</code> : folder for incremental output in CSV format</li></ul><p>NOTE : <code>pct_del</code> + <code>pct_upd</code> + <code>pct_unchanged</code> must equal 1.0</p><p>Example usage:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">$ spark-submit synthetic-cdc-data-generator.py 100000 100000 2 3 0.2 0.4 0.4 data/day1 data/day2</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Example output from the <strong><em>day1</em></strong> run for the above configuration would look like this:</p><iframe width="100%" frameborder="0" id="gist-befb034da2b4f25a1dbbc0e9b4b8eef6"></iframe><p>Note that this routine can be run subsequent times producing different key and non key values each time, as the keys are UUIDs and the values are random numbers.</p><p>We will use this application to generate random input data to demonstrate CDC using Spark in a subsequent post, see you soon!</p><blockquote><p>Full source code can be found at: <a href="https://github.com/avensolutions/synthetic-cdc-data-generator" target="_blank" rel="noopener noreferrer">https://github.com/avensolutions/synthetic-cdc-data-generator</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cdc">cdc</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/python">python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/spark">spark</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/molecule-ansible-azure.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/test-driven-infrastructure-and-test-automation-with-ansible-molecule-and-azure">Test Driven Infrastructure and Test Automation with Ansible, Molecule and Azure</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2019-01-31T00:00:00.000Z" itemprop="datePublished">January 31, 2019</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/datwiz" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="http://0.gravatar.com/avatar/f9af9c3fae755ac170c5798c53c5267d?s=80" alt="Chris Ottinger"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/datwiz" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Chris Ottinger</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Technologist</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Molecule Ansible Azure" src="/assets/images/molecule-ansible-azure-2437124ec0927c07c287635f016a6265.png" width="524" height="351" class="img_ev3q"></p><p>A few years back, before the rise of the hyper-scalers, I had my first infracode &#x27;aha moment&#x27; with OpenStack. The second came with <a href="https://kitchen.ci/" target="_blank" rel="noopener noreferrer">Kitchen</a>.</p><p>I had already been using test driven development for application code and configuration automation for infrastructure but Kitchen brought the two together. Kitchen made it possible to write tests, spin up infrastructure, and then tear everything down again - the Red/Green/Refactor cycle for infrastructure. What made this even better was that it wasn&#x27;t a facsimile of a target environment, it was the same - same VM&#x27;s, same OS, same network.</p><p>Coming from a Chef background for configuration automation, Kitchen is a great fit to the Ruby ecosystem. Kitchen works with Ansible and Azure, but a Ruby environment and at least a smattering of Ruby coding skills are required.</p><p><a href="https://molecule.readthedocs.io/" target="_blank" rel="noopener noreferrer">Molecule</a> provides a similar red-green development cycle to Kitchen, but without the need to step outside of the familiar Python environment.</p><p>Out of the box, Molecule supports development of Ansible roles using either a Docker or Virtual Box infrastructure provider. Molecule also leverages the Ansible drivers for private and public cloud platforms.</p><p>Molecule can be configured to test an individual role or collections of roles in Ansible playbooks.</p><p>This tutorial demonstrates how to use Molecule with Azure to develop and test an individual Ansible role following the red/green/refactor infracode workflow, which can be generalised as:</p><ul><li><strong>Red</strong>-<!-- --> write a failing infrastructure test</li><li><strong>Green</strong> - write the Ansible tasks needed to pass the test</li><li>Refactor - repeat the process</li></ul><p>The steps required for this tutorial are as follows:</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="azure-setup">Azure setup<a class="hash-link" href="#azure-setup" title="Direct link to heading">​</a></h2><p>Ensure there is an existing Azure Resource Group that will be used for infracode development and testing. Within the resource group, ensure there is a single virtual network (vnet) with a single subnet. Ansible will use these for the default network setup.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="setup-a-working-environment">Setup a working environment<a class="hash-link" href="#setup-a-working-environment" title="Direct link to heading">​</a></h2><p>There are a number of options for setting up a Python environment for Ansible and Molecule, including Python virtualenv or a Docker container environment.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="create-a-docker-image-for-ansiblemoleculeazure">Create a Docker image for Ansible+Molecule+Azure<a class="hash-link" href="#create-a-docker-image-for-ansiblemoleculeazure" title="Direct link to heading">​</a></h2><p>This tutorial uses a Docker container environment. A <code>Dockerfile</code> for the image can be found in <code>./molecule-azure-image/Dockerfile</code>. The image sets up a sane Python3 environment with Ansible, Ansible<!-- -->[<!-- -->azure<!-- -->]<!-- -->, and Molecule <code>pip</code> modules installed.</p><iframe width="100%" frameborder="0" id="gist-4bd0c2ccae06dcaedffc2d91e594145f"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="create-a-docker-workspace">Create a Docker workspace<a class="hash-link" href="#create-a-docker-workspace" title="Direct link to heading">​</a></h2><p>Setup a working environment using the Docker image with Ansible, Molecule, and the <code>azure-cli</code> installed.</p><iframe width="100%" frameborder="0" id="gist-f80ef20a720914cfd4e02cf9783fec06"></iframe><p>This example assumes the following:</p><ul><li>a resource group already exists with access rights to create virtual machines; and</li><li>the resource group contains a single vnet with a single subnet</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="log-into-an-azure-subcription">Log into an Azure subcription<a class="hash-link" href="#log-into-an-azure-subcription" title="Direct link to heading">​</a></h2><p>Ansible supports a number of different methods for authenticating with Azure. This example uses the <code>azure-cli</code> to login interactively.</p><iframe width="100%" frameborder="0" id="gist-fd8987e7f724de5393a411c24c74978b"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="create-an-empty-ansible-role-with-molecule">Create an empty Ansible role with Molecule<a class="hash-link" href="#create-an-empty-ansible-role-with-molecule" title="Direct link to heading">​</a></h2><p>Molecule provides an <code>init</code> function with defaults for various providers. The molecule-azure-role-template creates an empty role with scaffolding for Azure.</p><iframe width="100%" frameborder="0" id="gist-f9b301d950a2254ab9af4806f2110544"></iframe><p>Check that the environment is working by running the following code:</p><iframe width="100%" frameborder="0" id="gist-d56c3cd1e25b51acc634e5adb8a0a256"></iframe><p>The output should look be similar to…</p><iframe width="100%" frameborder="0" id="gist-a3f8aed99a7c910588a5651d8cabf0e8"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="spin-up-an-azure-vm">Spin up an Azure VM<a class="hash-link" href="#spin-up-an-azure-vm" title="Direct link to heading">​</a></h2><p>Spin up a fresh VM to be used for infra-code development.</p><iframe width="100%" frameborder="0" id="gist-14a621ee65f9c2db583ed5ef94274c71"></iframe><p>Molecule provides a handy option for logging into the new VM:</p><iframe width="100%" frameborder="0" id="gist-456aa8a8860bf785b382e18ede204d33"></iframe><p>There is now a fresh Ubuntu 18.04 virtual machine ready for infra-code development. For this example, a basic Nginx server will be installed and verified.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="write-a-failing-test">Write a failing test<a class="hash-link" href="#write-a-failing-test" title="Direct link to heading">​</a></h2><p><a href="https://testinfra.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">Testinfra</a> provides a <code>pytest</code> based framework for verifying server and infrastructure configuration. Molecule then manages the execution of those <code>testinfra</code> tests. The Molecule template provides a starting point for crafting tests of your own. For this tutorial, installation of the <code>nginx</code> service is verified. Modify the tests file using <code>vi molecule/default/tests/test_default.py</code></p><iframe width="100%" frameborder="0" id="gist-5b22b20a192aecbecb8cc229cb5f2a69"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="execute-the-failing-test">Execute the failing test<a class="hash-link" href="#execute-the-failing-test" title="Direct link to heading">​</a></h2><p>The Ansible task needed to install and enable <code>nginx</code> has not yet been written, so the test should fail:</p><iframe width="100%" frameborder="0" id="gist-38eb4bb776a41db7aa68f5962a97af62"></iframe><p>If the initial sample tests in <code>test_default.py</code> are kept, then 3 tests should fail and 2 tests should pass.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="write-a-task-to-install-nginx">Write a task to install <code>nginx</code><a class="hash-link" href="#write-a-task-to-install-nginx" title="Direct link to heading">​</a></h2><p>Add a task to install the <code>nginx</code> service using <code>vi tasks/main.yml</code>:</p><iframe width="100%" frameborder="0" id="gist-40d884f0c3a39fc4b3e921d451d60358"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="apply-the-role">Apply the role<a class="hash-link" href="#apply-the-role" title="Direct link to heading">​</a></h2><p>Apply the role to the instance created using Molecule.</p><iframe width="100%" frameborder="0" id="gist-5787aee41e2e3e9373f656677567ae41"></iframe><p>The <code>nginx</code> package should now be installed, both enabled and started, and listening on port 80. Note that the <code>nginx</code> instance will not be accessible from the Internet due to the Azure network security rules. The <code>nginx</code> instance can be confirmed manually by logging into the instance and using <code>curl</code> to make a request to the <code>nginx</code> service.</p><iframe width="100%" frameborder="0" id="gist-fb02518e7129bf28e27822c42221f706"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="execute-the-passing-test">Execute the passing test<a class="hash-link" href="#execute-the-passing-test" title="Direct link to heading">​</a></h2><p>After applying the Ansible task to the instance, the <code>testinfra</code> tests should now pass.</p><iframe width="100%" frameborder="0" id="gist-b6359519ca6068615f8f1473636f90ea"></iframe><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cleanup">Cleanup<a class="hash-link" href="#cleanup" title="Direct link to heading">​</a></h2><p>Now that the Ansible role works as defined in the test specification, the development environment can be cleaned up.</p><iframe width="100%" frameborder="0" id="gist-150971a02b3f4b2c65d551cb09a203d0"></iframe><p>Molecule removes the Azure resources created to develop and test the configuration role. Note that deletion may take a few minutes.</p><p>Finally, once you are done, exit the container environment. If the container was started with the <code>--rm</code> switch, the container will also be removed, leaving you with a clean workspace and newly minted Ansible role with automated test cases.</p><iframe width="100%" frameborder="0" id="gist-4fbb00b116b1a389b0343f6424b19a1b"></iframe></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/ansible">ansible</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/azure">azure</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cloud">cloud</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/infrastructure-code">infrastructure-code</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/molecule">molecule</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/python">python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/test-automation">test-automation</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/s3-object-notifications-using-Lambda-and-SES-with-Terraform.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/s3-object-notifications-using-lambda-and-ses">S3 Object Notifications using Lambda and SES</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2019-01-18T00:00:00.000Z" itemprop="datePublished">January 18, 2019</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="S3 object notifications using Lambda and SES with Terraform" src="/assets/images/s3-object-notifications-using-Lambda-and-SES-with-Terraform-3caec5211828aa843515f4004a922eba.png" width="353" height="257" class="img_ev3q"></p><p>Following on from the previous post in the Really Simple Terraform series <a href="https://cloudywithachanceofbigdata.com/really-simple-terraform-infrastructure-automation-using-aws-lambda/" target="_blank" rel="noopener noreferrer">simple-lambda-ec2-scheduler</a>, where we used Terraform to deploy a Lambda function including the packaging of the Python function into a ZIP archive and creation of all supporting objects (roles, policies, permissions, etc) – in this post we will take things a step further by using templating to update parameters in the Lambda function code before the packaging and creation of the Lambda function.</p><p>S3 event notifications can be published directly to an SNS topic which you could create an email subscription, this is quite straightforward. However the email notifications you get look something like this:</p><p><img loading="lazy" alt="Email Notification sent via an SNS Topic Subscription" src="/assets/images/sns-object-notification-email-173693c0618474107c4e13e72ea5e805.png" width="1056" height="593" class="img_ev3q"></p><p>There is very little you can do about this.</p><p>However if you take a slightly different approach by triggering a Lambda function to send an email via SES you have much more control over content and formatting. Using this approach you could get an email notification that looks like this:</p><p><img loading="lazy" alt="Email Notification sent using Lambda and SES" src="/assets/images/ses-object-notification-email-f2ec2abb7f361ac49d08d5b54ac369ad.png" width="643" height="714" class="img_ev3q"></p><p>Much easier on the eye!</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">​</a></h2><p>You will need verified AWS SES (Simple Email Service) email addresses for the sender and recipient’s addresses used for your object notification emails. This can be done via the console as shown here:</p><p><img loading="lazy" alt="SES Email Address Verification" src="/assets/images/ses-verify-67af4f0a0493eddb21c1500407857016.png" width="1126" height="801" class="img_ev3q"></p><p><em>Note that SES is not available in every AWS region, pick one that is generally closest to your particular reason (but it really doesn&#x27;t matter for this purpose).</em></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deployment">Deployment<a class="hash-link" href="#deployment" title="Direct link to heading">​</a></h2><p>The Terraform module creates an IAM Role and associated policy for the Lambda function as shown here:</p><iframe width="100%" frameborder="0" id="gist-023fab404c0df759d6d1d4bdb02ab4e8"></iframe><p>Variables in the module are substituted into the function code template, the rendered template file is then packaged as a ZIP archive to be uploaded as the Lambda function source as shown here:</p><iframe width="100%" frameborder="0" id="gist-7d72d8c67114a9df0af1528a3b754d9e"></iframe><p><em>As in the previous post, I will reiterate that although Terraform is technically not a build tool, it can be used for simple build operations such as this.</em></p><p>The Lambda function is deployed using the following code:</p><iframe width="100%" frameborder="0" id="gist-5e7f2a238e8e0270cd55def40a389903"></iframe><p>Finally the S3 object notification events are configured as shown here:</p><iframe width="100%" frameborder="0" id="gist-e7de65f20c79e0efb115024597864a75"></iframe><p>Use the following commands to run this example (I have created a default credentials profile, but you could supply your API credentials directly, use STS, etc):</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd simple-notifications-with-lambda-and-ses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform init</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform apply</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><blockquote><p><em>Full source code can be found at: <a href="https://github.com/avensolutions/simple-notifications-with-lambda-and-ses" target="_blank" rel="noopener noreferrer"><strong>https://github.com/avensolutions/simple-notifications-with-lambda-and-ses</strong></a></em></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/aws">aws</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/boto-3">boto3</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/lambda">lambda</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/python">python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/s-3">s3</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/ses">ses</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/terraform">terraform</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://fullstackchronicles.io/images/automate-infrastructure-tasks-using-lambda-with-terraform.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/infrastructure-automation-using-aws-lambda">Infrastructure Automation using AWS Lambda</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2019-01-15T00:00:00.000Z" itemprop="datePublished">January 15, 2019</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://s.gravatar.com/avatar/f96573d092470c74be233e1dded5376f?s=80" alt="Jeffrey Aven"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/jeffreyaven/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeffrey Aven</span></a></div><small class="avatar__subtitle" itemprop="description">Technologist and Cloud Consultant</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="Automate infrastructure tasks using Lambda with Terraform" src="/assets/images/automate-infrastructure-tasks-using-lambda-with-terraform-9e982eba53048591f885eb6fc5d1bad6.png" width="364" height="257" class="img_ev3q"></p><p>There are many other blog posts and examples available for either scheduling infrastructure tasks such as the starting or stopping of EC2 instances; or deploying a Lambda function using Terraform. However, I have found many of the other examples to be unnecessarily complicated, so I have put together a very simple example doing both.</p><p>The function itself could be easily adapted to take other actions including interacting with other AWS services using the boto3 library (the Python AWS SDK). The data payload could be modified to pass different data to the function as well.</p><p>The script only requires input variables for <strong><em>schedule<!-- -->_<!-- -->expression</em></strong> (cron schedule based upon GMT for triggering the function – could also be expressed as a rate, e.g. <strong><em>rate(5 minutes))</em></strong> and <strong><em>environment</em></strong> (value passed to the function on each invocation). In this example the Input data is the value for the “Environment” key for an EC2 instance tag – a user defined tag to associate the instance to a particular environment (e.g. Dev, Test. Prod). The key could be changed as required, for instance if you wanted to stop instances based upon their given name or part thereof you could change the tag key to be “Name”.</p><p>When triggered, the function will stop all running EC2 instances with the given Environment tag.</p><p>The Terraform script creates:</p><ul><li>an IAM Role and associated policy for the Lambda Function</li><li>the Lambda function</li><li>a Cloudwatch event rule and trigger</li></ul><p>The IAM role and policies required for the Lambda function are deployed as shown here:</p><iframe width="100%" frameborder="0" id="gist-6b8ed7c149a60e823361ee282615b826"></iframe><p>The function source code is packaged into a ZIP archive and deployed using Terraform as follows:</p><iframe width="100%" frameborder="0" id="gist-ca6a26a62302ff809eae028bbfb28b41"></iframe><p>Admittedly Terraform is an infrastructure automation tool and not a build/packaging tool (such as Jenkins, etc), but in this case the packaging only involves zipping up the function source code, so Terraform can be used as a ‘one stop shop’ to keep things simple.</p><p>The Cloudwatch schedule trigger is deployed as follows:</p><iframe width="100%" frameborder="0" id="gist-7920fda821eb4f03d8ba942da572180c"></iframe><p>Use the following commands to run this example (I have created a default credentials profile, but you could supply your API credentials directly, use STS, etc):</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cd simple-lambda-ec2-scheduler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform init</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">terraform apply</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" alt="Terraform output" src="/assets/images/terraform-screenshot-2f677526d3d3dc10870393e11e6e85b8.png" width="1042" height="683" class="img_ev3q"></p><blockquote><p><em>Full source code can be found at: <a href="https://github.com/avensolutions/simple-lambda-ec2-scheduler" target="_blank" rel="noopener noreferrer"><strong>https://github.com/avensolutions/simple-lambda-ec2-scheduler</strong></a></em></p></blockquote><p>Stay tuned for more simple Terraform deployment recipes in coming posts…</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/aws">aws</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/automation">automation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/boto-3">boto3</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/cloudwatch">cloudwatch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/ec-2">ec2</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/lambda">lambda</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/python">python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/scheduled-tasks">scheduled-tasks</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/terraform">terraform</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/tags/python"><div class="pagination-nav__label">Newer Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Blog</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">Home</a></li><li class="footer__item"><a class="footer__link-item" href="/archive">Archive</a></li><li class="footer__item"><a class="footer__link-item" href="/tags">Tags</a></li></ul></div><div class="col footer__col"><div class="footer__title">Sponsors</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackql.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">StackQL<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://gammadata.io/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Gamma Data<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.windrate.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">WindRate<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/stackql/fullstackchronicles.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div></div></footer></div>
<script src="/assets/js/runtime~main.5afc7ff5.js"></script>
<script src="/assets/js/main.26aa8672.js"></script>
</body>
</html>